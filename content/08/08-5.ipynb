{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度校验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。 \n",
    "\n",
    "为了避免这样的问题，我们采取一种叫做**梯度的数值检验（Numerical Gradient Checking）**的方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。\n",
    " \n",
    "对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的$\\theta$，我们计算出在 $\\theta-\\sigma$处和$\\theta+\\sigma$的代价值（$\\sigma$是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在$\\theta$处的代价值。 \n",
    "\n",
    "![](https://i.loli.net/2018/12/02/5c03d7c2cb557.png)\n",
    "\n",
    "Octave 中代码如下： \n",
    "gradApprox = (J(theta + eps) – J(theta - eps)) / (2*eps) \n",
    "\n",
    "当$\\theta$是一个向量时，我们则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验，下面是一个只针对$\\theta_1$进行检验的示例： \n",
    "$$\\frac{\\partial}{\\partial\\theta_1} = \\frac{J(\\theta_1+\\sigma_1,\\theta_2,\\theta_3,...,\\theta_n)-J(\\theta_1-\\sigma_1,\\theta_2,\\theta_3,...,\\theta_n)}{2\\sigma}$$\n",
    "\n",
    "最后我们还需要对通过反向传播方法计算出的偏导数进行检验。 \n",
    "\n",
    "根据上面的算法，计算出的偏导数存储在矩阵$D^{(l)}_{ij}$中。检验时，我们要将该矩阵展开成为向量，同时我们也将$\\theta$矩阵展开为向量，我们针对每一个$\\theta$都计算一个近似的梯度值，将\n",
    "这些值存储于一个近似梯度矩阵中，最终将得出的这个矩阵同$D^{(l)}_{ij}$进行比较。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
