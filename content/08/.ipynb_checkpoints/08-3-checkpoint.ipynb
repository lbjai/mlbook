{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理解反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的学习优化体现在参数的改变上, 那么我们该怎么去改变它们呢?\n",
    "\n",
    "这一节将帮助大家探索和理解优化算法的核心. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理(前向传播)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**学习反向传播之前, 我们先学习一下前向传播(9-3视频, 1 minutes 37 seconds, 可以查看吴恩达教授视频中的详细推导过程).**\n",
    "\n",
    "![](http://imgbed.momodel.cn/5cc1a0b3e3067ce9b6abf75f.jpg)\n",
    "\n",
    "\n",
    "$x_1, x_2, +1$分别代表了输入和偏差项, 不考虑偏差项, 我们有两个输入, 两个隐藏层, 每个隐藏层有两个神经元, 最后有一个输出.\n",
    "\n",
    "那么我们会有两个权重矩阵.$W_{hidden1}, W_{hidden2}$, 他们的$W_{shape} = (input_{num}, numofunits_{hidden})$\n",
    "\n",
    "**我们的计算公式为(这里是矩阵乘法)**\n",
    "\n",
    "$$\n",
    "    Output_{layer} = input_{hidden} * W_{hidden}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    Output_{layer} = \\sum_{i=1}^{feature_{num}}input_{i} * W_{i} \n",
    "$$\n",
    "\n",
    "**我们来进行一个特例的计算**\n",
    "\n",
    "$$\n",
    "    W = [[1], [2], [3]]\n",
    "$$\n",
    "\n",
    "$$\n",
    "    Intput = [[1, 2, 3]]\n",
    "$$\n",
    "\n",
    "$$\n",
    "    output = Input * W = 1 * 1 + 2 * 2 + 3 * 3 = 14\n",
    "$$\n",
    "\n",
    "**接下里我们来实战一下, 看看自己是否掌握了呢~**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def forward_progragation(inputs, weights):\n",
    "    if inputs.shape[1] != weights.shape[0]:\n",
    "        raise RuntimeError('维度不匹配, 请检查输入')\n",
    "    # todo 计算输出层\n",
    "    output = np.matmul(inputs, weights)\n",
    "    return output\n",
    "\n",
    "inputs = np.random.uniform(size=(1, 3))\n",
    "weights = np.random.uniform(size=(3, 1))\n",
    "\n",
    "output = forward_progragation(inputs, weights)\n",
    "\n",
    "print(output.shape)\n",
    "\n",
    "assert output.shape == (1, 1), '输出结果不对, 请继续检查'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**恭喜你, 现在你已经掌握了全连接网络的单层前向传播, 可以自己尝试多层的编写~**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数\n",
    "\n",
    "![](http://imgbed.momodel.cn/5cc1a0b3e3067ce9b6abf75e.jpg)\n",
    "\n",
    "\n",
    "吴恩达教授有讲, 我们使用`S`型函数进行激活(视频, 3 minute 10 seconds), 我们来看一下`S`型函数的曲线以及表达式\n",
    "\n",
    "$$\n",
    "    sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "**我们来练习一下~**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # todo 实现sigmoid激活函数\n",
    "    result = 1.0 / (1 + np.exp(-x))\n",
    "    return result\n",
    "\n",
    "x = 0\n",
    "result = sigmoid(x)\n",
    "assert result == 0.5, '函数实现错了呢, 请修改'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**现在我们已经掌握了层的计算以及激活函数的编写, 先表扬一下自己, 已经掌握了神经网络的基本骨架了, 多层网络只是对这种结构进行堆叠~**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**理解反向传播**\n",
    "\n",
    "![](http://imgbed.momodel.cn/5cc1a0b4e3067ce9b6abf760.jpg)\n",
    "\n",
    "接下来我们来一起看看反向传播.\n",
    "\n",
    "### 反向传播\n",
    "\n",
    "首先我们需要定义代价函数, 这是反向传播的第一步\n",
    "\n",
    "**代价函数的含义**\n",
    "\n",
    "代价函数代表的是我们模型的输出与真实值的差距, 越接近就代表我们的模型越准确, 那么我们的目标应该就是最小化这个损失, 模型输出和前面的权重有关(因为输入是固定的), 那么我们最后要的就是优化我们权重$W$, 方式就是对$W$求导了(我们的代价函数可以看做是关于$W$的函数), 可以得到变化最大的方向, 进行修正就完成了这个步骤了. \n",
    "\n",
    "**拓展**\n",
    "\n",
    "常用的代价函数\n",
    "\n",
    "**分类(交叉熵)**\n",
    "\n",
    "$$\n",
    "    J(\\theta) = \\frac{1}{2n}\\sum_i^n(y_ilogp_i + (1 - y_i)logp_i)\n",
    "$$\n",
    "\n",
    "**回归(MSE)**\n",
    "\n",
    "$$\n",
    "    J(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - p_i)^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拓展小练习\n",
    "\n",
    "**我们来实现一下这几个函数(可选)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(p, y):\n",
    "    log_p = np.log(p)\n",
    "    # todo 计算交叉熵\n",
    "    loss = (1.0 / n) * np.sum(np.multiply(y, p) + np.multiply(1 - y, p))\n",
    "    return loss\n",
    "\n",
    "def mse(p, y):\n",
    "    if len(p) != len(y):\n",
    "        raise RuntimeError('样本数不一致')\n",
    "    loss = 1.0 / len(p)\n",
    "    loss *= np.sum(np.power((p - y), 2))\n",
    "    return loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算偏导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**理解反向传播**\n",
    "\n",
    "![](http://imgbed.momodel.cn/5cc1a0b4e3067ce9b6abf761.jpg)\n",
    "\n",
    "**我们其实是在计算偏导数来进行更新的, 复合函数的求导其实就是链式法则**\n",
    "\n",
    "$$\n",
    "    f(g(x))' = \\frac{df(gx)}{dg(x)} * \\frac{dg(x)}{dx}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**现在我们已经学习了前向, 反向传播, 我们接着来实现一个单隐藏层, 代价函数是 MSE 的神经网络**\n",
    "\n",
    "我们有$X => (1, 5)$, $W => (5, 1)$, $output => (1, 1)$, $loss = mse$的结构.\n",
    "\n",
    "$$\n",
    "    output = np.matmul(W^T, X)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    loss = \\frac{1}{2}(output - y)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(p, y):\n",
    "    if len(p) != len(y):\n",
    "        raise RuntimeError('样本数不一致')\n",
    "    loss = 1.0 / len(p)\n",
    "    loss *= np.sum(np.power((p - y), 2))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_progragation(inputs, weights):\n",
    "    if inputs.shape[1] != weights.shape[0]:\n",
    "        raise RuntimeError('维度不匹配, 请检查输入')\n",
    "    output = np.matmul(inputs, weights)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_progragation(error, x, p):\n",
    "    delta_error = (error - p)\n",
    "    dw = np.matmul(x.T, delta_error)\n",
    "    return dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward:[[1.14818486]] \n",
      "loss:0.021958754174409168\n",
      "backward:\n",
      "[[-0.21902523]\n",
      " [-1.0724607 ]\n",
      " [-0.70203568]\n",
      " [-0.16878467]\n",
      " [-0.89255638]]\n"
     ]
    }
   ],
   "source": [
    "def net(x, w, y):\n",
    "    forward = forward_progragation(x, w)\n",
    "    loss = mse(forward, y)\n",
    "    dw = back_progragation(loss, x, forward)\n",
    "    print(\"forward:{0} \\nloss:{1}\\nbackward:\\n{2}\".format(forward, loss, dw))\n",
    "\n",
    "x = np.random.uniform(size=(1, 5))\n",
    "w = np.random.uniform(size=(5, 1))\n",
    "y = np.array([[1]])\n",
    "\n",
    "net(x, w, y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
