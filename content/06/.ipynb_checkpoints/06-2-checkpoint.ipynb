{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代价函数 II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 代价函数中的正则化处理\n",
    "+ 正则化参数选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高次项导致了过拟合的产生。\n",
    "\n",
    "正则化的基本方法：对高次项添加惩罚值，让高次项的系数接近于0。\n",
    "\n",
    "假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设： \n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} [ \\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^{2} + \\lambda\\sum_{j=1}^n\\theta^2_j  ]\n",
    "$$\n",
    " \n",
    "其中$\\lambda$又称为正则化参数（Regularization Parameter）。 注：根据惯例，我们不对$\\theta_0$进行惩罚。\n",
    "\n",
    "**牛刀小试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055000000000000014"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo 实现带正则的mse代价函数\n",
    "import numpy as np\n",
    "\n",
    "def mseWithRegular(predict, y, w, lmd=0.1):\n",
    "    '''\n",
    "        predict: 模型输出\n",
    "        y: 真实标签\n",
    "        w: 模型权重\n",
    "        lmd: 正则化参数\n",
    "    '''\n",
    "    # todo 实现上方公式\n",
    "    constrct_loss = np.sum((predict - y) ** 2)\n",
    "    experience_loss = lmd * np.sum(w ** 2)\n",
    "    loss = (constrct_loss + experience_loss) / (2 * len(predict))\n",
    "    return loss\n",
    "\n",
    "predict = np.array([1, 1.5, 2])\n",
    "y = np.array([0.9, 1.4, 2.1])\n",
    "w = np.array([[1], [1], [1]])\n",
    "mseWithRegular(predict, y, w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正则化参数选择**\n",
    "\n",
    "如果选择的正则化参数$\\lambda$过大，则会把所有的参数都最小化了，导致模型变成 $h_\\theta(x) = \\theta_0$，造成欠拟合。 \n",
    "\n",
    "原因是：增加$\\lambda\\sum_{j=1}^n\\theta^2_j$后，如果令λ的值很大的话，为了使 Cost Function 尽可能的小，所有的 $\\theta$的值（不包括$\\theta_0$）都会在一定程度上减小。 但若λ的值太大了，那么$\\theta$的值（不包括$\\theta_0$）都会趋近于 0，这样我们所得到的只能是一条平行于 x 轴的直线。\n",
    "\n",
    "所以对于正则化，我们要取一个合理的λ的值，这样才能更好的应用正则化。 \n",
    "\n",
    "回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
