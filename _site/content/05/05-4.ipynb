{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代价函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义代价函数用来拟合逻辑回归的参数，这便是监督学习问题中的逻辑回归模型的拟合问题。\n",
    "\n",
    "![](https://i.loli.net/2018/12/01/5c018f3228d7d.png)\n",
    "\n",
    "线性回归模型的代价函数是所有模型误差的平方和。理论上来说，对逻辑回归模型也可以沿用这个定义，但是问题在于，当我们将$h_\\theta(x) =  \\frac{1}{1+e^{-\\theta^TX}}$带入到这样的代价函数中时，得到的代价函数将是一个非凸函数（non-convex function）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.loli.net/2018/12/01/5c018ffd42ea4.png)\n",
    " \n",
    "这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。  \n",
    "\n",
    "线性回归的代价函数为： \n",
    "\n",
    "$$J(\\theta)= \\frac{1}{m}\\sum^m_{i=1}\\frac{1}{2}(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    " \n",
    "重新定义逻辑回归的代价函数为： \n",
    "$$J(\\theta)= \\frac{1}{m}\\sum^m_{i=1}Cost(h_\\theta(x^{(i)}), y^{(i)})$$ ，其中\n",
    "\n",
    "![](https://i.loli.net/2018/12/01/5c01cd69c8f26.png)\n",
    "\n",
    "$h_\\theta(x)$与$Cost(h_\\theta(x), y)$之间的关系如下图所示： \n",
    " \n",
    "![](https://i.loli.net/2018/12/01/5c01ce01cb59e.png)\n",
    "\n",
    "![](https://i.loli.net/2018/12/01/5c01ce2c9ce0e.png)\n",
    "\n",
    "\n",
    "这样构建的$Cost(h_\\theta(x), y)$函数的特点是：\n",
    "\n",
    "当实际的 y=1 且$h_\\theta(x)$也为1 时误差为 0，当 y=1 但$h_\\theta(x)$不为 1 时误差随着$h_\\theta(x)$的变小而变大；\n",
    "\n",
    "当实际的 y=0 且$h_\\theta(x)$也为 0 时代价为 0，当 y=0 但$h_\\theta(x)$不为 0 时误差随着 $h_\\theta(x)$的变大而变大。 \n",
    "\n",
    "将构建的$Cost(h_\\theta(x), y)$简化如下：  \n",
    "$$Cost(h_\\theta(x), y)=-y\\times{log(h_\\theta(x))}-(1-y)\\times{log(1-h_\\theta(x))}$$\n",
    "带入代价函数得到： \n",
    "$$J(\\theta) = \\frac{1}{m}\\sum^m_{i=1}[-y^{(i)}log(h_\\theta(x^{(i)}))-(1-y^{(i)}log(1-h_\\theta(x^{(i)}))]$$\n",
    "即： \n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum^m_{i=1}[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)}log(1-h_\\theta(x^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**牛刀小试**\n",
    "\n",
    "Todo: 补全下方的 cost 逻辑回归的代价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逻辑回归代价函数的Python代码实现：\n",
    "import numpy as np\n",
    "def cost(theta, X, y):\n",
    "    theta = np.matrix(theta)\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n",
    "    # Todo: 补全  second  \n",
    "    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n",
    "    return np.sum(first - second) / (len(X))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
