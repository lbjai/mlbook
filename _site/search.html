<!DOCTYPE html>
<html lang="en">
  

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Search the site</title>
  <meta name="description" content="吴恩达机器学习入门书">

  <link rel="canonical" href="http://0.0.0.0:4000//search">
  <link rel="alternate" type="application/rss+xml" title="吴恩达机器学习入门" href="http://0.0.0.0:4000//feed.xml">

  <meta property="og:url"         content="http://0.0.0.0:4000//search" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Search the site" />
<meta property="og:description" content="吴恩达机器学习入门书" />
<meta property="og:image"       content="" />


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "http://0.0.0.0:4000//search",
  "headline":
    "Search the site",
  "datePublished":
    "2019-07-04T08:14:11-05:00",
  "dateModified":
    "2019-07-04T08:14:11-05:00",
  "description":
    "吴恩达机器学习入门书",
  "author": {
    "@type": "Person",
    "name": "小莫团队"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "http://0.0.0.0:4000/",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "http://0.0.0.0:4000/",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">
  <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    CommonHTML: {
        linebreaks: {
            automatic: true,
        },
    },
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="/assets/js/anchor.min.js"  type="text/javascript"></script>
  <script>

initFunction(function () {
    anchors.add("main h1, main h2, main h3, main h4")
});

</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->


<!-- Display Thebelab button in each code cell -->
<script>
/**
 * Set up thebelab button for code blocks
 */

const thebelabCellButton = id =>
  `<a id="thebelab-cell-button-${id}" class="btn thebebtn o-tooltip--left" data-tooltip="Interactive Mode">
    <img src="/assets/images/edit-button.svg" alt="Start interactive mode">
  </a>`


const addThebelabButtonToCodeCells =  () => {

  const codeCells = document.querySelectorAll('div.input_area > div.highlighter-rouge:not(.output) pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("thebelab-cell-button-" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', thebelabCellButton(id));
    }
  })
}

initFunction(addThebelabButtonToCodeCells);
</script>



<script type="text/x-thebe-config">
    {
      requestKernel: true,
      binderOptions: {
        repo: 'YOUR-ORG/YOUR-REPO',
        ref: 'master',
      },
      codeMirrorConfig: {
        theme: "abcdef"
      },
      kernelOptions: {
        name: 'python3',
      }
    }
</script>
<script src="https://unpkg.com/thebelab@0.4.0/lib/index.js"></script>
<script>
    /**
     * Add attributes to Thebelab blocks
     */

    const initThebelab = () => {
        const addThebelabToCodeCells = () => {
            console.log("Adding thebelab to code cells...");
            // If Thebelab hasn't loaded, wait a bit and try again. This
            // happens because we load ClipboardJS asynchronously.
            if (window.thebelab === undefined) {
                setTimeout(addThebelabToCodeCells, 250)
            return
            }

            // If we already detect a Thebelab cell, don't re-run
            if (document.querySelectorAll('div.thebelab-cell').length > 0) {
                return;
            }

            // Find all code cells, replace with Thebelab interactive code cells
            const codeCells = document.querySelectorAll('.input_area pre')
            codeCells.forEach((codeCell, index) => {
                const id = codeCellId(index)
                codeCell.setAttribute('data-executable', 'true')

                // Figure out the language it uses and add this too
                var parentDiv = codeCell.parentElement.parentElement;
                var arrayLength = parentDiv.classList.length;
                for (var ii = 0; ii < arrayLength; ii++) {
                    var parts = parentDiv.classList[ii].split('language-');
                    if (parts.length === 2) {
                        // If found, assign dataLanguage and break the loop
                        var dataLanguage = parts[1];
                        break;
                    }
                }
                codeCell.setAttribute('data-language', dataLanguage)

                // If the code cell is hidden, show it
                var inputCheckbox = document.querySelector(`input#hidebtn${codeCell.id}`);
                if (inputCheckbox !== null) {
                    setCodeCellVisibility(inputCheckbox, 'visible');
                }
            });

            // Remove the event listener from the page so keyboard press doesn't
            // Change page
            document.removeEventListener('keydown', initPageNav)
            keyboardListener = false;

            // Init thebelab
            thebelab.bootstrap();

            // Remove copy buttons since they won't work anymore
            const copyAndThebeButtons = document.querySelectorAll('.copybtn, .thebebtn')
            copyAndThebeButtons.forEach((button, index) => {
                button.remove();
            });

            // Remove outputs since they'll be stale
            const outputs = document.querySelectorAll('.output *, .output')
            outputs.forEach((output, index) => {
                output.remove();
            });
        }

        // Add event listener for the function to modify code cells
        const thebelabButtons = document.querySelectorAll('[id^=thebelab], [id$=thebelab]')
        thebelabButtons.forEach((thebelabButton,index) => {
            if (thebelabButton === null) {
                setTimeout(initThebelab, 250)
                return
            };
            thebelabButton.addEventListener('click', addThebelabToCodeCells);
        });
    }

    // Initialize Thebelab
    initFunction(initThebelab);
</script>



  <!-- Load the auto-generating TOC -->
  <script src="/assets/js/tocbot.min.js"  type="text/javascript"></script>
  <script>
var initToc = function () {
  tocbot.init({
    tocSelector: 'nav.onthispage',
    contentSelector: '.c-textbook__content',
    headingSelector: 'h2, h3',
    orderedList: false,
    collapseDepth: 6,
    listClass: 'toc__menu',
    activeListItemClass: "",  // Not using
    activeLinkClass: "", // Not using
  });
  tocbot.refresh();
}
initFunction(initToc);
</script>

  <!-- Google analytics -->
  <script src="/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const codeCellId = index => `codecell${index}`

const clipboardButton = id =>
  `<a id="copy-button-${id}" class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  const codeCells = document.querySelectorAll('div.c-textbook__content > div.highlighter-rouge > div.highlight > pre, div.input_area pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("copy-button" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>


  <!-- Hide cell code -->
  
<script>
/**
Add buttons to hide code cells
*/


var setCodeCellVisibility = function(inputField, kind) {
    // Update the image and class for hidden
    var id = inputField.getAttribute('data-id');
    var codeCell = document.querySelector(`#${id} div.highlight`);

    if (kind === "visible") {
        codeCell.classList.remove('hidden');
        inputField.checked = true;
    } else {
        codeCell.classList.add('hidden');
        inputField.checked = false;
    }
}

var toggleCodeCellVisibility = function (event) {
    // The label is clicked, and now we decide what to do based on the input field's clicked status
    if (event.target.tagName === "LABEL") {
        var inputField = event.target.previousElementSibling;
    } else {
        // It is the span inside the target
        var inputField = event.target.parentElement.previousElementSibling;
    }

    if (inputField.checked === true) {
        setCodeCellVisibility(inputField, "visible");
    } else {
        setCodeCellVisibility(inputField, "hidden");
    }
}


// Button constructor
const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

var addHideButton = function () {
  // If a hide button is already added, don't add another
  if (document.querySelector('div.hidecode input') !== null) {
      return;
  }

  // Find the input cells and add a hide button
  document.querySelectorAll('div.input_area').forEach(function (item, index) {
    if (!item.classList.contains("hidecode")) {
        // Skip the cell if it doesn't have a hidecode class
        return;
    }

    const id = codeCellId(index)
    item.setAttribute('id', id);
    // Insert the button just inside the end of the next div
    item.querySelector('div').insertAdjacentHTML('beforeend', hideCodeButton(id))

    // Set up the visibility toggle
    hideLink = document.querySelector(`#${id} div.highlight + input + label`);
    hideLink.addEventListener('click', toggleCodeCellVisibility)
  });
}


// Initialize the hide buttos
var initHiddenCells = function () {
    // Add hide buttons to the cells
    addHideButton();

    // Toggle the code cells that should be hidden
    document.querySelectorAll('div.hidecode input').forEach(function (item) {
        setCodeCellVisibility(item, 'hidden');
        item.checked = true;
    })
}

initFunction(initHiddenCells);

</script>


  <!-- Load custom website scripts -->
  <script src="/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  
<script>
/**
  * To auto-embed hub URLs in interact links if given in a RESTful fashion
 */

function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}
    
function dict2param(dict) {
    params = Object.keys(dict).map(function(k) {
        return encodeURIComponent(k) + '=' + encodeURIComponent(dict[k])
    });
    return params.join('&')
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return dict2param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = document.querySelectorAll("a").forEach(function(link) {
    var href = link.href;
    // If the link is an internal link...
    if (href.search("http://0.0.0.0:4000") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['jupyterhub'] = hub;
      } else {
        // Create the REST params
        params = {'jupyterhub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + dict2param(params);
      link.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}


// Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    jupyterHubUrl = rest['jupyterhub'];
    var hubType = null;
    var hubUrl = null;
    if (jupyterHubUrl !== undefined) {
      hubType = 'jupyterhub';
      hubUrl = jupyterHubUrl;
    }

    if (hubType !== null) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);

      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      var interactButtons = document.querySelectorAll("button.interact-button")
      var lastButton = interactButtons[interactButtons.length-1];
      var link = lastButton.parentElement;

      // If we've already run this, skip the link updating
      if (link.nextElementSibling !== null) {
        return;
      }

      // Update the link and add context div
      var href = link.getAttribute('href');
      if (lastButton.id === 'interact-button-binder') {
        // If binder links exist, we need to re-work them for jupyterhub
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // If localhost, assume we're working from a local Jupyter server and remove `/hub`
          first = [hubUrl, 'git-sync'].join('/')
        } else {
          first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
        }
        href = first + '?' + binder2Jupyterhub(href);
      } else {
        // If interact button isn't binderhub, assume it's jupyterhub
        // If JupyterHub links, we only need to replace the hub url
        href = href.replace("", hubUrl);
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // Assume we're working from a local Jupyter server and remove `/hub`
          href = href.replace("/hub/user-redirect", "");
        }
      }
      link.setAttribute('href', decodeURIComponent(href));

      // Add text after interact link saying where we're launching
      hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
      link.insertAdjacentHTML('afterend', '<div class="interact-context">on ' + hubUrlNoHttp + '</div>');

      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

runWhenDOMLoaded(updateInteractLink)
document.addEventListener('turbolinks:load', updateInteractLink)
</script>


  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="/assets/js/lunr/lunr.min.js" type="text/javascript"></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>
</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  <a href="https://jupyter.org/jupyter-book/"><img src="/images/logo/ML.jpg" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/></a>
  <h2 class="c-sidebar__title">吴恩达机器学习入门</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/01-1.html"
        >
          
          目录
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="http://www.momodel.cn:8899/classroom"
        >
          
          GitHub repository
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__chapter"><a class="c-sidebar__entry" href="/search.html">查询</a></li>
        
      
      
        <li class="c-sidebar__divider"></li>
        
      
      
        <li><h2 class="c-sidebar__title">书籍内容</li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第一章 引言(Introduction)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/01/01-2.html"
                >
                  
                  01-2 机器学习简介
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/01/01-3.html"
                >
                  
                  01-3 什么是机器学习
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/01/01-4.html"
                >
                  
                  01-4 监督学习
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/01/01-5.html"
                >
                  
                  01-5 非监督学习
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第二章 单变量线性回归(Linear Regression with One Variable)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/02/02-1.html"
                >
                  
                  02-1 模型表示
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/02/02-2.html"
                >
                  
                  02-2 代价函数
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/02/02-3.html"
                >
                  
                  02-3 理解代价函数 - I
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/02/02-4.html"
                >
                  
                  02-4 理解代价函数 - II
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/02/02-5.html"
                >
                  
                  02-5 梯度下降
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/02/02-6.html"
                >
                  
                  02-6 理解梯度下降
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/02/02-7.html"
                >
                  
                  02-7 线性回归中的梯度下降
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第三章 线性代数回顾(Linear Algebra Review)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/03/03-1.html"
                >
                  
                  03-1 矩阵和向量
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/03/03-2.html"
                >
                  
                  03-2 加法和乘法
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/03/03-3.html"
                >
                  
                  03-3 向量乘法
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/03/03-4.html"
                >
                  
                  03-4 矩阵乘法
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/03/03-5.html"
                >
                  
                  03-5 矩阵乘法特性
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/03/03-6.html"
                >
                  
                  03-6 矩阵转置与求逆
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第四章 多变量线性回归(Linear Regression with Multiple Variables)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/04/04-1.html"
                >
                  
                  04-1 多变量线性回归
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/04/04-2.html"
                >
                  
                  04-2 多变量梯度下降
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/04/04-3.html"
                >
                  
                  04-3 梯度下降 - 特征缩放
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/04/04-4.html"
                >
                  
                  04-4 梯度下降 - 学习率
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/04/04-5.html"
                >
                  
                  04-5 特征与多项式回归
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/04/04-6.html"
                >
                  
                  04-6 正规方程
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/04/04-7.html"
                >
                  
                  04-7 正规方程不可逆的情况
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第五章 逻辑回归(Logistic Regression)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/05/05-1.html"
                >
                  
                  05-1 分类
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/05/05-2.html"
                >
                  
                  05-2 假设函数表达式
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/05/05-3.html"
                >
                  
                  05-3 决策边界
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/05/05-4.html"
                >
                  
                  05-4 代价函数 I
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/05/05-5.html"
                >
                  
                  05-5 简化的代价函数和梯度下降
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/05/05-6.html"
                >
                  
                  05-6 高级优化技巧
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/05/05-7.html"
                >
                  
                  05-7 多分类任务
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第六章 正则化(Regularization)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/06/06-1.html"
                >
                  
                  06-1 过拟合问题
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/06/06-2.html"
                >
                  
                  06-2 代价函数 II
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/06/06-3.html"
                >
                  
                  06-3 正则化与线性回归
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/06/06-4.html"
                >
                  
                  06-4 正则化与逻辑回归
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第七章 神经网络-表述(Neural Networks-Representation)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/07/07-1.html"
                >
                  
                  07-1 非线性假设
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/07/07-2.html"
                >
                  
                  07-2 神经元与大脑
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/07/07-3.html"
                >
                  
                  07-3 模型表示 I
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/07/07-4.html"
                >
                  
                  07-4 模型表示 II
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/07/07-5.html"
                >
                  
                  07-5 模型表示实例 I
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/07/07-6.html"
                >
                  
                  07-6 模型表示实例 II
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/07/07-7.html"
                >
                  
                  07-7 多分类任务
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第八章 神经网络的学习(Neural Networks-Learning)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/08/08-1.html"
                >
                  
                  08-1 代价函数 III
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/08/08-2.html"
                >
                  
                  08-2 反向传播
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/08/08-3.html"
                >
                  
                  08-3 理解反向传播
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/08/08-4.html"
                >
                  
                  08-4 实现参数展开
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/08/08-5.html"
                >
                  
                  08-5 梯度校验
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/08/08-6.html"
                >
                  
                  08-6 随机初始化
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/08/08-7.html"
                >
                  
                  08-7 把前面的内容放在一起
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/08/08-8.html"
                >
                  
                  08-8 自动驾驶
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第九章 应用机器学习的建议(Advice for Applying Machine Learning)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/09/09-1.html"
                >
                  
                  09-1 决定下一步做什么
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/09/09-2.html"
                >
                  
                  09-2 假设检验
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/09/09-3.html"
                >
                  
                  09-3 模型选择与训练集、验证集、测试集切分
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/09/09-4.html"
                >
                  
                  09-4 诊断偏差和方差
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/09/09-5.html"
                >
                  
                  09-5 正则化、偏差和方差
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/09/09-6.html"
                >
                  
                  09-6 学习曲线
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/09/09-7.html"
                >
                  
                  09-7 决定下一步做什么(回顾)
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第十章 机器学习系统的设计(Machine Learning System Design)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/10/10-1.html"
                >
                  
                  10-1 优先考虑什么
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/10/10-2.html"
                >
                  
                  10-2 误差分析
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/10/10-3.html"
                >
                  
                  10-3 误差矩阵
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/10/10-4.html"
                >
                  
                  10-4 精确率和召回率的权衡
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/10/10-5.html"
                >
                  
                  10-5 机器学习的数据
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第十一章 支持向量机(Support Vector Machines)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/11/11-1.html"
                >
                  
                  11-1 优化问题
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/11/11-2.html"
                >
                  
                  11-2 直观理解大间距分类器
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/11/11-3.html"
                >
                  
                  11-3 大间距分类器背后的数学原理 (选学)
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/11/11-4.html"
                >
                  
                  11-4 核技巧 I
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/11/11-5.html"
                >
                  
                  11-5 核技巧 II
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/11/11-6.html"
                >
                  
                  11-6 使用 SVM
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第十二章 聚类(Clustering)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/12/12-1.html"
                >
                  
                  12-1 非监督学习介绍
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/12/12-2.html"
                >
                  
                  12-2 K-Means 算法
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/12/12-3.html"
                >
                  
                  12-3 优化目标
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/12/12-4.html"
                >
                  
                  12-4 随机初始化 II
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/12/12-5.html"
                >
                  
                  12-5 选择聚类数目
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第十三章 降维(Dimensionality Reduction)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/13/13-1.html"
                >
                  
                  13-1 用途 I 数据压缩
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/13/13-2.html"
                >
                  
                  13-2 用途 II 可视化
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/13/13-3.html"
                >
                  
                  13-3 主成分分析问题
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/13/13-4.html"
                >
                  
                  13-4 主成分分析算法
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/13/13-5.html"
                >
                  
                  13-5 选择主成分数目
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/13/13-6.html"
                >
                  
                  13-6 压缩表示重建
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/13/13-7.html"
                >
                  
                  13-7 使用 PCA 的建议
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第十四章 异常检测(Anomaly Detection)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/14/14-1.html"
                >
                  
                  14-1 问题驱动
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/14/14-2.html"
                >
                  
                  14-2 高斯分布
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/14/14-3.html"
                >
                  
                  14-3 算法
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/14/14-4.html"
                >
                  
                  14-4 开发并评估一个异常检测系统
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/14/14-5.html"
                >
                  
                  14-5 异常检测 vs 监督学习
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/14/14-6.html"
                >
                  
                  14-6 选择合适的特征
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/14/14-7.html"
                >
                  
                  14-7 多元高斯分布
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/14/14-8.html"
                >
                  
                  14-8 使用高斯分布进行异常检测
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第十五章 推荐系统(Recommender Systems)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/15/15-1.html"
                >
                  
                  15-1 问题公式化
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/15/15-2.html"
                >
                  
                  15-2 基于内容的推荐问题
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/15/15-3.html"
                >
                  
                  15-3 协同过滤
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/15/15-4.html"
                >
                  
                  15-4 协同过滤算法
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/15/15-5.html"
                >
                  
                  15-5 向量化
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/15/15-6.html"
                >
                  
                  15-6 平均标准化的实现细节
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第十六章 大规模机器学习(Large Scale Machine Learning)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/16/16-1.html"
                >
                  
                  16-1 基于大数据的学习
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/16/16-2.html"
                >
                  
                  16-2 随机梯度下降
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/16/16-3.html"
                >
                  
                  16-3 小批量梯度下降
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/16/16-4.html"
                >
                  
                  16-4 随机梯度下降的收敛性
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/16/16-5.html"
                >
                  
                  16-5 在线学习
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/16/16-6.html"
                >
                  
                  16-6 Map Reduce 和数据并行
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href=".html"
        >
          
          第十七章 应用实例-图片文字识别(Application Example-Photo OCR)
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/17/17-1.html"
                >
                  
                  17-1 问题描述与管道
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/17/17-2.html"
                >
                  
                  17-2 滑动窗口
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/17/17-3.html"
                >
                  
                  17-3 获取海量数据和人工数据
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/17/17-4.html"
                >
                  
                  17-4 上限分析
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
  </ul>
  <p class="sidebar_footer">Powered by <a href="http://www.momodel.cn:8899/classroom">吴恩达机器学习入门</a></p>
</nav>

      
      <main class="c-textbook__page" tabindex="-1">
          <div class="o-wrapper">
            <div class="c-sidebar-toggle">
  <!-- We show the sidebar by default so we use .is-active -->
  <button
    id="js-sidebar-toggle"
    class="hamburger hamburger--arrowalt is-active"
  >
    <span class="hamburger-box">
      <span class="hamburger-inner"></span>
    </span>
    <span class="c-sidebar-toggle__label">Toggle Sidebar</span>
  </button>
</div>

            

            <div class="c-textbook__content">
              <div class="search-content__inner-wrap">
    <input type="text" id="lunr_search" class="search-input" tabindex="-1" placeholder="'Enter your search term...''" />
    <div id="results" class="results"></div>
</div>

<script>
    // Add the lunr store since we will now search it
    var store = [{
        "title": "目录",
        
        "excerpt":
            "吴恩达教授机器学习课程指引   我们马上就要开始进行机器学习课程的学习了, 是不是很开心, 因为我们即将成为世界最前沿的技术中的一员   欲速则不达, 我们先来看看课程的结构, 确定接下来的学习方向   1、 引言(Introduction)   1.1 欢迎   1.2 机器学习是什么？   1.3 监督学习   1.4 无监督学习   2、单变量线性回归(Linear Regression with One Variable)   2.1 模型表示   2.2 代价函数   2.3 代价函数的直观理解I   2.4 代价函数的直观理解II   2.5 梯度下降   2.6 梯度下降的直观理解   2.7 梯度下降的线性回归   2.8 接下来的内容   3、线性代数回顾(Linear Algebra Review)   3.1 矩阵和向量   3.2 加法和标量乘法   3.3 矩阵向量乘法   3.4 矩阵乘法   3.5 矩阵乘法的性质   3.6 逆、转置   4、多变量线性回归(Linear Regression with Multiple Variables)   4.1 多维特征   4.2 多变量梯度下降   4.3 梯度下降法实践1-特征缩放   4.4 梯度下降法实践2-学习率   4.5 特征和多项式回归   4.6 正规方程   4.7 正规方程及不可逆性（选修）   5、逻辑回归(Logistic Regression)   5.1 分类问题   5.2 假说表示   5.3 判定边界   5.4 代价函数   5.5 简化的成本函数和梯度下降   5.6 高级优化   5.7 多类别分类：一对多   6、正则化(Regularization)   6.1 过拟合的问题   6.2 代价函数   6.3 正则化线性回归   6.4 正则化的逻辑回归模型   7、神经网络：表述(Neural Networks: Representation)   7.1 非线性假设   7.2 神经元和大脑   7.3 模型表示1   7.4 模型表示2   7.5 样本和直观理解1   7.6 样本和直观理解II   7.7 多类分类   8、神经网络的学习(Neural Networks: Learning)   8.1 代价函数   8.2 反向传播算法   8.3 反向传播算法的直观理解   8.4 实现注意：展开参数   8.5 梯度检验   8.6 随机初始化   8.7 综合起来   8.8 自主驾驶   9、应用机器学习的建议(Advice for Applying Machine Learning)   9.1 决定下一步做什么   9.2 评估一个假设   9.3 模型选择和交叉验证集   9.4 诊断偏差和方差   9.5 正则化和偏差/方差   9.6 学习曲线   9.7 决定下一步做什么   10、机器学习系统的设计(Machine Learning System Design)   10.1 首先要做什么   10.2 误差分析   10.3 类偏斜的误差度量   10.4 查准率和查全率之间的权衡   10.5 机器学习的数据   11、支持向量机(Support Vector Machines)   11.1 优化目标   11.2 大边界的直观理解   11.3 数学背后的大边界分类（选修）   11.4 核函数1   11.5 核函数2   11.6 使用支持向量机   12、聚类(Clustering)   12.1 无监督学习：简介   12.2 K-均值算法   12.3 优化目标   12.4 随机初始化   12.5 选择聚类数   13、降维(Dimensionality Reduction)   13.1 动机一：数据压缩   13.2 动机二：数据可视化   13.3 主成分分析问题   13.4 主成分分析算法   13.5 选择主成分的数量   13.6 重建的压缩表示   13.7 主成分分析法的应用建议   14、异常检测(Anomaly Detection)   14.1 问题的动机   14.2 高斯分布   14.3 算法   14.4 开发和评价一个异常检测系统   14.5 异常检测与监督学习对比   14.6 选择特征   14.7 多元高斯分布（选修）   14.8 使用多元高斯分布进行异常检测（选修）   15、推荐系统(Recommender Systems)   15.1 问题形式化   15.2 基于内容的推荐系统   15.3 协同过滤   15.4 协同过滤算法   15.5 向量化：低秩矩阵分解   15.6 推行工作上的细节：均值归一化   16、大规模机器学习(Large Scale Machine Learning)   16.1 大型数据集的学习   16.2 随机梯度下降法   16.3 小批量梯度下降   16.4 随机梯度下降收敛   16.5 在线学习   16.6 映射化简和数据并行   17、应用实例：图片文字识别(Application Example: Photo OCR)   17.1 问题描述和流程图   17.2 滑动窗口   17.3 获取大量数据和人工数据   17.4 上限分析：哪部分管道的接下去做   18、总结(Conclusion)   18.1 总结和致谢   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/01-1.html",
        "teaser":null},{
        "title": "第一章 引言",
        
        "excerpt":
            "欢迎   这一节我们主要学习 - 0m5s      什么是机器学习   机器学习能用来做什么   机器学习是什么 - 0m10s   机器学习是目前信息技术中最激动人心的方向之一。   应用举例：     各类内容网站对用户进行推荐   搜索网站谷歌、必应使用学习算法来进行网页排序   Facebook或苹果使用图片分类算法来认出你朋友的照片   电子邮件垃圾邮件服务器通过学习算法过滤大量的垃圾邮件      机器学习，致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。     机器学习研究的主要内容，是关于在计算机上从数据中产生“模型”（model）的算法，即“学习算法”（learning algorithm）。    我们的目标就是想去实现这一些算法, 并将它运用于我们的生活中, 是不是想想就有点激动呢   机器学习能用来做什么 - 2m54s   为什么机器学习如此受欢迎呢？原因是，机器学习不只是用于人工智能领域，现在它涉及到各个行业和基础科学中。   一些案例：     数据挖掘：大量的硅谷公司正在收集web上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务。   医疗记录：电子医疗记录可以将医疗记录变成医学知识，帮助人们更好地理解疾病   计算生物学：因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组。   机械应用：无人直升机   手写识别   自然语言处理   计算机视觉、图像理解   自定制程序   在这门课中，你还讲学习到关于机器学习的前沿状况。因为事实上只了解算法、数学并不能解决你关心的实际的问题。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/01-2.html",
        "teaser":null},{
        "title": "第一章 引言",
        
        "excerpt":
            "什么是机器学习   这一节我们主要学习 - 0m5s      尝试定义机器学习   何时会使用机器学习   欢迎来到机器学习 - 0m20s     第一个机器学习的定义来自于 Arthur Samuel。他定义机器学习为            在进行特定编程的情况下，给予计算机学习能力的领域。            Arthur Samuel的西洋棋实验 - 0m40s   &lt;img src=’https://i.loli.net/2018/11/29/5bffdd2e18d54.png’ width=200 height=200 &gt;      近一些的定义由 卡内基梅隆大学 的 Tom Mitchell 提出，一个好的学习问题定义如下，      一个程序被认为能从经验 E 中学习，解决任务T，达到性能度量值P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。    下面我们一起来完成吴恩达教授提出的问题   以下哪一项是垃圾邮件辨别系统的任务 - 2m40s   &lt;img src=’https://i.loli.net/2018/11/29/5bffddb2da07f.png’ width=884 height=488 &gt;   查看答案   何时会使用机器学习   术业有专攻, 我们需要对算法进行分类, 才能在不同的场景使用正确的算法去解决问题   学习算法可以有以下几种分类 - 4m20s      监督学习   无监督学习   强化学习   最重要的是知道如何使用这些机器学习算法的，了解如何设计和构建机器学习和人工智能系统  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/01-3.html",
        "teaser":null},{
        "title": "第一章 引言",
        
        "excerpt":
            "监督学习   这一节我们主要学习 - 0m5s      监督学习   回归问题   分类问题   最典型的区别 - 0m10s   监督学习（Supervised Learning)   根据训练数据是否拥有标记信息，学习任务可大致被分为两类：监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）   监督学习的代表是回归和分类。    无监督学习的代表是聚类。   回归（Regression）   波士顿房价案例 - 0m20s   回归问题：预测连续值的模型。   下面是一个回归问题的最简单例子，房价的推断：      这是一些房价数据，横轴表示房子的面积，单位是平方英尺，纵轴表示房价，单位是千美元。问题是基于这组数据，假如你有一个朋友，他有一套 750 平方英尺房子，现在他希望把房子卖掉，他想知道这房子能卖多少钱？   &lt;img src=’https://i.loli.net/2018/11/29/5bffe12d89096.png’ width=617 height=308&gt;   将数据可视化表现出来，既是图中的“红叉”。   应用学习算法，可以在这组数据中拟合一条直线（紫红色线），根据这条线可以推测出，这套房子可能卖150,000美元。   当然这不是唯一的算法。可能还有更好的，比如不用直线拟合这些数据，用二次方程去拟合可能效果会更好。根据二次方程的蓝色曲线，可以从这个点推测出，这套房子能卖接近200,000美元。   稍后我们将讨论如何选择学习算法，如何决定用直线还是二次方程来拟合，来使得预测合理。   分类（Classification）   乳腺癌案例 - 3m0s   分类问题：预测离散值的模型。   下面是一个分类问题的简单例子，通过查看病历来推测乳腺癌良性与否：     数据集如下：横轴表示肿瘤的大小，纵轴上， 1 和 0 分别表示是或者不是恶性肿瘤。如果是恶性则记为1，不是恶性（良性）记为 0。     问题是有一个朋友很不幸检查出乳腺肿瘤。假设说她的肿瘤大概这么大，那么能否估算出肿瘤是恶性的或是良性的概率？   &lt;img src=’https://i.loli.net/2018/11/29/5bffe611271c0.png’ width=493 height=325 &gt;   分类指的是，我们试着推测出离散的输出值：0（良性）或 1（恶性）。   而事实上在分类问题中，输出可能不止两个值。只有两个输出的被称为二分类问题（binary classification），多个输出被成为多分类（multi-class classification）   当需要处理的特征更多时，可以采用支持向量机等算法进行处理。   &lt;img src=’https://i.loli.net/2018/11/29/5bffe84cd0aa4.png’ width=740 height=327 &gt;   练习   现在来个小测验：假设你经营着一家公司，你想开发学习算法来处理这两个问题：     你有一大批同样的货物，想象一下，你有上千件货物等待出售，这时你想预测接下来的三个月能有多少营业额？   你有许多客户，这时你想写一个软件来检验每一个用户的账户。对于每一个账户，你要判断它们是否曾经被盗过？   那这两个问题，它们属于分类问题、还是回归问题?   查看答案  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/01-4.html",
        "teaser":null},{
        "title": "第一章 引言",
        
        "excerpt":
            "非监督学习   这一节我们主要学习 - 0m5s      无监督学习   编程语言的选择   聚类   无监督学习（Unsupervised Learning）   无监督学习和监督学习的区别 - 0m50s   根据训练数据是否拥有标记信息，学习任务可大致被分为两类：监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）   监督学习的代表是回归和分类。    无监督学习的代表是聚类。     &lt;img src=’https://i.loli.net/2018/11/29/5bffead5ad718.png’ width=617 height=295 &gt;   聚类（Clustering）   通过案例了解无监督学习 - 1m20s   针对无标签的数据集，聚类算法可能会把这些数据分成两个不同的簇。   聚类应用：     谷歌新闻分组   基因学的理解应用   无监督学习或聚集有着大量的应用。     组织大型计算机集群。大数据中心有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的据中心工作得更高效。   社交网络的分析。已知你朋友的信息，比如你经常发 email 的，或是你 Facebook 的朋友、谷歌+圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？   市场分割。许多公司有大型的数据库，存储消费者信息。通过检索这些顾客数据集，可以自动地发现市场分类并自动地把顾客划分到不同的细分市场中，这样才能自动并更有效地销售或不同的细分市场一起进行销售。   天文数据分析。这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。   编程语言的选择   Cocktail Party Problem - 6m10s   鸡尾酒宴问题（Cocktail Party Problem）：是在计算机语音识别领域的一个问题，当前语音识别技术已经可以以较高精度识别一个人所讲的话，但是当说话的人数为两人或者多人时，语音识别率就会极大的降低。   假设一个场景：现在是在个有些小的鸡尾酒宴中，放两个麦克风在房间中，记录两个人同时说话。因为这些麦克风在两个地方，离说话人的距离不同每个麦克风记录下不同的声音，虽然是同样的两个说话人，得到的是两份不同的录音。   &lt;img src=’https://i.loli.net/2018/11/29/5bfff18e98f9c.png’ width=617 height=430 &gt;   如何把录音中的两个声音区分开来，需要一个比较复杂的程序。但是，采用比较好的编程环境，我们可以用几行代码实现它。比如matlab或者octave。   它的学习模型的代码实现：   [W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');   该课程我们会使用Python语言进行算法实现, 我们是直接提供Python运行环境支持的, 你可以在这里直接进行运行, 达到边学边做的效果  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/01-5.html",
        "teaser":null},{
        "title": "01-2 机器学习简介",
        
        "excerpt":
            "欢迎   这一节我们主要学习      什么是机器学习   机器学习能用来做什么   机器学习是什么   机器学习是目前信息技术中最激动人心的方向之一。   应用举例：     各类内容网站对用户进行推荐   搜索网站谷歌、必应使用学习算法来进行网页排序   Facebook或苹果使用图片分类算法来认出你朋友的照片   电子邮件垃圾邮件服务器通过学习算法过滤大量的垃圾邮件      机器学习，致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。     机器学习研究的主要内容，是关于在计算机上从数据中产生“模型”（model）的算法，即“学习算法”（learning algorithm）。    我们的目标就是想去实现这一些算法, 并将它运用于我们的生活中, 是不是想想就有点激动呢   机器学习能用来做什么   为什么机器学习如此受欢迎呢？原因是，机器学习不只是用于人工智能领域，现在它涉及到各个行业和基础科学中。   一些案例：     数据挖掘：大量的硅谷公司正在收集web上的单击数据，也称为点击流数据，并尝试使用机器学习算法来分析数据，更好的了解用户，并为用户提供更好的服务。   医疗记录：电子医疗记录可以将医疗记录变成医学知识，帮助人们更好地理解疾病   计算生物学：因为自动化技术，生物学家们收集的大量基因数据序列、DNA序列和等等，机器运行算法让我们更好地了解人类基因组。   机械应用：无人直升机   手写识别   自然语言处理   计算机视觉、图像理解   自定制程序   在这门课中，你还讲学习到关于机器学习的前沿状况。因为事实上只了解算法、数学并不能解决你关心的实际的问题。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/01/01-2.html",
        "teaser":null},{
        "title": "01-3 什么是机器学习",
        
        "excerpt":
            "什么是机器学习   这一节我们主要学习      尝试定义机器学习   何时会使用机器学习   欢迎来到机器学习      第一个机器学习的定义来自于 Arthur Samuel。他定义机器学习为            在进行特定编程的情况下，给予计算机学习能力的领域。            Arthur Samuel的西洋棋实验         近一些的定义由 卡内基梅隆大学 的 Tom Mitchell 提出，一个好的学习问题定义如下，      一个程序被认为能从经验 E 中学习，解决任务T，达到性能度量值P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。    下面我们一起来完成吴恩达教授提出的问题   1.以下哪一项是垃圾邮件辨别系统的任务      查看答案   何时会使用机器学习   术业有专攻, 我们需要对算法进行分类, 才能在不同的场景使用正确的算法去解决问题   学习算法可以有以下几种分类&lt;/span&gt;      监督学习   无监督学习   强化学习   最重要的是知道如何使用这些机器学习算法的，了解如何设计和构建机器学习和人工智能系统  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/01/01-3.html",
        "teaser":null},{
        "title": "01-4 监督学习",
        
        "excerpt":
            "监督学习   这一节我们主要学习      监督学习   回归问题   分类问题   最典型的区别   监督学习（Supervised Learning)   根据训练数据是否拥有标记信息，学习任务可大致被分为两类：监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）   监督学习的代表是回归和分类。    无监督学习的代表是聚类。   回归（Regression）   波士顿房价案例   回归问题：预测连续值的模型。   下面是一个回归问题的最简单例子，房价的推断：      这是一些房价数据，横轴表示房子的面积，单位是平方英尺，纵轴表示房价，单位是千美元。问题是基于这组数据，假如你有一个朋友，他有一套 750 平方英尺房子，现在他希望把房子卖掉，他想知道这房子能卖多少钱？      将数据可视化表现出来，既是图中的“红叉”。   应用学习算法，可以在这组数据中拟合一条直线（紫红色线），根据这条线可以推测出，这套房子可能卖150,000美元。   当然这不是唯一的算法。可能还有更好的，比如不用直线拟合这些数据，用二次方程去拟合可能效果会更好。根据二次方程的蓝色曲线，可以从这个点推测出，这套房子能卖接近200,000美元。   稍后我们将讨论如何选择学习算法，如何决定用直线还是二次方程来拟合，来使得预测合理。   分类（Classification）   乳腺癌案例   分类问题：预测离散值的模型。   下面是一个分类问题的简单例子，通过查看病历来推测乳腺癌良性与否：     数据集如下：横轴表示肿瘤的大小，纵轴上， 1 和 0 分别表示是或者不是恶性肿瘤。如果是恶性则记为1，不是恶性（良性）记为 0。     问题是有一个朋友很不幸检查出乳腺肿瘤。假设说她的肿瘤大概这么大，那么能否估算出肿瘤是恶性的或是良性的概率？      分类指的是，我们试着推测出离散的输出值：0（良性）或 1（恶性）。   而事实上在分类问题中，输出可能不止两个值。只有两个输出的被称为二分类问题（binary classification），多个输出被成为多分类（multi-class classification）   当需要处理的特征更多时，可以采用支持向量机等算法进行处理。    练习   现在来个小测验：假设你经营着一家公司，你想开发学习算法来处理这两个问题：     你有一大批同样的货物，想象一下，你有上千件货物等待出售，这时你想预测接下来的三个月能有多少营业额？   你有许多客户，这时你想写一个软件来检验每一个用户的账户。对于每一个账户，你要判断它们是否曾经被盗过？   那这两个问题，它们属于分类问题、还是回归问题?   查看答案  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/01/01-4.html",
        "teaser":null},{
        "title": "01-5 非监督学习",
        
        "excerpt":
            "非监督学习   这一节我们主要学习      无监督学习   编程语言的选择   聚类   无监督学习（Unsupervised Learning）   无监督学习和监督学习的区别   根据训练数据是否拥有标记信息，学习任务可大致被分为两类：监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）   监督学习的代表是回归和分类。    无监督学习的代表是聚类。      聚类（Clustering）   通过案例了解无监督学习   针对无标签的数据集，聚类算法可能会把这些数据分成两个不同的簇。   聚类应用：     谷歌新闻分组   基因学的理解应用   无监督学习或聚集有着大量的应用。     组织大型计算机集群。大数据中心有大型的计算机集群，他们想解决什么样的机器易于协同地工作，如果你能够让那些机器协同工作，你就能让你的据中心工作得更高效。   社交网络的分析。已知你朋友的信息，比如你经常发 email 的，或是你 Facebook 的朋友、谷歌+圈子的朋友，我们能否自动地给出朋友的分组呢？即每组里的人们彼此都熟识，认识组里的所有人？   市场分割。许多公司有大型的数据库，存储消费者信息。通过检索这些顾客数据集，可以自动地发现市场分类并自动地把顾客划分到不同的细分市场中，这样才能自动并更有效地销售或不同的细分市场一起进行销售。   天文数据分析。这些聚类算法给出了令人惊讶、有趣、有用的理论，解释了星系是如何诞生的。   编程语言的选择   Cocktail Party Problem   鸡尾酒宴问题（Cocktail Party Problem）：是在计算机语音识别领域的一个问题，当前语音识别技术已经可以以较高精度识别一个人所讲的话，但是当说话的人数为两人或者多人时，语音识别率就会极大的降低。   假设一个场景：现在是在个有些小的鸡尾酒宴中，放两个麦克风在房间中，记录两个人同时说话。因为这些麦克风在两个地方，离说话人的距离不同每个麦克风记录下不同的声音，虽然是同样的两个说话人，得到的是两份不同的录音。      如何把录音中的两个声音区分开来，需要一个比较复杂的程序。但是，采用比较好的编程环境，我们可以用几行代码实现它。比如matlab或者octave。   它的学习模型的代码实现：   [W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');   该课程我们会使用Python语言进行算法实现, 我们是直接提供Python运行环境支持的, 你可以在这里直接进行运行, 达到边学边做的效果  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/01/01-5.html",
        "teaser":null},{
        "title": "第一章 引言",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/01/features.html",
        "teaser":null},{
        "title": "第二章 单变量线性回归",
        
        "excerpt":
            "代价函数   这一节我们主要学习以下内容 - 0m5s      定义代价函数   代价函数（Cost Function   上一节我们已经完成了模型构建：   &lt;img src=’https://i.loli.net/2018/11/30/5c00c682e4350.png’ width=493 height=271 &gt;   在线性回归中我们有一个像这样的训练集，m 代表了训练样本的数量，比如 m = 47。   假设函数模型为：线性函数模型 $h_{0}(x) = \\theta_{0} + \\theta_{1}x$ 。   本节要做的是为模型选择合适的参数（parameters） $\\theta_{0}$和 $\\theta_{1}$，在房价问题这个例子中便是直线的斜率和在 y 轴上的截距。   参数决定了得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差（modeling error）。   &lt;img src=’https://i.loli.net/2018/11/29/5bfffef3551cc.png’ width=493 height=330 &gt;   我们的目标便是选择出可以使得建模的均方误差能够最小的模型参数。   即使得代价函数最小。   下一节我们将详细理解代价函数。   牛刀小试   Todo:了解一下什么是均方误差?写下你的理解。   查看答案   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/02-2.html",
        "teaser":null},{
        "title": "第二章 单变量线性回归",
        
        "excerpt":
            "理解代价函数   这一节我们主要学习以下内容 - 0m5s      代价函数的直观理解 I   **代价函数的直观理解 I **   本节强烈建议观看视频理解，以下仅列出关键步骤。   回顾上文，我们已经得到了单线性回归的模型：   hypothesis(假设):             $h_{0}(x) = \\theta_{0} + \\theta_{1}x$   parameters（参数）:      $\\theta_{0}$ , $\\theta_{1}$   通过选择不同的参数,会得到不同的直线拟合.   Cost Function(代价函数）:     $J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m}\\sum_{i=1}^m(h_{0}(x^{(i)})-y^{(i)})^{2}$   Goal:     $minimize_{\\theta_{0},\\theta_{1}}J(\\theta_{0}, \\theta_{1})$   我们的优化目标是,最小化代价函数   为理解方便，本节讲解中将h函数简化为$h_{0}(x) =\\theta_{1}x$，即设$\\theta_{0}=0$   首先明确一个概念：     在假设h中，$\\theta_{1}$是一个固定的参数，只有$x$才是自变量。因变量为预测值 $h_{0}(x)$   在优化函数$J(\\theta)$中，$\\theta_{1}$是自变量，因变量为预测值$h_{0}(x)$与真实值$y$的误差$J(\\theta)$   从这里开始,用实例来说明,不同的 $\\theta_{1}$ , 代价函数的值.建议查看视频   理解代价函数 - I - 1m40s   由此我们可以画出假设h和优化函数$J(\\theta)$对应的函数图像。   &lt;img src=’https://i.loli.net/2018/12/04/5c0652fe1637f.png’ width=617 height=345 &gt;   牛刀小试   Todo: 当 $\\theta_{1}$ 是 0 的时候, $J(\\theta)$ 的值是多少?   查看答案  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/02-3.html",
        "teaser":null},{
        "title": "第二章 单变量线性回归",
        
        "excerpt":
            "理解代价函数 II   这一节我们主要学习以下内容 - 0m5s      代价函数的直观理解 II -等高线   代价函数的直观理解 II -等高线   本节强烈建议观看视频理解，以下仅列出关键步骤。   本节中，我们将结合等高线图，更深入地学习代价函数的作用。   照例先给出单线性回归模型： hypothesis(假设):             $h_{0}(x) = \\theta_{0} + \\theta_{1}x$   parameters（参数）:      $\\theta_{0}$ , $\\theta_{1}$   Cost Function(代价函数）:     $J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m}\\sum_{i=1}^m(h_{0}(x^{(i)})-y^{(i)})^{2}$   Goal:     $minimize_{\\theta_{0},\\theta_{1}}J(\\theta_{0}, \\theta_{1})$   在上一节中，我们假设$\\theta_0=0$，仅考虑了$\\theta_1$, 得到的图像是一个弓形曲线：   &lt;img src=’https://i.loli.net/2018/12/05/5c07dbf999eef.png’ width=300 &gt;   如果我们考虑$[\\theta_0, \\theta_1]$两个参数，得到的图像则如下：   理解代价函数 - II - 2m0s   &lt;img src=’https://i.loli.net/2018/12/04/5c0653fdcfe29.png’ width=350 &gt;   从代价函数的样子（等高线图）中可以看出在三维空间中存在一个使得 $J(\\theta_{0}, \\theta_{1})$最小的点。   下面为等高线图的二维图像，同一线圈上的 $J(\\theta_{0}, \\theta_{1})$取值相同。   理解代价函数 - II - 3m30s   &lt;img src=’https://i.loli.net/2018/12/04/5c065398af755.png’ width=500 &gt;   通过这些图形，我希望你能更好地理解这些代价函数$J$所表达的值是什么样的，它们对应的假设是什么样的，以及什么样的假设对应的点，更接近于代价函数 $J$ 的最小值。   当然，我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数$J$取最小值的参数 $\\theta_{0}$ 和 $\\theta_{1}$来。   我们也不希望编个程序把这些点画出来，然后人工的方法来读出这些点的数值，这很明显不是一个好办法。我们会遇到更复杂、更高维度、更多参数的情况，而这些情况是很难画出图的，因此更无法将其可视化，因此我们真正需要的是编写程序来找出这些最小化代价函数的 $\\theta_{0}$和$\\theta_{1}$的值。   在下一节视频中，我们将介绍一种算法，能够自动地找出能使代价函数$J$最小化的参数$\\theta_{0}$和$\\theta_{1}$的值。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/02-4.html",
        "teaser":null},{
        "title": "02-1 模型表示",
        
        "excerpt":
            "模型表示   这一节我们主要学习以下内容      单变量线性回归(Linear Regression with One Variable) 模型表示   监督学习的完整流程   单变量线性回归的假设函数建模   单变量线性回归 模型表示   模型表示   让我们通过一个例子来开始：     数据集: 俄勒冈州波特兰市的住房价格（面积，价格）。   问题：构建一个预测住房价格的模型，告知面积，给出最可能的售价。      它是一个典型的监督学习模型。因为对于每个数据，我们都给出了“正确的答案”，即告诉我们：根据我们的数据来说，房子实际的价格是多少，更具体来说，这是一个回归问题。   回归一词指的是，我们根据之前的数据预测出一个准确的输出值，对于这个例子就是价格。   在监督学习中我们有一个数据集，这个数据集被称训练集。   **整个课程中会小写的 m 来表示训练样本的数目。 **   以之前的房屋交易问题为例，假使我们回归问题的训练集（Training Set）如下表所示：      这个回归问题的标记如下:     $m$ 代表训练集中实例的数量      $x$ 代表特征/输入变量      $y$ 代表目标变量/输出变量      $(x^{(i)},y^{(i)})$ 代表训练集中的第i个实例      $h$ 代表学习算法的解决方案或函数也称为假设（hypothesis）    监督学习的完整流程   模型表示      这就是一个监督学习算法的工作方式。这里有训练集，把它喂给学习算法，学习算法工作，然后输出一个函数，通常表示为小写$h$（hypothesis，假设）。$h$表示一个函数，输入是房屋尺寸大小$x$，得到的输出值为$y$，对应房子的价格。因此，$h$是一个从x到y的函数映射。   假设函数的建模   模型表示   对于上述房价预测问题，我们该如何表达h？   一种可能的表达方式为：      因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/02/02-1.html",
        "teaser":null},{
        "title": "02-2 代价函数",
        
        "excerpt":
            "代价函数   代价函数（Cost Function   上一节我们已经完成了模型构建：      在线性回归中我们有一个像这样的训练集，m 代表了训练样本的数量，比如 m = 47。   假设函数模型为：线性函数模型 $h_{0}(x) = \\theta_{0} + \\theta_{1}x$ 。   本节要做的是为模型选择合适的参数（parameters） $\\theta_{0}$和 $\\theta_{1}$，在房价问题这个例子中便是直线的斜率和在 y 轴上的截距。   参数决定了得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差（modeling error）。      我们的目标便是选择出可以使得建模的均方误差能够最小的模型参数。   即使得代价函数最小。   下一节我们将详细理解代价函数。   牛刀小试   Todo:了解一下什么是均方误差?写下你的理解。   查看答案   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/02/02-2.html",
        "teaser":null},{
        "title": "02-3 理解代价函数 - I",
        
        "excerpt":
            "理解代价函数   代价函数的直观理解 I   本节强烈建议观看视频理解，以下仅列出关键步骤。   回顾上文，我们已经得到了单线性回归的模型：   hypothesis(假设):             $h_{0}(x) = \\theta_{0} + \\theta_{1}x$   parameters（参数）:      $\\theta_{0}$ , $\\theta_{1}$   通过选择不同的参数,会得到不同的直线拟合.   Cost Function(代价函数）:     $J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m}\\sum_{i=1}^m(h_{0}(x^{(i)})-y^{(i)})^{2}$   Goal:     $minimize_{\\theta_{0},\\theta_{1}}J(\\theta_{0}, \\theta_{1})$   我们的优化目标是,最小化代价函数   为理解方便，本节讲解中将h函数简化为$h_{0}(x) =\\theta_{1}x$，即设$\\theta_{0}=0$   首先明确一个概念：     在假设h中，$\\theta_{1}$是一个固定的参数，只有$x$才是自变量。因变量为预测值 $h_{0}(x)$   在优化函数$J(\\theta)$中，$\\theta_{1}$是自变量，因变量为预测值$h_{0}(x)$与真实值$y$的误差$J(\\theta)$   从这里开始,用实例来说明,不同的 $\\theta_{1}$ , 代价函数的值.建议查看视频理解代价函数   由此我们可以画出假设h和优化函数$J(\\theta)$对应的函数图像。      牛刀小试   Todo: 当 $\\theta_{1}$ 是 0 的时候, $J(\\theta)$ 的值是多少?   查看答案  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/02/02-3.html",
        "teaser":null},{
        "title": "02-4 理解代价函数 - II",
        
        "excerpt":
            "理解代价函数 II   代价函数的直观理解 II -等高线   本节强烈建议观看视频理解，以下仅列出关键步骤。   本节中，我们将结合等高线图，更深入地学习代价函数的作用。   照例先给出单线性回归模型： hypothesis(假设):             $h_{0}(x) = \\theta_{0} + \\theta_{1}x$   parameters（参数）:      $\\theta_{0}$ , $\\theta_{1}$   Cost Function(代价函数）:     $J(\\theta_{0}, \\theta_{1}) = \\frac{1}{2m}\\sum_{i=1}^m(h_{0}(x^{(i)})-y^{(i)})^{2}$   Goal:     $minimize_{\\theta_{0},\\theta_{1}}J(\\theta_{0}, \\theta_{1})$   在上一节中，我们假设$\\theta_0=0$，仅考虑了$\\theta_1$, 得到的图像是一个弓形曲线：      如果我们考虑$[\\theta_0, \\theta_1]$两个参数，得到的图像则如下：      从代价函数的样子（等高线图）中可以看出在三维空间中存在一个使得 $J(\\theta_{0}, \\theta_{1})$最小的点。   下面为等高线图的二维图像，同一线圈上的 $J(\\theta_{0}, \\theta_{1})$取值相同。   理解代价函数      通过这些图形，我希望你能更好地理解这些代价函数$J$所表达的值是什么样的，它们对应的假设是什么样的，以及什么样的假设对应的点，更接近于代价函数 $J$ 的最小值。   当然，我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数$J$取最小值的参数 $\\theta_{0}$ 和 $\\theta_{1}$来。   我们也不希望编个程序把这些点画出来，然后人工的方法来读出这些点的数值，这很明显不是一个好办法。我们会遇到更复杂、更高维度、更多参数的情况，而这些情况是很难画出图的，因此更无法将其可视化，因此我们真正需要的是编写程序来找出这些最小化代价函数的 $\\theta_{0}$和$\\theta_{1}$的值。   在下一节视频中，我们将介绍一种算法，能够自动地找出能使代价函数$J$最小化的参数$\\theta_{0}$和$\\theta_{1}$的值。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/02/02-4.html",
        "teaser":null},{
        "title": "02-5 梯度下降",
        
        "excerpt":
            "梯度下降   我们有函数 $J(\\theta_{0}, \\theta_{1})$, 我们不断的调整 $\\theta_{0}$ 和 $\\theta_{1}$, 来使得 $J(\\theta_{0}, \\theta_{1})$ 不断的变小, 直到 $J(\\theta_{0}, \\theta_{1})$ 达到最小值为止      梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\\theta_{0}, \\theta_{1})$的最小值。   梯度下降背后的思想是：开始时我们随机选择一个参数的组合$(\\theta_{0},\\theta_{1},……,\\theta_{n})$ ，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到抵达一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。      想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转 360 度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。   批量梯度下降（batch gradient descent）算法的公式为：      其中 α 是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。   细节注意：更新$\\theta_{0}$和 $\\theta_{1}$。   实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，你需要同时更新 $\\theta_{0}$和 $\\theta_{1}$，实现方法是：你应该计算公式右边的部分，通过那一部分计算出$\\theta_{0}$和 $\\theta_{1}$的值，然后同时更新$\\theta_{0}$和$\\theta_{1}$。      在梯度下降算法中，这是正确实现同时更新的方法。注意区别不正确的写法：      在接下来的视频中，我们要进入这个微分项的细节之中。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/02/02-5.html",
        "teaser":null},{
        "title": "02-6 理解梯度下降",
        
        "excerpt":
            "理解梯度下降   这一节我们主要学习以下内容      梯度下降的数学含义   梯度下降的更新过程   梯度下降算法如下：  $\\theta_{j} := \\theta_{j} - \\alpha\\frac{\\partial}{\\partial{\\theta_{j}}}J(\\theta)$   描述：对$\\theta$赋值，使得 $J(\\theta)$按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中$\\alpha$是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。   对于这个问题，求导的目的，基本上可以说取这个红点的切线，就是这样一条红色的直线，刚好与函数相切于这一点，这条直线的斜率正好是这个三角形的高度除以这个水平长度。现在，这条线有一个正斜率，也就是说它有正导数，因此，我得到的新的 $\\theta_{1}$，$\\theta_{1}$更新后等于 $\\theta_{1}$减去一个正数乘以$\\alpha$。      这就是梯度下降法的更新规则：    学习率$\\alpha$的选择   让我们来看看如果$\\alpha$太小或$\\alpha$太大会出现什么情况：      如果$\\alpha$太小或$\\alpha$太大会出现什么情况      如果$\\alpha$太小了，即我的学习速率太小，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。   如果$\\alpha$太大，那么梯度下降法可能会越过最低点，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，最终会导致无法收敛，甚至发散。   细节： 关于梯度下降的收敛   提问，如果我们预先把$\\theta_{1}$放在一个局部的最低点，你认为下一步梯度下降法会怎样工作？   假设你将$\\theta_{1}$初始化在局部最低点，它已经在一个局部的最优处。结果是局部最优点的导数将等于零，因为它是那条切线的斜率。这意味着你已经在局部最优点，它使得$\\theta_{1}$不再改变，也就是新的 $\\theta_{1}$等于原来的$\\theta_{1}$，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使学习速率$\\alpha$保持不变时，梯度下降也可以收敛到局部最低点。 我们来看一个例子，这是代价函数$J(\\theta)$。      想找到它的最小值，首先初始化梯度下降算法（品红色的点），如果更新一步梯度下降，也许它会带我到这个绿色的点，因为这个点的导数是相当陡的。现在再更新一步，你会发现导数也即斜率，是没那么陡的。随着接近最低点，导数越来越接近零。自然地，用一个稍微跟刚才在那个品红点时比再小一点的一步，从绿色点到了新的红色点，更接近全局最低点了，这点的导数会比在绿点时更小。当再进行一步梯度下降时，导数项是更小的，更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，已经收敛到局部极小值。   在梯度下降法中，当接近局部最低点时，梯度下降法会自动采取更小的幅度。这是因为当我们接近局部最低时，导数值会自动变得越来越小，在局部最低时导数等于零。所以梯度下降将自动采取较小的幅度，所以实际上没有必要再另外减小$\\alpha$。   这就是梯度下降算法，你可以用它来最小化任何代价函数$J$，不只是线性回归中的代价函数$J$。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/02/02-6.html",
        "teaser":null},{
        "title": "02-7 线性回归中的梯度下降",
        
        "excerpt":
            "线性回归中的梯度下降   这一节我们主要学习以下内容      结合梯度下降和代价函数，完成线性回归算法建模   线性回归中的梯度下降   梯度下降是很常用的算法，它被用在很多学习模型上，包括在不限于线性回归和逻辑回归等等。   在这段视频中，我们要将梯度下降应用于具体的拟合直线的线性回归算法里。   梯度下降算法和线性回归算法，如图：      对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：          则算法改写成：      线性回归中的梯度下降细节   我们刚刚使用的算法，被称为批量梯度下降。它指的是在梯度下降的每一步中，都用到了所有的训练样本，在计算微分求导项时，需要进行求和运算。所以，在每一个单独的梯度下降中，都需要对所有m个训练样本求和。而事实上，也有其他类型的梯度下降法每次只关注训练集中的一些小的子集，后面会介绍。   但就目前而言，应用刚刚学到的算法，你应该已经掌握了批量梯度算法，并且能把它应用到线性回归中了，这就是用于线性回归的梯度下降法。   线性代数中，有一种计算代价函数J最小值的数值解法，正规方程(normal equations)，不需要梯度下降这种迭代算法（后续会讲到）。但是处理大型数据集时，梯度下降是更好的方法。   牛刀小试   Todo: 批量梯度下降,在实际使用中会不会遇到什么问题呢?写下你的思考   查看答案   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/02/02-7.html",
        "teaser":null},{
        "title": "第一章 单变量线性回归",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/02/features.html",
        "teaser":null},{
        "title": "03-1 矩阵和向量",
        
        "excerpt":
            "矩阵与向量   如图：这个是 4×2 矩阵，即 4 行 2 列，如 m 为行，n 为列，那么 m×n 即 4×2         矩阵的维数(dimension)即行数(row)×列数(column)   矩阵元素(elements, entries): $A_{ij}$指第 i 行，第 j 列的元素。           # 在python中通常导入numpy包，进行矩阵操作 import numpy as np # 创建一个矩阵 a=np.array([[1, 2], [3, 4]]) print(\"a:\\n\", a)                         a:  [[1 2]  [3 4]]                  向量      向量是列数为1的特殊矩阵，如图：         向量元素: $y_{i}$指向量 y 第 i 行的元素。   如下图为1-索引向量和0-索引向量，无特指本课中使用1-索引向量。              # 创建一个矩阵 b = np.array(np.zeros((3,1))) print(\"b:\\n\", b)                         b:  [[0.]  [0.]  [0.]]                 ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/03/03-1.html",
        "teaser":null},{
        "title": "03-2 加法和乘法",
        
        "excerpt":
            "加法与标量乘法   矩阵的加法   行列数相等的才可以做加法，两个矩阵相加就是行列对应的元素相加。   我们看这个例子      牛刀小试           import numpy as np a = np.mat([[1,0],[2,5],[3,1]]) b = np.mat([[4,0.5],[2,5],[0,1]]) print (\"a: \\n\",a, \"\\nb: \\n\",b) print (\"a+b: \\n\",a+b)  # a + b，矩阵相加 print(\"a-b:\\n\",a-b)  # a-b, 矩阵相减                         a:   [[1 0]  [2 5]  [3 1]]  b:   [[4.  0.5]  [2.  5. ]  [0.  1. ]] a+b:   [[ 5.   0.5]  [ 4.  10. ]  [ 3.   2. ]] a-b:  [[-3.  -0.5]  [ 0.   0. ]  [ 3.   0. ]]                  矩阵的标量乘法   矩阵和标量的乘法也很简单,就是矩阵的每个元素都与标量相乘。              print (\"a: \\n\",a) print (\"3*a: \\n\",3* a)  #矩阵标量乘法                         a:   [[1 0]  [2 5]  [3 1]] 3*a:   [[ 3  0]  [ 6 15]  [ 9  3]]                 ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/03/03-2.html",
        "teaser":null},{
        "title": "03-3 向量乘法",
        
        "excerpt":
            "向量乘法   矩阵和向量的乘法如图：m×n 的矩阵乘以 n×1 的向量，得到的是 m×1 的向量   计算详情请直接查看视频               import numpy as np a = np.mat([[-1,2],[2,3]]) c = np.mat([[3],[4]]) print(\"向量a与c的成绩:\\n\",a*c)                         向量a与c的成绩:  [[ 5]  [18]]                  为什么要学习矩阵和向量的乘法呢？      假如我们有4个房子的面积数据，有一个 hypotheses，那么我们可以把这些房子的数据写成矩阵的形式，把 hypotheses 写成向量的形式，利用矩阵和向量乘法，即可快速进行计算了。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/03/03-3.html",
        "teaser":null},{
        "title": "03-4 矩阵乘法",
        
        "excerpt":
            "矩阵乘法   m×n 矩阵乘以 n×o 矩阵，变成 m×o 矩阵。      我们可以把它和我们上一节课所讲的矩阵和向量的乘法结合起来, 我们吧矩阵和矩阵的乘法拆解为多个矩阵和向量的乘法,然后再拼接起来,看下图              import numpy as np  a = np.mat([[-1,2],[2,3]]) b = np.mat([[3,4],[4,5]]) print (\"a: \\n\",a, \"\\nb: \\n\",b) print(\"a*b:\\n\",a*b)                         a:   [[-1  2]  [ 2  3]]  b:   [[3 4]  [4 5]] a*b:  [[ 5  6]  [18 23]]                  我们为什么要学习矩阵乘法？      假如我们有4个房子的面积数据，有三个 hypotheses，那么我们可以把这些数据写成矩阵的形式，利用矩阵乘法，即可快速进行计算了。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/03/03-4.html",
        "teaser":null},{
        "title": "03-5 矩阵乘法特性",
        
        "excerpt":
            "矩阵乘法的性质      矩阵的乘法不满足交换律：$A×B≠B×A$   矩阵的乘法满足结合律。即：$A×（B×C）=（A×B）×C$   单位矩阵   在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的 1,我们称这种矩阵为单位矩阵．它是个方阵，一般用 I 或者 E 表示，本讲义都用 I 代表单位矩阵，从左上角到右下角的对角线（称为主对角线）上的元素均为 1 以外全都为 0。如：      对于单位矩阵，有 $ AI = IA = A$。           import numpy as np  a = np.mat([[-1,2],[2,3]]) b = np.mat([[3,4],[4,5]]) print (\"a: \\n\",a, \"\\nb: \\n\",b)                         a:   [[-1  2]  [ 2  3]]  b:   [[3 4]  [4 5]]                          # 试试 a*b 和 b*a 是不是相同呢? print (\"a*b: \\n\",a*b) print (\"b*a: \\n\",b*a)                         a*b:   [[ 5  6]  [18 23]] b*a:   [[ 5 18]  [ 6 23]]                          c = np.mat([[1,3],[2,4]]) print (\"c: \\n\",c, )                         c:   [[1 3]  [2 4]]                          # 试试 a*b*c 和 a*(b*c) 是不是相同呢? print (\"a*b*c: \\n\",a*b*c) print (\"a*(b*c): \\n\",a*(b*c))                         a*b*c:   [[ 17  39]  [ 64 146]] a*(b*c):   [[ 17  39]  [ 64 146]]                 ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/03/03-5.html",
        "teaser":null},{
        "title": "03-6 矩阵转置与求逆",
        
        "excerpt":
            "矩阵转置与求逆   矩阵的逆   矩阵 A 是一个 m×m 矩阵（方阵），如果有逆矩阵，则：    没有逆矩阵的矩阵, 称为奇异 (singlar/degenerate)矩阵           import numpy as np  a = np.mat([[1,2],[3,4]]) print ('a:\\n',a)                          a:  [[1 2]  [3 4]]                          # 计算 矩阵 a 逆矩阵 res = np.linalg.inv(a) print('a inverse:\\n', res)                         a inverse:  [[-2.   1. ]  [ 1.5 -0.5]]                  矩阵的转置   设 A 为 m×n 阶矩阵（即 m 行 n 列），第 i 行 j 列的元素是 a(i,j)，即：A=a(i,j)  定义 A 的转置为这样一个 n×m 阶矩阵 B，满足 B=a(j,i)，即 b (i,j)=a (j,i)（B 的第 i 行第 j 列元素是 A 的第 j 行第 i 列元素），记 $A^T=B$。      直观来看，将 A 的所有元素绕着一条从第 1 行第 1 列元素出发的右下方 45 度的射线作镜面反转，即得到 A 的转置。           a = np.mat([[1,2],[3,4]]) print ('a:\\n',a)                         a:  [[1 2]  [3 4]]                          # todo: 计算 矩阵 a 转置 res = a.T print('a transpose:\\n', res)                         a transpose:  [[1 3]  [2 4]]                  矩阵的转置基本性质   $(A \\pm B)^T = A^T \\pm B^T $    $(A \\times B)^T = A^T \\times B^T $    $(A^T)^T = A $  $(KA)^T = KA^T $  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/03/03-6.html",
        "teaser":null},{
        "title": "第三章 线性代数回顾(Linear Algebra Review)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/03/features.html",
        "teaser":null},{
        "title": "04-1 多变量线性回归",
        
        "excerpt":
            "多变量线性回归      引入多种特征后的假设h模型   对于（面积，价格）的房价模型，现在我们对房价模型增加更多的特征 (房间数量, 楼层数量, 房屋年龄)：      增添更多特征后，我们引入一系列新的注释：      如上图的  $x^{(2)} = \\begin{bmatrix} 1416 \\ 3 \\ 2 \\ 40 \\end{bmatrix}$，表示第2个训练实例。   其中，$x^{(i)_j}$ 代表特征矩阵中第 i 行的第 j 个特征，也就是第 i 个训练实例的第 j 个特征。如 $x^{(2)}_2 = 3$。   支持多变量的假设 $h$ 表示为:       为方便表示，引入 $x_{0} = 1$，则公式转化为：  此时模型中的参数是一个 n+1 维的向量，任何一个训练实例也都是 n+1 维的向量，特征矩阵 X 的维度是 m*(n+1)。 因此公式可以简化为：  其中上标 T 代表矩阵转置。   牛刀小试   如果我们有 5 个特征,那么使用多项式回归, 我们有多少个 ${\\theta}$ 参数?   答：6个   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/04/04-1.html",
        "teaser":null},{
        "title": "04-2 多变量梯度下降",
        
        "excerpt":
            "多变量梯度下降   与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价 函数是所有建模误差的平方和，即：   其中：   我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。   相应地，多变量线性回归的模型如下图：      对梯度下降求导数后得到：      我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。   牛刀小试   Todo: 实现多变量的 cost function           # 代价函数的python代码实现 def Cost(X, y, theta):     inner = np.power(((X * theta.T) - y), 2)     return np.sum(inner) / (2 * len(X))           ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/04/04-2.html",
        "teaser":null},{
        "title": "04-3 梯度下降 - 特征缩放",
        
        "excerpt":
            "梯度下降 - 特征缩放      梯度下降中的特征缩放: 确保特征具有相近的尺度   在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这可以帮助梯度下降算法更快地收敛。   以房价问题为例，假设我们使用两个特征，房屋尺寸的值为 0-2000 平方英尺，而房间数量的值则是 0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图，能看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。   为什么尺度相差很大的两个特征做梯度下降的时候, 会需要非常多次的迭代?  原因是, 回顾我们上一节的多变量梯度更新的公式(下图), 我们在更新每一个 ${\\theta}$ 的时候, 使用的学习率 ${\\alpha}$ 都是同一个值, 但是对于尺度差距很大的 ${\\theta}$, 他们的梯度的尺度也会差距很大 (见下面的第一张等高线图), 可想而知, 如果我们要保证尺度较大的 ${\\theta}$ 的下降速度, ${\\alpha}$ 就会需要设置得很大, 这就会导致在小尺度的 ${\\theta}$ 上来回波动很难向最低点靠拢, 也就是说满足了一个 ${\\theta}$ 的学习效率就会导致另一个 ${\\theta}$ 的学习效率变低, 想要满足所有 ${\\theta}$ 上面的学习效率会变得很困难.         解决的方法是尝试将所有特征的尺度都尽量缩放到-1 到 1 之间。如图：      最简单的方法是令：   其中, $\\mu_n$是平均值，$s_n$是标准差。   牛刀小试   Todo:  $s_n$除了标准差,还能采用什么形式?   答：可以采用max-min的形式   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/04/04-3.html",
        "teaser":null},{
        "title": "04-4 梯度下降 - 学习率",
        
        "excerpt":
            "梯度下降 - 学习率      梯度下降中的学习率处理：如何选择正确的学习率来保证梯度下降正确进行   参数${\\theta}$的更新如下图, 在确保梯度下降在正确的运行的情况下, 如何选择合适的学习率呢?      梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，因此可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。      也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如 0.001）进行比较，但通常看上面这样的图表更好。   下面展示了两种不正常的代价函数变化的图:   下图的左上的图表, 代价函数的值不降反升   下图的左下的图表, 代价函数的值不断在上下震荡   通常这两种图出现的原因都很可能是学习率太大, 代价函数不断的越过最低点 (下图右边的图表), 可以尝试通过减小学习率 $\\alpha$ 来解决      梯度下降算法的每次迭代受到学习率的影响:      如果学习率$\\alpha$过小，则达到收敛所需的迭代次数会非常高；   如果学习率$\\alpha$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。      牛刀小试   Todo:  你会如何选择学习率$\\alpha$呢?   答：通常可以考虑尝试些学习率： 0.01，0.03，0.1，0.3，1，3，10   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/04/04-4.html",
        "teaser":null},{
        "title": "04-5 特征与多项式回归",
        
        "excerpt":
            "特征与多项式回归      多项式回归   在房价预测问题中， 假设我们的特征数据为$x_1=frontage（临街宽度）$，$x_2=depth（纵向深度）$。      线性回归建模为：   但是我们可以自己创建一个特征 $x = Area(面积) = frontage \\times depth$    再进行建模：  效果会好很多。   线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：   或者三次方模型：       通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：      从而将模型转化为线性回归模型。   根据函数图形特性，我们还可以使用：   或者：     注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。因为幂运算很容易拉大特征之间尺度的差距   牛刀小试   Todo:  观察二次方模型和三次方模型拟合曲线的差异,思考,n 次方模型的 n 越大越好吗?   答：不是的, 合理的选择 n 非常重要。   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/04/04-5.html",
        "teaser":null},{
        "title": "04-6 正规方程",
        
        "excerpt":
            "正规方程      正规方程   梯度下降与正规方程的比较   对于某些线性回归问题，正规方程方法是更好的解决方案。如：      正规方程是通过求解下面的方程, 求解代价函数的梯度等于 0, 来找出使得代价函数最小的参数的：    假设我们的训练集特征矩阵为 X（包含了$x_0=1$）并且我们的训练集结果为向量 y， 则利用正规方程解出向量   上标 T 代表矩阵转置，上标-1 代表矩阵的逆。设矩阵$A =X^TX$，则：$(X^TX)^{-1} = A^{-1}$   以下表的数据为例, 直接运用正规方程方法求解参数：      注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。（其实也可以使用，详细讨论在04-7）   梯度下降与正规方程的比较      总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\\theta$的替代方法。具体地说，只要特征变量的数量小于一万，通常使用标准方程法，而不使用梯度下降法。   随着我们要讲的学习算法越来越复杂，例如，分类算法的逻辑回归算法，我们会看到， 实际上对于那些算法，并不能使用正规方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为正规方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。   牛刀小试           # 正规方程的 python 实现 import numpy as np def normalEqn(X, y):     theta = np.linalg.inv(X.T@X)@X.T@y        #X.T@X 等价于 X.T.dot(X)     return theta            ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/04/04-6.html",
        "teaser":null},{
        "title": "04-7 正规方程不可逆的情况",
        
        "excerpt":
            "正规方程不可逆的情况      正规方程 ( normal equation )的不可逆情况   正规方程解的推导过程   我们要讲的问题如下：     对于$\\theta = (X^TX)^{-1}X^Ty$，当$X^TX$不可逆时该怎么办？           $X^TX$不可逆很少发生。  在Octave里，可逆矩阵求解逆使用inv()，不可逆矩阵求解伪逆使用pinv()。            使用大量的特征值的情况下，可能会导致矩阵$X^TX$的结果是不可逆的。     具体地说，在 m 小于或等于 n 的时候，例如，有 m 等于 10 个的训练样本也有 n 等于100 的特征数量。要找到适合的 ( n +1 ) 维参数矢量$\\theta$ ，这将会变成一个 101 维的矢量，尝试从 10 个训练样本中找到满足 101 个参数的值，这工作可能会让你花上一阵子时间，但这并不总是一个好主意。因为，正如我们所看到你只有 10 个样本，以适应这 100 或 101 个参数，数据还是有些少。 稍后我们将看到，如何使用小数据样本以得到这 100 或 101 个参数，通常，我们会使用一种叫做正则化的线性代数方法，通过删除某些特征或者是使用某些技术，来解决当 m 比n 小的时候的问题。即使你有一个相对较小的训练集，也可使用很多的特征来找到很多合适的参数。       解决方法：              看特征值里是否有一些多余的特征，像这些 $x_1$和$x_2$是线性相关的，互为线性函数。删除重复的特征将解决不可逆性的问题。如果特征数量实在太多，可以删除些用较少的特征来反映尽可能多内容，或者考虑使用正则化方法。            如果矩阵$X^TX$是不可逆的，（通常来说，不会出现这种情况），如果在 Octave 里，可以用伪逆函数 pinv ( ) 来实现。即使 X’X 的结果是不可逆的，但算法执行的流程是正确的。       总之，出现不可逆矩阵的情况极少发生，所以在大多数实现线性回归中，不应该过多的关注$X^TX$不可逆。   正规方程解的推导过程   $\\theta = (X^TX)^{-1}X^Ty$ 的推导过程：  其中：   将向量表达形式转为矩阵表达形式，则有$J(\\theta) = \\frac{1}{2}(X\\theta - y)^{2}$，其中 X 为$mn$的矩阵（m为样本个数，n为特征个数），$\\theta$为$n1$的矩阵，y 为$m*1$的矩阵，对$J(\\theta)$进行如下变换:      接下来对$J(\\theta)$偏导，需要用到以下几个矩阵的求导法则:    对于（a）式：由于： $X^TAX = AX^2$ ，所以其导数为2AX 。  所以有:      ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/04/04-7.html",
        "teaser":null},{
        "title": "第四章 多变量线性回归(Linear Regression with Multiple Variables)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/04/features.html",
        "teaser":null},{
        "title": "05-1 分类",
        
        "excerpt":
            "分类      分类问题   逻辑回归   二分类问题   分类问题：需要预测的变量 y 是离散值。   在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；区别一个肿瘤是恶性的还是良性的。   逻辑回归 (Logistic Regression)   逻辑回归 (Logistic Regression) 是分类问题的一个代表算法，这是目前最流行使用最广泛的一种学习算法。   二分类问题   我们将因变量(dependant variable)可能属于的两个类分别称为负向类（negative class）和 正向类（positive class），则因变量 $y\\in0$，其中 0 表示负向类，1 表示正向类。      如果我们要用线性回归算法来解决一个分类问题， 即使所有训练样本的标签 y 都等于 0 或 1，算法得到的值可能会远大于 1 或者远小于 0 ，不符合我们想要的结果。   分类问题下，可以采用逻辑回归的分类算法，这个算法的性质是：它的输出值永远在 0 到 1 之间。 它适用于标签 y 取值离散的情况，如：1  0  0  1。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/05/05-1.html",
        "teaser":null},{
        "title": "05-2 假设函数表达式",
        
        "excerpt":
            "假设函数表达式      逻辑回归的假设函数   分类问题，希望分类器的输出值在 0 和 1 之间，因此，假设函数需要满足预测值要在 0 和 1 之间。   回归模型的假设是： $h_\\theta(x)=g(\\theta^TX)$ 其中：     X 代表特征向量     g 代表逻辑函数（logistic function）, 是一个常用的逻辑函数为 S 形函数（Sigmoid function），公式为：    该函数的图像为：      牛刀小试           #python 代码实现sigmoid函数： import numpy as np def sigmoid(z):     return 1 / (1 + np.exp(-z))             结合起来，获得逻辑回归的假设：                   $h_\\theta(x)$的作用是，对于给定的输入变量，根据选择的参数计算输出变量为1 的可能性（estimated probablity），即 $h_{\\theta}(x) = P(y=1       x;{\\theta})$。           例如，如果对于给定的 x，通过已经确定的参数计算得出 hθ(x)=0.7，则表示有 70%的几率 y 为正向类，相应地 y 为负向类的几率为 1-0.7=0.3  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/05/05-2.html",
        "teaser":null},{
        "title": "05-3 决策边界",
        
        "excerpt":
            "决策边界   逻辑回归模型中：      我们预测：  当 $h_\\theta(x)$大于等于 0.5 时，预测 y=1。  当$h_\\theta(x)$小于 0.5 时，预测 y=0 。   根据上面绘制出的 S 形函数图像，我们知道当   z=0 时 g(z)=0.5     z&gt;0 时 g(z)&gt;0.5      z&lt;0 时 g(z)&lt;0.5   又$z=\\theta^Tx$ ，即：     $\\theta^Tx$大于等于 0 时，预测 y=1     $\\theta^Tx$小于 0 时，预测 y=0   现在假设我们有一个模型：      并且参数$\\theta$是向量[-3 1 1]。 则当$-3 + x_1 + x_2$大于等于 0，将预测 y=1。   我们可以绘制直线$x_1+x_2 = 3$，这条线便是我们模型的分界线，将预测为 1 的区域和预 测为 0 的区域分隔开。      假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？      牛刀小试   Todo: 想一想,可以通过构造什么特征来分隔开上图的两类数据呢?   答：可以构造二次方特征   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/05/05-3.html",
        "teaser":null},{
        "title": "05-4 代价函数 I",
        
        "excerpt":
            "代价函数   定义代价函数用来拟合逻辑回归的参数，这便是监督学习问题中的逻辑回归模型的拟合问题。      线性回归模型的代价函数是所有模型误差的平方和。理论上来说，对逻辑回归模型也可以沿用这个定义，但是问题在于，当我们将$h_\\theta(x) =  \\frac{1}{1+e^{-\\theta^TX}}$带入到这样的代价函数中时，得到的代价函数将是一个非凸函数（non-convex function）。      这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。   线性回归的代价函数为：     重新定义逻辑回归的代价函数为：   ，其中      $h_\\theta(x)$与$Cost(h_\\theta(x), y)$之间的关系如下图所示：         这样构建的$Cost(h_\\theta(x), y)$函数的特点是：   当实际的 y=1 且$h_\\theta(x)$也为1 时误差为 0，当 y=1 但$h_\\theta(x)$不为 1 时误差随着$h_\\theta(x)$的变小而变大；   当实际的 y=0 且$h_\\theta(x)$也为 0 时代价为 0，当 y=0 但$h_\\theta(x)$不为 0 时误差随着 $h_\\theta(x)$的变大而变大。   将构建的$Cost(h_\\theta(x), y)$简化如下：   带入代价函数得到：   即：     牛刀小试   Todo: 补全下方的 cost 逻辑回归的代价函数           # 逻辑回归代价函数的Python代码实现： import numpy as np def cost(theta, X, y):     theta = np.matrix(theta)     X = np.matrix(X)     y = np.matrix(y)     first = np.multiply(-y, np.log(sigmoid(X * theta.T)))     # Todo: 补全  second       second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))     return np.sum(first - second) / (len(X))            ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/05/05-4.html",
        "teaser":null},{
        "title": "05-5 简化的代价函数和梯度下降",
        
        "excerpt":
            "简化的代价函数和梯度下降      简化代价函数   梯度下降   逻辑回归的代价函数：      这个式子可以合并成：      要试图找出让 $J(\\theta)$取得最小值的参数$\\theta$。   梯度下降   最小化代价函数的方法，是使用梯度下降法(gradient descent)。这是通常用的梯度下降法的模板：      反复更新每个参数，用这个式子减去学习率 α 乘以后面的微分项。求导后得到：      计算得到等式：     来它同时更新所有$\\theta$的值。   这个更新规则和之前用来做线性回归梯度下降的式子是一样的， 但是假设的定义发生了变化。即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西。   牛刀小试   Todo:  列出你知道的梯度下降的技巧   答:监控梯度下降法以确保其收敛、参数向量化、特征缩放等   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/05/05-5.html",
        "teaser":null},{
        "title": "05-6 高级优化技巧",
        
        "excerpt":
            "高级优化技巧      逻辑回归的高级优化算法   如何使用这些算法   通过一些高级优化算法和一些高级的优化概念，能够使梯度下降进行逻辑回归的速度大大提高，而这也将使算法更加适合解决大型的机器学习问题，比如，我们有数目庞大的特征量。   现在我们换个角度来看什么是梯度下降，我们有个代价函数$J(\\theta)$ ，而我们想要使其最小化，那么我们需要做的是编写代码，当输入参数 $\\theta$时，它们会计算出两样东西：$J(\\theta)$以及 $J=0,1,…,n$ 时的偏导数项。      假设我们已经完成了可以实现这两件事的代码，那么梯度下降所做的就是反复执行这些更新。 另一种考虑梯度下降的思路是：我们需要写出代码来计算$J(\\theta)$和这些偏导数，然后把这些插入到梯度下降中，然后它就可以为我们最小化这个函数。   计算代价函数$J(\\theta)$和偏导数项 $\\frac{\\partial}{\\partial \\theta_j}J(\\theta)$可以用其他方法：Conjugate gradient，共轭梯度法 BFGS (变尺度法) 和 L-BFGS (限制变尺度法) ，使用比梯度下降更复杂的算法来最小化代价函数。这两种算法的具体细节超出了本门课程的范畴。   Conjugate gradient，共轭梯度法 BFGS (变尺度法) 和 L-BFGS (限制变尺度法) 这三种算法的优点：      不需要手动选择学习率 α。   收敛更快。   缺点：     过于复杂   一个例子      比方说，一个含两个参数的问题，这两个参数是$\\theta_0$ 和 $ \\theta_1 $ ，因此，通过这个代价函数，你可以得到 $\\theta_1$和$\\theta_2$的值，如果你将$J(\\theta)$最小化的话，那么它的最小值将是$\\theta_1=5$, $\\theta_2=5$。   代价函数$J(\\theta)$的导数推出来就是这两个表达式：      如果我们不知道最小值，但你想要代价函数找到这个最小值，是用比如梯度下降这些算法，但最好是用比它更高级的算法，你要做的就是运行一个像这样的 Octave 函数：  function [jVal, gradient]=costFunction(theta)          jVal=(theta(1)-5)^2+(theta(2)-5)^2;              gradient=zeros(2,1);            gradient(1)=2*(theta(1)-5);         gradient(2)=2*(theta(2)-5);       end        这样就计算出这个代价函数，函数返回的第二个值是梯度值，梯度值应该是一个 2×1的向量，梯度向量的两个元素对应这里的两个偏导数项，运行这个 costFunction 函数后，你就可以调用高级的优化函数，这个函数叫 fminunc，它表示 Octave 里无约束最小化函数。   调用它的方式如下：  options=optimset('GradObj','on','MaxIter',100);       initialTheta=zeros(2,1);       [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);        你要设置几个 options，这个 options 变量作为一个数据结构可以存储你想要的 options，所以 GradObj 和 On，这里设置梯度目标参数为打开(on)，这意味着你现在确实要给这个算法提供一个梯度，然后设置最大迭代次数，比方说 100，我们给出一个 θ 的猜测初始值，它是一个 2×1 的向量，那么这个命令就调用 fminunc，这个@符号表示指向我们刚刚定义的costFunction 函数的指针。如果你调用它，它就会使用众多高级优化算法中的一个，当然你也可以把它当成梯度下降，只不过它能自动选择学习速率 α，你不需要自己来做。然后它会尝试使用这些高级的优化算法，就像加强版的梯度下降法，为你找到最佳的 θ 值。   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/05/05-6.html",
        "teaser":null},{
        "title": "05-7 多分类任务",
        
        "excerpt":
            "多分类任务      逻辑回归 (logistic regression) 解决多分类问题   “一对多” (one-vs-all)分类算法   问题场景举例：     自动邮件归类， 第一个例子：假如说你现在需要一个学习算法能自动地将邮件归类到不同的文件夹里，区分开来自工作的邮件、来自朋友的邮件、来自家人的邮件或者是有关兴趣爱好的邮件，那么，就有了一个四分类问题：其类别有四个，分别用 y=1、y=2、y=3、y=4 来代表。   药物诊断。如果一个病人因为鼻塞来到你的诊所，他可能并没有生 病，用 y=1 这个类别来代表；或者患了感冒，用 y=2 来代表；或者得了流感用 y=3 来代表。   天气。要区分哪些天是晴天、多云、雨天、或者下雪天   二分类与多分类的数据集区别：      一个学习算法来进行分类呢？   下面将介绍如何进行一对多的分类工作，有时这个方法也被称为”一对余”方法。   现在我们有一个训练集，有三个类别，我们用三角形表示 y=1，方框表示 y=2，叉叉表示 y=3。我们下面要做的就是使用一个训练集，将其分成三个二元分类问题。      先从用三角形代表的类别 1 开始，实际上我们可以创建一个，新的”伪”训练集，类型 2 和类型 3 定为负类，类型 1 设定为正类，我们创建一个新的训练集，我们要拟合出一个合适的分类器。      这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为 1，圆形的值为 0，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。      为了能实现这样的转变，我们将多个类中的一个类标记为正向类（y=1），然后将其他所有类都标记为负向类，这个模型记作$h^{(1)}{\\theta}(x)$。接着，类似地第我们选择另一个类标记为 正向类（y=2），再将其它类都标记为负向类，将这个模型记作  $h^{(2)}{\\theta}(x)$,依此类推。 最后我们得到一系列的模型简记为：     最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。 总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h^{(i)}{\\theta}(x)$， 其中 i 对应每一个可能的 y=i，最后，为了做出预测，我们给出输入一个新的 x 值做预测。我们要做的就是在我们三个分类器里面输入 x，然后我们选择一个让 $h^{(i)}{\\theta}(x)$ 最大的 i，即 $\\max_{i}h^{(i)}_\\theta(x)$。   你现在知道了基本的挑选分类器的方法，选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论 i 值是多少，我们都有最高的概率值，我们预测 y 就是那个值。这就是多类别分类问题，以及一对多的方法，通过这个小方法，你现在也可以将逻辑回归分类器用在多类分类的问题上。   牛刀小试   Todo:  如果要按照这种方式进行三分类,需要几个分类器?   答：3个   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/05/05-7.html",
        "teaser":null},{
        "title": "第五章 逻辑回归(Logistic Regression)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/05/features.html",
        "teaser":null},{
        "title": "06-1 过拟合问题",
        
        "excerpt":
            "过拟合问题   过拟合（Overfitting)              第一个模型是一个线性模型，欠拟合，不能很好地适应我们的训练集；            第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。            我们可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好；而中间的模型似乎最合适。          就以多项式理解，x 的次数越高，拟合的越好，但相应的预测的能力就可能变差。   如何解决过拟合问题           丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如 PCA, LDA），缺点是丢弃特征的同时，也丢弃了这些相应的信息；            正则化。 保留所有的特征，但是减少参数的大小（magnitude），当我们有大量的特征，每个特征都对目标值有一点贡献的时候，比较有效。            还有一个解决方式就是增加数据集, 因为过拟合导致的原因就过度拟合测试数据集, 那么增加数据集就很大程度提高了泛化性了.      ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/06/06-1.html",
        "teaser":null},{
        "title": "06-2 代价函数 II",
        
        "excerpt":
            "代价函数 II      代价函数中的正则化处理   正则化参数选择   高次项导致了过拟合的产生。   正则化的基本方法：对高次项添加惩罚值，让高次项的系数接近于0。   假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：     其中$\\lambda$又称为正则化参数（Regularization Parameter）。 注：根据惯例，我们不对$\\theta_0$进行惩罚。   牛刀小试           # todo 实现带正则的mse代价函数 import numpy as np  def mseWithRegular(predict, y, w, lmd=0.1):     '''         predict: 模型输出         y: 真实标签         w: 模型权重         lmd: 正则化参数     '''     # todo 实现上方公式     constrct_loss = np.sum((predict - y) ** 2)     experience_loss = lmd * np.sum(w ** 2)     loss = (constrct_loss + experience_loss) / (2 * len(predict))     return loss  predict = np.array([1, 1.5, 2]) y = np.array([0.9, 1.4, 2.1]) w = np.array([[1], [1], [1]]) mseWithRegular(predict, y, w)                          0.055000000000000014                   正则化参数选择   如果选择的正则化参数$\\lambda$过大，则会把所有的参数都最小化了，导致模型变成 $h_\\theta(x) = \\theta_0$，造成欠拟合。   原因是：增加$\\lambda\\sum_{j=1}^n\\theta^2_j$后，如果令λ的值很大的话，为了使 Cost Function 尽可能的小，所有的 $\\theta$的值（不包括$\\theta_0$）都会在一定程度上减小。 但若λ的值太大了，那么$\\theta$的值（不包括$\\theta_0$）都会趋近于 0，这样我们所得到的只能是一条平行于 x 轴的直线。   所以对于正则化，我们要取一个合理的λ的值，这样才能更好的应用正则化。   回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/06/06-2.html",
        "teaser":null},{
        "title": "06-3 正则化与线性回归",
        
        "excerpt":
            "正则化与线性回归      如何通过正则化来解决线性回归过拟合问题   对于线性回归的求解，我们之前推导了两种学习算法：一种基于梯度下降，一种基于正规方程。   正则化线性回归的代价函数为：     如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对$\\theta_0$进行正则化，所以梯度下降算法将分两种情形：      对上面的算法中 $j=1,2,…,n$ 时的更新式子进行调整可得：     可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令$\\theta$值减少了一个额外的值。   我们同样也可以利用正规方程来求解正则化线性回归模型，方法如下所示：      图中的矩阵尺寸为 (n+1)*(n+1)。   在前面讲过，如果 样本数 m 小于等于特征数 n 的时候，会遇到矩阵不可逆的问题，但是加入了正则项，可以证明到只要 $ \\lambda &gt; 0$ ,就可以解决不可逆的问题。     ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/06/06-3.html",
        "teaser":null},{
        "title": "06-4 正则化与逻辑回归",
        
        "excerpt":
            "正则化与逻辑回归   针对逻辑回归问题，我们在之前的课程已经学习过两种优化算法：梯度下降法，更高级的优化算法需要你自己设计代价函数$J(\\theta)$。   给代价函数增加一个正则化的表达式，得到代价函数:    牛刀小试           def sigmoid(x, derivative=False):     sigm = 1. / (1. + np.exp(-x))     if derivative:         return sigm * (1. - sigm)     return sigm  # 代码实现 import numpy as np def costReg(theta, X, y, learningRate):     theta = np.matrix(theta)     X = np.matrix(X)     y = np.matrix(y)     # todo 实现代价函数第一部分的计算     first = np.multiply(-y, np.log(sigmoid(X * theta.T)))      # todo 实现代价函数第二部分的计算     second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))      # todo 实现代价函数正则化的计算     reg = (learningRate / 2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))      return np.sum(first - second) / (len(X)) + reg                注：看上去同线性回归一样，但是由于假设$h_\\theta(x)=g(\\theta^TX)$，所以与线性回归不同。   注意：     虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但 由于两者的$h_\\theta(x)$不同所以还是有很大差别。   $\\theta_0$不参与其中的任何一个正则化。   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/06/06-4.html",
        "teaser":null},{
        "title": "第六章 正则化(Regularization)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/06/features.html",
        "teaser":null},{
        "title": "07-1 非线性假设",
        
        "excerpt":
            "非线性假设      非线性假设及其缺点   图像识别汽车例子引入   我们之前学的，无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。  下面是一个例子：      当我们使用$x_1$,$x_2$的多次项式进行预测时,效果很好。   之前我们已经看到过，使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于 100 个变量，我们希望用这 100 个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合，我们也会有接近 5000 个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。   假设我们希望训练一个模型来识别视觉对象（例如识别一张图片上是否是一辆汽车）一种方法是我们利用很多汽车的图片和很多非汽车的图片，然后利用这些图片上一个个像素的值（饱和度或亮度）来作为特征。      假如我们只选用灰度图片，每个像素则只有一个值（而非 RGB 值），我们可以选取图片上的两个不同位置上的两个像素，然后训练一个逻辑回归算法利用这两个像素的值来判断图片上是否是汽车：      假使我们采用的都是 50x50 像素的小图片，并且我们将所有的像素视为特征，则会有 2500 个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约$2500^2/2$个（接近 3 百万个）特征。   普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/07/07-1.html",
        "teaser":null},{
        "title": "07-2 神经元与大脑",
        
        "excerpt":
            "神经元与大脑   神经网络背景知识   起源：尝试使用算法模拟人脑   历史：神经网络逐渐兴起于二十世纪八九十年代，应用得非常广泛。但由于各种原因，在90年代的后期应用减少了。最近，神经网络又东山再起了。其中一个原因是：神经网络是计算量有些偏大的算法，近些年计算机的运行速度变快，才足以真正运行起大规模的神经网络。   生物学实验：把耳朵到听觉皮层的神经切断，将眼睛到视神经的信号传到听觉皮层。如果你对动物这样做，那么动物就可以完成视觉辨别任务，它们可以看图像，并根据图像做出适当的决定。      大脑处理生物信号的一些例子：         用舌头学会“看”。它的原理是，你在前额上带一个灰度摄像头，面朝前，它就能获取你面前事物的低分辨率的灰度图像。你连一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头的某个位置上，可能电压值高的点对应暗像素，电压值低的点对应于亮像素。使用这种系统就能让你我在几十分钟里就学会用我们的舌头“看”东西。   人体回声定位或者说人体声纳。 YouTube上有一个令人称奇的孩子，他因为癌症眼球惨遭移除，但是通过打响指，他可以四处走动而不撞到任何东西，他能滑滑板，他可以将篮球投入篮框中。注意这是一个没有眼球的孩子。   触觉皮带。如果你把它戴在腰上，蜂鸣器会响，而且总是朝向北时发出嗡嗡声。它可以使人拥有方向感，用类似于鸟类感知方向的方式。   青蛙的第三只眼。如果你在青蛙身上插入第三只眼，青蛙也能学会使用那只眼睛。   从某种意义上来说，如果我们能找出大脑的学习算法，然后在计算机上执 行大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。人工智能的梦想就是：有一天能制造出真正的智能机器。   这节课中讲授的神经网络，主要是对于现代机器学习应用。它是最有效的技术方法。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/07/07-2.html",
        "teaser":null},{
        "title": "07-3 模型表示 I",
        
        "excerpt":
            "模型表示 I      大脑的神经网络   神经网络建模   大脑神经网络运行原理   每一个神经元都可以被认为是一个处理单元/神经核（processing unit/ Nucleus），它含有许多输入/树突（input/Dendrite），并且有一个输出/轴突（output/Axon）。神经网络是大量神经元相互链 接并通过电脉冲来交流的一个网络。      神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是一些微弱的电流。所以如果神经元想要传递一个消息，它就会就通过它的轴突，发送一段微弱电流给其他神经元。   所有人类思考的模型：我们的神经元把自己的收到的消息进行计算，并向其他神经元传递消息。   神经网络模型   神经网络模型是许多神经元按照不同层级组织起来的网络，每一层的输出变量都是下一层的输入变量。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输入，并且根据本身的模型提供一个输出。下图为一个 3 层的神经网络，第一层称为输入层（Input Layer），最后一 层称为输出层（Output Layer），中间一层称为隐藏层（Hidden Layers）。在神经网络中，参数又可被称为权重（weight）。我们为每一层都增加一个偏差单位（bias unit）      其中，$x_1$,$x_2$,$x_3$是输入单元（input units），我们将原始数据输入给它们。$a^{(2)}1$，$a^{(2)}_2$，$a^{(2)}_3$  是中间单元，它们负责将数据进行处理，然后呈递到下一层。 最后是输出单元，它负责计算$h\\theta(x)$。   下面引入一些标记法来帮助描述模型：  $a^{(j)}_i$代表第 j 层的第 i 个激活单元。$\\theta^{(j)}$ 代表从第 j 层映射到第 j+1 层时的权重的矩阵，其尺寸为：以第 j+1 层的激活单元数量为行数，以第 j 层的激活单元数加一为列数的矩阵。例如$\\theta^{(1)}$代表从第一层映射到第二层的权重的矩阵,它的尺寸为 3*4。   对于上图所示的模型，激活单元和输出分别表达为：   ![]https://i.loli.net/2018/12/01/5c01f52908141.png)   上面进行的讨论中只是将特征矩阵中的一行（一个训练实例）喂给了神经网络，我们需要将整个训练集都喂给我们的神经网络算法来学习模型。  我们可以知道：每一个 a 都是由上一层所有的 x 和每一个 x 所对应的 $\\theta$决定的。  （我们把这样从左到右的算法称为前向传播算法( FORWARD PROPAGATION )）  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/07/07-3.html",
        "teaser":null},{
        "title": "07-4 模型表示 II",
        
        "excerpt":
            "模型表示 II   大脑中的神经网络是怎样的？   前向传播算法( FORWARD PROPAGATION ) 相对于与使用循环来编码，利用向量化的方法会使得计算更为简便。   以上面的神经网络为例，试着计算第二层的值：         我们令 $z^{(2)}=\\theta^{(1)}x$，则 $a^{(2)}=g(z^{(2)})$，计算后添加$a^{(2)}_0=1$。 计算输出的值为：      我们令 $z^{(3)}=\\theta^{(2)}a^{(2)}$，则 $h_\\theta(x)=a^{(3)}=g(z^{(3)})$。   这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算，我们需要将训练集特征矩阵进行转置，使得同一个实例的特征都在同一列里。即：      为了更好了了解 Neuron Networks 的工作原理，我们先把左半部分遮住：      右半部分其实就是以$a_0, a_1, a_2, a_3$,按照 Logistic Regression 的方式输出$h_\\theta(x)$：      我们可以把$a_0, a_1, a_2, a_3$看成更为高级的特征值，也就是$x_0, x_1, x_2, x_3$的进化体，并且它们是由 x 与$\\theta$决定的，因为是梯度下降的，所以 a 是变化的，并且变得越来越厉害，所以 这些更高级的特征值远比仅仅将 x 次方厉害，也能更好的预测新数据。  这就是神经网络相比于逻辑回归和线性回归的优势。   牛刀小试   &lt;img src=’https://i.loli.net/2018/12/01/5c01f2d2b8ac5.png’ width=50% height=50%&gt;   我们假设没有激活函数           # todo 实现上图绘制的神经网络, 我们可以采用 np.random.uniform(size=())去随机生成参数 import numpy as np  def sigmoid(x):     return 1/(1+np.exp(-x))  def net():     # todo 确定输入和权重的维度     X = np.array([[1],[-2],[3],[-4]])     theta1 = np.random.uniform(size=(3, 4))     hidden_input = sigmoid(np.matmul(theta1,X))     print('hidden_input',hidden_input)     hidden_input = np.insert(hidden_input, 0, [1], axis=0)     print('hidden_input',hidden_input)     theta2 = np.random.uniform(size=(1, 4))     output = sigmoid(np.matmul( theta2,hidden_input))     return output            ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/07/07-4.html",
        "teaser":null},{
        "title": "07-5 模型表示实例 I",
        
        "excerpt":
            "模型表示实例 I      神经网络的样例与直观理解   单层神经元表示逻辑运算: 逻辑与（AND）、逻辑或（OR）   神经网络的直观理解   从本质上讲，神经网络能够通过学习得出其自身的一系列特征。   在普通的逻辑回归中，我们被限制为使用数据中的原始特征$x_0, x_1, x_2, x_3$我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。   在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。   神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与（AND）、逻辑或（OR）。   逻辑与（AND)&lt;/span&gt;   我们可以用这样的一个神经网络表示 AND 函数： 假设输入$x_1,x_2 \\in 0$      其中 $\\theta_0 = -30$, $\\theta_1 = 20$, $\\theta_2 = 20$ 输出函数 $h_\\theta(x)$即为：$h_\\theta(x)=g(-30 + 20x_1 + 20x_2)$   g(x)的图像是：      那么，$x_1,x_2$一共四种可能取值，得到的结果分别如下：      所以我们的： $h_\\theta(x) \\approx x_1 AND x_2$。   逻辑或（OR） 与 AND 整体一样，区别只在于权值的取值不同。   牛刀小试   我们来实现一个单层网络(感知机).           # todo 实现与网络  import numpy as np  def sigmoid(x):     return 1/(1+np.exp(-x))  class Net():     def __init__(self,theta):         self.theta=theta     def run(self,X):            output = sigmoid(np.matmul(self.theta,X))         return output  net = Net(np.array([[-30,20,20]]))                 ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/07/07-5.html",
        "teaser":null},{
        "title": "07-6 模型表示实例 II",
        
        "excerpt":
            "模型表示实例 II      单层神经网络组成二元逻辑运算符   逻辑非（NOT）   下图的神经元（两个权重分别为 10，-20）可以被视为作用等同于逻辑非（NOT）：      二元逻辑运算符（BINARY LOGICAL OPERATORS）   当输入特征为布尔值（0 或 1）时，我们可以用一个单一的激活层可以作为二元逻辑运算符，表示不同的运算符只需要选择不同的权重即可。      逻辑与（AND） 下图的神经元（三个权重分别为-30，20，20）可以被视为作用同于逻辑与（AND）：         逻辑或（OR） 下图的神经元（三个权重分别为-10，20，20）可以被视为作用等同于逻辑或（OR）：         逻辑异或（XOR） 下图的神经元（三个权重分别为10，-20，-20）可以被视为作用等同于逻辑异或（XOR）：      实现更复杂的运算   例如：实现 XNOR 功能（输入的两个值必须一样，均为 1 或均为 0），即:     第二层实现AND 和XOR ，第三层实现OR：      我们就得到了一个能实现 XNOR 运算符功能的神经网络。   按这种方法我们可以逐渐构造出越来越复杂的函数，也能得到更加厉害的特征值。   手写数字识别的视频实例   此处为视频演示，请观看视频教程      这就是神经网络的厉害之处。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/07/07-6.html",
        "teaser":null},{
        "title": "07-7 多分类任务",
        
        "excerpt":
            "多分类任务      多分类神经网络   多分类神经网络   如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有 4 个值。例如，第一个值为 1 或 0 用于预测是否是行人，第二个值用于判断是否为汽车。 输入向量 x 有三个维度，两个中间层，输出层 4 个神经元分别用来表示 4 类，也就是每一个数据在输出层都会出现$[a,b,c,d]^T$ ，且 a,b,c,d 中仅有一个为 1，表示当前类。下面是该神经网络的可能结构示例：      训练集的形式如下，注意使用one-hot方式表示y：      补充说明  分类问题的Label（即y）编码问题：   one-hot的基本思想：将离散型特征的每一种取值都看成一种状态，若你的这一特征中有N个不相同的取值，那么我们就可以将该特征抽象成N种不同的状态，one-hot编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。   如果分类问题有四个结果，    我们不会将y的取值为：0，1，2，3 而是会将y表示为一个1*4的向量，如上图。   牛刀小试   实现one-hot编码函数           import numpy as np import tqdm def onehot(x):     unique_values = list(set(x))     number_of_dimension = len(unique_values)     onehot_features = np.zeros(shape=(len(x), number_of_dimension))     for row in tqdm.tqdm(range(len(x))):         onehot_features[row, unique_values.index(x[row])] = 1     return onehot_features  y = np.array([1, 2, 3, 4, 5, 1, 2]) onehot(y)                         array([[1., 0., 0., 0., 0.],        [0., 1., 0., 0., 0.],        [0., 0., 1., 0., 0.],        [0., 0., 0., 1., 0.],        [0., 0., 0., 0., 1.],        [1., 0., 0., 0., 0.],        [0., 1., 0., 0., 0.]])                  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/07/07-7.html",
        "teaser":null},{
        "title": "第七章 神经网络-表述(Neural Networks-Representation)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/07/features.html",
        "teaser":null},{
        "title": "08-1 代价函数 III",
        
        "excerpt":
            "代价函数 III      神经网络的代价函数   标记约定   假设神经网络的训练样本有 m 个，每个包含一组输入 x 和一组输出信号 y，L 表示神经网络层数，$S_I$表示每层的 neuron 个数( SL表示输出层神经元个数)，$S_L$代表最后一层中处理单元的个数。   神经网络分类问题   将神经网络的分类定义为两种情况：二类分类和多类分类，     二类分类： $S_L=1$, y=0 or 1表示哪一类；   K 类分类： $S_L=K$,$y_i$表示分到第 i 类；（K &gt; 2）      代价函数   在逻辑回归中，我们只有一个输出变量，又称标量（scalar），也只有一个因变量 y,逻辑回归问题中代价函数为：      但是在神经网络中，可以有很多输出变量，我们的$h_\\theta(x)$是一个维度为 K 的向量，并且训练集中的因变量也是同样维度的一个向量，因此我们的代价函数会比逻辑回归更加复杂一些。      通过代价函数来观察算法预测的结果与真实情况的误差有多大，与逻辑回归唯一不同的是，对于每一行特征，我们都会给出 K 个预测，基本上我们可以利用循环，对每一行特征都预测 K 个不同结果，然后在利用循环 在 K 个预测中选择可能性最高的一个，将其与 y 中的实际数据进行比较。   正则化的那一项只是排除了每一层$\\theta^{(0)}$后，每一层的矩阵$\\theta$的和。最里层的循环 j 循环所有的行（由 $s_l + 1$层的激活单元数决定），循环 i 则循环所有的列，由该层（$s_l$层）的激 活单元数所决定。即：$h_\\theta(x)$与真实值之间的距离为每个样本-每个类输出的加和，对参数进行 regularization 的 bias 项处理所有参数的平方和。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/08/08-1.html",
        "teaser":null},{
        "title": "08-2 反向传播",
        
        "excerpt":
            "反向传播   反向传播算法   计算神经网络预测结果的时候我们采用了一种正向传播方法，我们从第一层开始正向一层一层进行计算，直到最后一层的$h_\\theta(x)$。   计算代价函数的偏导数$\\frac{\\partial}{\\partial\\theta^{(l)}_{ji}}j(\\theta)$，我们需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。   反向传播的实例   假设训练集只有一个实例$(x^{(1)},y^{(1)})$，神经网络是一个四层的神经网络，其中$K=4,S_L=4,L=4$：   前向传播算法：      我们从最后一层的误差开始计算，误差是激活单元的预测$(a^{(4)}_k)$与实际值$(y^k)$之间的误差，$(k=1:K)$。           第四层：用δ来表示误差，则：$\\delta^{(4)} = a^{(4)} -y$            第三层：利用$\\delta^{(4)}$来计算前一层的误差：$\\delta^{(3)} = (\\theta^{(3)})^T\\delta^{(4)} * g^(z^{(3)})$       其中$g^(z^{(3)})$是 S 形函数的导数，$g^`(z^{(3)})=a^{(3)}*(1-a^{(3)})$。而 $(\\theta^{(3)})^T\\delta^{(4)} $则是权重导致的误差的和。       继续计算第二层的误差：$\\delta^{(2)} = (\\theta^{(2)})^T\\delta^{(3)} * g^`(z^{(2)})$   第一层是输入变量，不存在误差。   有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设 $\\lambda=0$，即我们不做任何正则化处理时有：    重要的是清楚地知道上面式子中上下标的含义：      l 代表目前所计算的是第几层   j 代表目前计算层中的激活单元的下标，也将是下一层的第 j 个输入变量的下标。   i 代表下一层中误差单元的下标，是受到权重矩阵中第 i 行影响的下一层中的误差单元的下标。   如果我们考虑正则化处理，并且我们的训练集是一个特征矩阵而非向量。在上面的特殊情况中，我们需要计算每一层的误差单元来计算代价函数的偏导数。在更为一般的情况中，我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，我们用$\\Delta^{(l)}_{ij}$来表示这个误差矩阵。第 l 层的第 i 个激活单元受到第 j 个参数影响而导致的误差。   我们的算法表示为：      即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。   在求出了$D^{(l)}_{ij}$之后，我们便可以计算代价函数的偏导数了，计算方法如下：       ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/08/08-2.html",
        "teaser":null},{
        "title": "08-3 理解反向传播",
        
        "excerpt":
            "理解反向传播   模型的学习优化体现在参数的改变上, 那么我们该怎么去改变它们呢?   这一节将帮助大家探索和理解优化算法的核心.   推理(前向传播)   学习反向传播之前, 我们先学习一下前向传播(9-3视频, 1 minutes 37 seconds, 可以查看吴恩达教授视频中的详细推导过程).      $x_1, x_2, +1$分别代表了输入和偏差项, 不考虑偏差项, 我们有两个输入, 两个隐藏层, 每个隐藏层有两个神经元, 最后有一个输出.   那么我们会有两个权重矩阵.$W_{hidden1}, W_{hidden2}$, 他们的$W_{shape} = (input_{num}, numofunits_{hidden})$   我们的计算公式为(这里是矩阵乘法)       我们来进行一个特例的计算         接下里我们来实战一下, 看看自己是否掌握了呢~           import numpy as np                      def forward_progragation(inputs, weights):     if inputs.shape[1] != weights.shape[0]:         raise RuntimeError('维度不匹配, 请检查输入')     # todo 计算输出层     output = np.matmul(inputs, weights)     return output  inputs = np.random.uniform(size=(1, 3)) weights = np.random.uniform(size=(3, 1))  output = forward_progragation(inputs, weights)  print(output.shape)  assert output.shape == (1, 1), '输出结果不对, 请继续检查'                         (1, 1)                  恭喜你, 现在你已经掌握了全连接网络的单层前向传播, 可以自己尝试多层的编写~   激活函数      吴恩达教授有讲, 我们使用S型函数进行激活(视频, 3 minute 10 seconds), 我们来看一下S型函数的曲线以及表达式     我们来练习一下~           def sigmoid(x):     # todo 实现sigmoid激活函数     result = 1.0 / (1 + np.exp(-x))     return result  x = 0 result = sigmoid(x) assert result == 0.5, '函数实现错了呢, 请修改'             现在我们已经掌握了层的计算以及激活函数的编写, 先表扬一下自己, 已经掌握了神经网络的基本骨架了, 多层网络只是对这种结构进行堆叠~   理解反向传播      接下来我们来一起看看反向传播.   反向传播   首先我们需要定义代价函数, 这是反向传播的第一步   代价函数的含义   代价函数代表的是我们模型的输出与真实值的差距, 越接近就代表我们的模型越准确, 那么我们的目标应该就是最小化这个损失, 模型输出和前面的权重有关(因为输入是固定的), 那么我们最后要的就是优化我们权重$W$, 方式就是对$W$求导了(我们的代价函数可以看做是关于$W$的函数), 可以得到变化最大的方向, 进行修正就完成了这个步骤了.   拓展   常用的代价函数   分类(交叉熵)     回归(MSE)     拓展小练习   我们来实现一下这几个函数(可选)           def cross_entropy(p, y):     log_p = np.log(p)     # todo 计算交叉熵     loss = (1.0 / n) * np.sum(np.multiply(y, p) + np.multiply(1 - y, p))     return loss  def mse(p, y):     if len(p) != len(y):         raise RuntimeError('样本数不一致')     loss = 1.0 / len(p)     loss *= np.sum(np.power((p - y), 2))     return loss              计算偏导数   理解反向传播      我们其实是在计算偏导数来进行更新的, 复合函数的求导其实就是链式法则     现在我们已经学习了前向, 反向传播, 我们接着来实现一个单隐藏层, 代价函数是 MSE 的神经网络   我们有$X =&gt; (1, 5)$, $W =&gt; (5, 1)$, $output =&gt; (1, 1)$, $loss = mse$的结构.               def mse(p, y):     if len(p) != len(y):         raise RuntimeError('样本数不一致')     loss = 1.0 / len(p)     loss *= np.sum(np.power((p - y), 2))     return loss                     def forward_progragation(inputs, weights):     if inputs.shape[1] != weights.shape[0]:         raise RuntimeError('维度不匹配, 请检查输入')     output = np.matmul(inputs, weights)     return output                     def back_progragation(error, x, p):     delta_error = (error - p)     dw = np.matmul(x.T, delta_error)     return dw                     def net(x, w, y):     forward = forward_progragation(x, w)     loss = mse(forward, y)     dw = back_progragation(loss, x, forward)     print(\"forward:{0} \\nloss:{1}\\nbackward:\\n{2}\".format(forward, loss, dw))  x = np.random.uniform(size=(1, 5)) w = np.random.uniform(size=(5, 1)) y = np.array([[1]])  net(x, w, y)                         forward:[[1.14818486]]  loss:0.021958754174409168 backward: [[-0.21902523]  [-1.0724607 ]  [-0.70203568]  [-0.16878467]  [-0.89255638]]                 ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/08/08-3.html",
        "teaser":null},{
        "title": "08-4 实现参数展开",
        
        "excerpt":
            "实现参数展开      反向传播细节实现：把参数从矩阵展开成向量   展开参数            牛刀小试   我们来实现以下参数展开           def flatten_matrix(x):     if not isinstance(x, type(np.array([]))):         x = np.array(x)     return x.flatten()            ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/08/08-4.html",
        "teaser":null},{
        "title": "08-5 梯度校验",
        
        "excerpt":
            "梯度校验   当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。   为了避免这样的问题，我们采取一种叫做梯度的数值检验（Numerical Gradient Checking）的方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。   对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的$\\theta$，我们计算出在 $\\theta-\\sigma$处和$\\theta+\\sigma$的代价值（$\\sigma$是一个非常小的值，通常选取 0.001），然后求两个代价的平均，用以估计在$\\theta$处的代价值。      Octave 中代码如下：  gradApprox = (J(theta + eps) – J(theta - eps)) / (2*eps)   当$\\theta$是一个向量时，我们则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验，下面是一个只针对$\\theta_1$进行检验的示例：     最后我们还需要对通过反向传播方法计算出的偏导数进行检验。   根据上面的算法，计算出的偏导数存储在矩阵$D^{(l)}{ij}$中。检验时，我们要将该矩阵展开成为向量，同时我们也将$\\theta$矩阵展开为向量，我们针对每一个$\\theta$都计算一个近似的梯度值，将 这些值存储于一个近似梯度矩阵中，最终将得出的这个矩阵同$D^{(l)}{ij}$进行比较。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/08/08-5.html",
        "teaser":null},{
        "title": "08-6 随机初始化",
        
        "excerpt":
            "随机初始化   任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为 0，这样的 初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。   如果我们令所有的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非 0 的数，结果也是一样的。   我们通常初始参数为正负 ε 之间的随机值，假设我们要随机初始一个尺寸为 10×11 的参数矩阵，代码如下：  Theta1 = rand(10, 11) * (2*eps) – eps      牛刀小试   通过numpy实现随机初始化           import numpy as np a= np.random.rand(10,11) # 机初始一个尺寸为 10×11 的参数矩阵 print(a)                         [[0.89012924 0.08140487 0.95453883 0.10000767 0.99754677 0.63964653   0.11652367 0.32366237 0.06493215 0.29949962 0.23932125]  [0.34599237 0.28393063 0.50223968 0.8203895  0.70872595 0.2265806   0.4533645  0.97246446 0.8904674  0.82028052 0.82447659]  [0.72672059 0.11029451 0.17859903 0.90301048 0.71830531 0.95255698   0.43121794 0.9719399  0.57803923 0.42545519 0.52678866]  [0.97252596 0.73107581 0.6182679  0.61268221 0.56593853 0.21033142   0.40632278 0.30804307 0.84622809 0.13058157 0.25621354]  [0.86555812 0.36736487 0.92972801 0.14521097 0.89582942 0.18434383   0.6729972  0.79367812 0.9795744  0.0741685  0.81361421]  [0.60743163 0.99058131 0.09496072 0.6065123  0.1258677  0.36574205   0.2549608  0.07228038 0.54149799 0.08674347 0.08558238]  [0.08808488 0.36059832 0.00851983 0.48893007 0.60359639 0.74890883   0.22425914 0.45415193 0.95099883 0.10464225 0.50671484]  [0.23908324 0.87941705 0.6147011  0.57182956 0.58180014 0.25448078   0.74614772 0.6257922  0.80234479 0.56125048 0.70141577]  [0.03802879 0.04623665 0.6573583  0.49151533 0.86659065 0.18159533   0.7612263  0.79319728 0.60322461 0.41638314 0.50906066]  [0.67511789 0.51440033 0.76044891 0.62701232 0.87420669 0.1688621   0.84625497 0.86893233 0.31110609 0.73861569 0.37070833]]                 ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/08/08-6.html",
        "teaser":null},{
        "title": "08-7 把前面的内容放在一起",
        
        "excerpt":
            "神经网络小结   小结一下使用神经网络时的步骤   网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。   第一层的单元数即我们训练集的特征数量。   最后一层的单元数是我们训练集的结果的类的数量。   如果隐藏层数大于 1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。   我们真正要决定的是隐藏层的层数和每个中间层的单元数。   训练神经网络：     参数的随机初始化   利用正向传播方法计算所有的 hθ(x)   编写计算代价函数 J 的代码   利用反向传播方法计算所有偏导数   利用数值检验方法检验这些偏导数   使用优化算法来最小化代价函数  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/08/08-7.html",
        "teaser":null},{
        "title": "08-8 自动驾驶",
        
        "excerpt":
            "自动驾驶      案例介绍：使用神经网络实现自动驾驶   可视化技术  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/08/08-8.html",
        "teaser":null},{
        "title": "第八章 神经网络的学习(Neural Networks-Learning)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/08/features.html",
        "teaser":null},{
        "title": "09-1 决定下一步做什么",
        
        "excerpt":
            "决定下一步做什么      机器学习模型误差较大时怎么改进   如何改进机器学习模型      当运用训练好的模型来预测未知数据的时候发现有较大的误差时，通常有以下方法：     获得更多的训练实例——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。   尝试减少特征的数量来避免过拟合.   尝试获得更多的特征   尝试增加多项式特征   尝试减少正则化程度$\\lambda$   尝试增加正则化程度$\\lambda$   我们不应该随机选择上面的某种方法来改进我们的算法，而是运用一些机器学习诊断法来帮助我们知道上面哪些方法对我们的算法是有效的。   在接下来的两段视频中，我们将介绍”机器学习诊断法”。   这些诊断法的执行和实现，是需要花些时间的，有时候确实需要花很多时间来理解和实现，但是它通常能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/09/09-1.html",
        "teaser":null},{
        "title": "09-2 假设检验",
        
        "excerpt":
            "假设检验      如何评估假设函数   训练集与测试集   测试机误差计算   评估假设函数   假设函数误差很小时,可能已经过拟合了。   那么，你该如何判断一个假设函数是过拟合的呢？对于这个简单的例子，我们可以对假设函数 h(x) 进行画图，然后观察图形趋势，但对于特征变量不止一个的这种一般情况，还有像有很多特征变量的问题，想要通过画出假设函数来进行观察，会变得很难甚至是不可能实现的。   因此，我们需要另一种方法来评估我们的假设函数过拟合检验。   训练集与测试集   为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。   很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。      测试集误差计算   在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差：     对于线性回归模型，我们利用测试数据集计算代价函数J ；        对于逻辑回归模型，我们可以利用测试数据集来计算代价函数：     也可以计算误分类的比率，对于每一个测试集实例，计算：      然后对计算结果求平均。    ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/09/09-2.html",
        "teaser":null},{
        "title": "09-3 模型选择与训练集、验证集、测试集切分",
        
        "excerpt":
            "模型选择与训练集、验证集、测试集切分      模型选择   交叉验证集   模型选择   假设我们要在 10 个不同次数的二项式模型之间进行选择：      显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。   交叉验证集   交叉验证集：训练集（train）用60%的数据，交叉验证集（validation）用20%的数据，测试集(test)用20%的数据      模型选择的方法为：     使用训练集训练出 10 个模型   用 10 个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）   选取代价函数值最小的模型   用步骤 3 中选出的模型对测试集计算得出推广误差（代价函数的值）      牛刀小试   Todo:  在选定最终的模型以前, 能在测试集上进行测试吗?   答:不能, 只有选定了最终的模型, 才能在测试集上进行测试.   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/09/09-3.html",
        "teaser":null},{
        "title": "09-4 诊断偏差和方差",
        
        "excerpt":
            "诊断偏差和方差      诊断偏差或是方差，即判断欠拟合还是过拟合   诊断偏差或方差   高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。      我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析：      其中，训练集误差和交叉验证误差如下：      对于训练集，当 d 较小时，模型拟合程度更低，误差较大；随着 d 的增长，拟合程度提高，误差减小。      对于交叉验证集，当 d 较小时，模型拟合程度低，误差较大；但是随着 d 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。   如果我们的交叉验证集误差较大，我们如何判断是方差还是偏差呢？ 根据上面的图表，我们知道:         训练集误差和交叉验证集误差近似时：偏差   交叉验证集误差远大于训练集误差时：方差   牛刀小试   Todo:  当训练集误差和交叉验证集误差近似时,是欠拟合还是过拟合? 当交叉验证集误差远大于训练集误差时,是欠拟合还是过拟合?   答：当训练集误差和交叉验证集误差近似时,是欠拟合,当交叉验证集误差远大于训练集误差时,是过拟合.   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/09/09-4.html",
        "teaser":null},{
        "title": "09-5 正则化、偏差和方差",
        
        "excerpt":
            "正则化、偏差和方差   正则化系数的选择   选择正则化系数$\\lambda$ 的值时也需要思考欠拟合和过拟合的问题。      通常，我们选择一系列的想要测试的 λ 值，通常是 0-10 之间的呈现 2 倍关系的值（如：0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10 共 12 个）。 我们同样把数据分为训练集、交叉验证集和测试集。      选择$\\lambda$的方法为：     使用训练集训练出 12 个不同程度正则化的模型   用 12 模型分别对交叉验证集计算的出交叉验证误差   选择得出交叉验证误差最小的模型   运用步骤 3 中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与$\\lambda$的值绘制在一张图表上：         当$\\lambda$较小时，训练集误差较小（过拟合）而交叉验证集误差较大   随着$\\lambda$的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加   牛刀小试   Todo:  观察上图, 当$\\lambda$较小时，训练集误差较小（过拟合）而交叉验证集误差较大 , 随着$\\lambda$的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加, 这句话是正确的吗?   答:正确.   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/09/09-5.html",
        "teaser":null},{
        "title": "09-6 学习曲线",
        
        "excerpt":
            "学习曲线   学习曲线是学习算法的一个很好的合理检验（sanity check），可以用来判断某一个学习算法是否处于偏差、方差问题。学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（m）的函数绘制的图表。   思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。         如何利用学习曲线识别高偏差/欠拟合： 作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观：      也就是说在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。   如何利用学习曲线识别高方差/过拟合： 假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。   也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。      也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。   牛刀小试   Todo:  写下你知道的解决过拟合的方法.   答:1.增加正则项 2.减小模型复杂度   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/09/09-6.html",
        "teaser":null},{
        "title": "09-7 决定下一步做什么(回顾)",
        
        "excerpt":
            "决定下一步做什么(回顾)   选择处理方法      获得更多的训练实例——解决高方差   尝试减少特征的数量——解决高方差   尝试获得更多的特征——解决高偏差   尝试增加多项式特征——解决高偏差   尝试减少正则化程度 λ——解决高偏差   尝试增加正则化程度 λ——解决高方差   神经网络的方差和偏差         使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小   使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。   通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。   对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/09/09-7.html",
        "teaser":null},{
        "title": "第九章 应用机器学习的建议(Advice for Applying Machine Learning)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/09/features.html",
        "teaser":null},{
        "title": "10-1 优先考虑什么",
        
        "excerpt":
            "优先考虑什么      引入垃圾邮件分类器算法，讨论机器学习系统设计   准备工作   本周以一个垃圾邮件分类器算法为例进行讨论。   首先，决定如何选择并表达特征向量x：可以选择一个由 100 个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为 1，不出现为 0），尺寸为 100×1。   为了构建这个分类器算法，我们可以做很多事，例如：   优先考虑什么         收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本   基于邮件的路由信息开发一系列复杂的特征   基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理   为探测刻意的拼写错误（例如: 把 watch 写成 w4tch）开发复杂的算法  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/10/10-1.html",
        "teaser":null},{
        "title": "10-2 误差分析",
        
        "excerpt":
            "误差分析   误差分析   误差分析（error analysis）会帮助你更系统地做出决定。   如果你准备研究机器学习的东西，或者构造机器学习应用程序，最好的实践方法不是建立一个非常复杂的系统，拥有多么复杂的变量；而是构建一个简单的算法，这样你可以很快地实现它。      构建一个学习算法的推荐方法为：     从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法   绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择   进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势   以我们的垃圾邮件过滤器为例：    误差分析要做的首先是检验交叉验证集中我们的算法产生错误预测的所有邮件，看：是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃 圾邮件或者密码窃取邮件等。    然后看分类器对哪一组邮件的预测误差最大，并着手优化。 思考怎样能改进分类器。例如记录下错误拼写出现了多少次，异常的邮件路由情况出现了多少次等等，然后从出现次数最多的情况开始着手优化。   误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验 证集的误差。   牛刀小试   Todo: 我们应该在交叉验证集还是应该在测试集上来实施误差分析?   答:交叉验证集。   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/10/10-2.html",
        "teaser":null},{
        "title": "10-3 误差矩阵",
        
        "excerpt":
            "误差矩阵      偏斜类   查全率与查准率   偏斜类   偏斜类（skewed classes）问题，表现为训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例。   查全率与查准率   误差矩阵   将算法预测的结果分成四种情况：     正确肯定（True Positive,TP）：预测为真，实际为真   正确否定（True Negative,TN）：预测为假，实际为假   错误肯定（False Positive,FP）：预测为真，实际为假   错误否定（False Negative,FN）：预测为假，实际为真      查准率（Precision） = TP/（TP+FP）。    例：肿瘤预测中，在所有预测有恶性肿瘤的病人中，实际上有恶性肿 瘤的病人的百分比，越高越好。   查全率（Recall） =  TP/（TP+FN）。    例：肿瘤预测中，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。   牛刀小试   Todo: 对于肿瘤预测来说, 查准率和查全率哪个更重要?   答：查全率   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/10/10-3.html",
        "teaser":null},{
        "title": "10-4 精确率和召回率的权衡",
        
        "excerpt":
            "精确率和召回率的权衡      权衡查准率和查全率   F1 Score   权衡查准率和查全率   沿用刚才预测肿瘤性质的例子。假使，我们的算法输出的结果在 0-1 之间，我们使用阀值 0.5 来预测真和假。      查准率（Precision） = TP/（TP+FP）。    例：肿瘤预测中，在所有预测有恶性肿瘤的病人中，实际上有恶性肿 瘤的病人的百分比，越高越好。   查全率（Recall） =  TP/（TP+FN）。    例：肿瘤预测中，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。   如果希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比 0.5 更大的阀值，如 0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。   如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比 0.5 更小的阀值，如 0.3。      F1 Score   选择阈值的一种方法是是计算 F1 值（F1 Score），其计算公式为：    我们选择使得 F1值最高的阀值。   牛刀小试   Todo: 查准率和查全率都是0.5的时候, f1score 是多少?   答：0.5。   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/10/10-4.html",
        "teaser":null},{
        "title": "10-5 机器学习的数据",
        
        "excerpt":
            "机器学习的数据      讨论机器学习的数据量的影响   数据量与特征值选择   机器学习的数据   多年前，两位研究人员 Michele Banko 和 Eric Brill 进行了一项有趣的研究，他们尝试通过机器学习算法来区分常见的易混淆的单词，他们尝试了许多种不同的算法，并且改变了训练数据集的大小，并尝试将这些学习算法用于不同大小的训练数据集中，这就是他们得到的结果：      这些趋势非常明显。首先大部分算法，都具有相似的性能；其次，随着训练数据集的增大，在横轴上代表以百万为单位的训练集大小，从 0.1 个百万到 1000 百万，也就是到了 10 亿规模的训练集的样本，这些算法的性能也都对应地增强了。   这些结果表明，许多不同的学习算法有时倾向于表现出非常相似的表现，但是真正能提高性能的，是你能够给一个算法大量的训练数据。像这样的结果，引起了一种在机器学习中的普遍共识：”取得成功的人不是拥有最好算法的人，而是拥有最多数据的人“。   数据量与特征参数选择      关于机器学习数据与特征值的选取比较有效的检测方法：          一个人类专家看到了特征值 x，能很有信心的预测出 y 值吗？     因为这可以证明 y 可以根据特征值 x 被准确地预测出来。            我们实际上能得到一组庞大的训练集，并且在这个训练集中训练一个有很多参数的学习算法吗？      ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/10/10-5.html",
        "teaser":null},{
        "title": "第十章 机器学习系统的设计(Machine Learning System Design)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/10/features.html",
        "teaser":null},{
        "title": "11-1 优化问题",
        
        "excerpt":
            "优化问题      支持向量机（SVM）   由逻辑回归代价函数引入SVM代价函数   SVM的假设   支持向量机(Support Vector Machine)是一个广泛应用于工业界和学术界的更加强大的算法。与逻辑回归和神经网络相比，支持向量机，或者简称 SVM，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。   这是逻辑回归的优化目标：      其中，用z来表示$\\theta^Tx$。   在逻辑回归中：如果有一个$y=1$的样本，想要正确地将此样本分类，则$h_\\theta(x)$要趋近1，这就意味着当$\\theta^Tx$应当远大于$0$，这里的»意思是远远大于$0$。由于$z$表示$\\theta^Tx$，当$z$远大于$0$时，即到了该图的右边，你不难发现此时逻辑回归的输出将趋近于$1$。相反地，如果有另一个样本$y=0$，那么我们希假设函数的输出值应该趋近于$0$，这对应于 $\\theta^Tx$或者就是$z$会远小于 0。      进一步观察逻辑回归的代价函数，会发现每个样本 (x, y)都会为总代价函数增加了一项，因此，对于总代价函数通常会有对所有的训练样本求和，并且还有一个 1/m 项，   现在，先忽略 1/m 这一项，但是这一项是影响整个总代价函数中的这一项的。一起来考虑两种情况：一种是 y 等于 1 的情况；另一种是 y 等于 0 的情况。   在第一种情况中，假设 y=1，此时在目标函数中只需有第一项起作用，因为 y 等于 1 时，(1-y) 项将等于0。因此，当在 y=1 的样本中时，即在 (x, y) 中 y 等于 1，我们得到这样一项$-log(1-\\frac{1}{1 + e^{-z}})$：      用z来表示$\\theta^Tx$。当然，在代价函数中，y 前面有负号。如果画出关于 z 的函数，你会看到这条曲线，当 z 增大时，也就是$\\theta^Tx$增大时，z 对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本 y=1 时，试图将$\\theta^Tx$设置得非常大。因为，在代价函数中的这 一项会变的非常小。   代价函数的设计&lt;/span&gt;   逻辑回归的代价函数，也就是$-log(1-\\frac{1}{1 + e^{-z}})$。取z=1 点，先画出将要用的代价函数。 然后我再画一条同逻辑回归非常相似的直线，但是，在这里是一条直线，也就是我用紫红色画的曲线，就是这条紫红色的曲线。      给这两个方程（紫红色曲线）命名，左边的函数为$\\cos t^{(z)}_1$，右边函数为$\\cos t^{(z)}_0$ 。下标是指在代价函数中对应的 y=1 和 y=0 的情况.   这是我们在逻辑回归中使用代价函数$J(\\theta)$,将负号移到了表达式的里面:      对于支持向量机而言，实质上我们要将这替换为$\\cos t^{(z)}_1$ ，也就是 $\\cos t^{(\\theta^Tx)}_1$ ，同样地，我也将这一项替换为$\\cos t^{(\\theta^Tx)}_0$。这里的代价函数$\\cos t^{(\\theta^Tx)}_1$，就是之前所提到的那条线。然后，再加上正则化参数。对于支持向量机，我们得到了这里的最小化问题，即:      这个公式有一些不同：     1/m 这一项被除去。仅仅是由于人们使用支持向量机时的习惯所致，1/m 仅是个常量不影响最终得到的最优值$\\theta$。   概念上的变化。训练样本的代价表示为A，正则化项为B。逻辑回归的目标函数为$A+\\lambda B$，通过设置不同正则参数$\\lambda$达到优化目的,即最小化A。但对于支持向量机，目标函数为$C×A+B$，这里的参数 C 可以考虑成$\\frac{1}{\\lambda}$，同 1/λ 所扮演的角色相同。，这只是一种不同的方式来控制这种权衡或者一种不同的方法，即用参数来决定是更关心代价函数的优化，还是更关心正则项的优化。   支持向量机的假设      最后有别于逻辑回归输出的概率。在这里，我们的代价函数，获得参数$\\theta$时，支持向量机所做的是它来直接预测 y 的值等于 1，还是等于 0。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/11/11-1.html",
        "teaser":null},{
        "title": "11-2 直观理解大间距分类器",
        
        "excerpt":
            "直观理解大间距分类器      大间距分类器   直观理解SVM模型   对于参数C的选择   大间距分类器   这是支持向量机模型的代价函数      支持向量机的一个有趣性质——安全间距因子   如果你有一个正样本$y=1$，则只有在$z\\geq1$时，代价函数$\\cos t^{(z)}_1$才等于 0。换句话说，如果你有一个正样本，我们会希望 $\\theta^Tx\\ge1$，反之，如果$y=0$的，函数$\\cos t^{(z)}_0$只有在$ z\\leq1$的区间里函数值为0。事实上，如果你有一个正样本$y=1$，则仅仅要求 $\\theta^Tx\\ge0$，就能将该样本恰当分出，类似地，如果你有一个负样本，则仅需要  $\\theta^Tx\\leq0$就会将负例正确分离。   但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本，即不仅仅要求 $\\theta^Tx\\ge0$，我们需要的是比 0 值大很多，比如大于等于 1，这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说安全的间距因子。   安全间距因子的影响   考虑一个特例：我们将这个常数 C 设置成一个非常大的值，比如假设 C 的值为 100000，然后来观察支持向量机会给出什么结果。      如果 C 非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项为 0 的最优解。因此，让我们尝试在代价项的第一项为 0 的情形下理解该优化问题。   这将遵从以下的约束：如果$y^{(i)}=1$，则要求$\\theta^Tx^{(i)}\\geq1$;如果$y^{(i)}=0$，则要求$\\theta^Tx^{(i)}\\leq-1$;   这样当你求解这个优化问题的时候，当你最小化这个关于变量$\\theta$的函数的时候，你会得到一个非常有趣的决策边界。      具体而言，如果你考察这样一个数据集，其中有正样本，也有负样本，可以看到这个数据集是线性可分的。我的意思是，存在一条直线把正负样本分开。当然有多条不同的直线，可以把正样本和负样本完全分开。支持向量机将会选择这个黑色的决策边界，相较于之前粉色或者绿色的决策边界。黑线看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这是什么意思呢？这条黑线有更大的距离，这个距离叫做间距 (margin)。 这个距离叫做支持向量机的间距，这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为大间距分类器。   异常点 (outlier)的影响   事实上，支持向量机现在要比这个大间距分类器所体现得更成熟，尤其是当你使用大间距分类器的时候，你的学习算法会受异常点 (outlier) 的影响。比如我们加入一个额外的正样本。      如果加了这个样本，为了将样本用最大间距分开，最终会得到一条类似粉色的线的决策界。仅仅基于一个异常值，仅仅基于一个样本，就将我的决策界从这条黑线变到这条粉线，这实在是不明智的。而如果正则化参数 C设置的非常大，支持向量机会将决策界从黑线变到了粉线，但是如果将 C 设置的不要太大，则你最终会得到这条黑线。   实际上，应用支持向量机的时候，当 C 不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。   回顾 C=1/λ，因此：     C 较大时，相当于 λ 较小，可能会导致过拟合，高方差。   C 较小时，相当于 λ 较大，可能会导致低拟合，高偏差。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/11/11-2.html",
        "teaser":null},{
        "title": "11-3 大间距分类器背后的数学原理 (选学)",
        
        "excerpt":
            "大间距分类器背后的数学原理 (选学)      大间距分类器的数学实现   向量的内积   SVM的目标函数   向量内积   假设有两个向量，u和v，都是二维向量。$u^Tv$叫做向量u 和v之间的内积。      将两个向量画在图上。                     向量u在横轴上，取值为某个$u_1$，而在纵轴上，高度是$u_2$作为u的第二个分量。现在，很容易计算的一个量就是向量u的范数$               u               $,即向量$u$的欧几里得长度。根据毕达哥拉斯定理， $               u               =\\sqrt{u_1^2+u_2^2}$，这是向量u的长度，它是一个实数。                          计算u和v之间的内积的几何做法，我们将向量v做一个直角投影到向量u上，度量这条红线的长度p，因此 p 就是向量v投影到向量u上的量。因此可以将$u^Tv=p\\cdot               u               $ 。           另一种线性代数方法，$u^Tv=u_1\\times v_1+u_2\\times v_2$,会给出同样的结果。   顺便，有$u^Tv=v^Tu$。                  需要注意的就是p值是有符号的，即它可能是正值，也可能是负值。如下图，如果u 和v之间的夹角大于 90 度，则如果将v投影到u 上，会得到这样的一个投影，这是 p 的长度，在这个情形下我们仍然有$u^Tv=p\\cdot               u               $，唯一一点不同的是 p 在这里是负的。在内积计算中，如果u 和v之间的夹角小于 90 度，那么那条红线的长度 p 是正值。然而如果这个夹角大于 90 度，则 p 将会是负的。就是这个小线段的长度是负的。如果它们之间的夹角大于 90 度，两个向量之间的内积也是负的。              SVM的目标函数      这就是我们先前给出的支持向量机模型中的目标函数。接下来忽略掉截距，令$\\theta_0=0$，这样更容易画示意图。假设特征数 n = 2，则仅有两个特征$x_1$,$x_2$。 此时，支持向量机的优化目标函数可以写作：     当然你可以将其写作$\\theta_0,\\theta_1,\\theta_2$，但是，数学上不管你是否包含，其实并没有差别，因此在我们接下来的推导中去掉$\\theta_0$不会有影响这意味着我们的目标函数是等于 $\\frac{1}{2}||theta||^2$。因此支持向量机做的全部事情，就是极小化参数向量$\\theta$范数的平方。   现在我们将要深入地理解$\\theta^Tx$。给定参数向量$\\theta$给定一个样本$x^{(i)}就类似于u和v。      使用我们之前的方法，我们计算的方式就是我将训练样本投影到参数向量$\\theta$，然后我来看一看这个线段的长度，我将它画成红色。我将它称为$ p^{(i)}$用来表示这是第i个训练样本在参数向量$\\theta$上的投影,$\\theta^Tx$将会等于p乘以向量$\\theta$的长度或范数。   这里表达的意思是：这个$\\theta^Tx^{(i)} \\geq1$或者$\\theta^Tx^{(i)} &lt; -1$的,约束是可以被$p^{(i)} \\cdot x \\geq 1$这个约束所代替的。我们的优化目标就变成了$p^{(i)} \\cdot x$。                  需要提醒一点，我们之前曾讲过这个优化目标函数可以被写成等于$\\frac{1}{2}               \\theta               ^2$ 。              假设支持向量机会选择这个绿色决策边界。这不是一个非常好的选择，因为它的间距很小。这个决策界离训练样本的距离很近。我们来看一下为什么支持向量机不会选择它。   对于这样选择的参数$\\theta$，可以看到参数向量$\\theta$。事实上是和决策界是 90 度正交的，因此这个绿色的决策界对应着一个参数向量$\\theta$指向这个方向,顺便提一句$\\theta_0=0$的简化仅仅意味着决策界必须通过原点 (0,0)。现在让我们看一下这对于优化目标函数意味着什么。   比如这个样本，我们假设它是我的第一个样本$x^{(1)}$，如果我考察这个样本到参数$\\theta$的投影，投影是这个短的红线段，就等于$p^{(1)}$。类似地，样本$x^{(2)}$，它到$\\theta$的投影在这里。投影是这个短的粉线段$p^{(2)}$，它事实上是一个负值。      如果这是决策界，这就是相对应的参数 θ 的方向，因此，在这个决策界之下，垂直线是决策界。使用线性代数的知识，可以说明，这个绿色的决策界有一个垂直于它的向量$\\theta$ 。现在如果你考察你的数据在横轴 x 上的投影，比如这个我之前提到的样本，我的样本 $x^{(1)}$，当我将它投影到横轴 x 上，或说投影到 θ 上，就会得到这样的$p^{(1)}$。它的长度是 $p^{(1)}$，另一个样本，那个样本是 x(2)。我做同样的投影，我会发现，$p^{(2)}$的长度是负值。你会注意到现在$p^{(1)}$和$p^{(2)}$这些投影长度是长多了。如果我们仍然要满足这些约束，$p^{(i)} \\cdot x &gt; 1$，这意味着通过选择右边的决策界，而不是左边的那个，支持向量机可以使参数$\\theta$的范数变小很多。因此，如果我们想令$\\theta$的范数变小，从而令$\\theta$范数的平方变小，就能让支持向量机选择右边的决策界。这就是支持向量机如何能有效地产生大间距分类的原因。   看这条绿线，这个绿色的决策界。我们希望正样本和负样本投影到 θ 的值大。要做到这一点的唯一方式就是选择这条绿线做决策界。这是大间距决策界来区分开正样本和负样本这个间距的值。这个间距的值就是$p^{(1)}$,$p^{(2)}$,$p^{(3)}$等等的值。通过让间距变大，即通过这些$p^{(1)}$,$p^{(2)}$,$p^{(3)}$等等的值，支持向量机最终可以找到一个较小的$\\theta$范数。这正是支持向量机中最小化目标函数的目的。   以上就是为什么支持向量机最终会找到大间距分类器的原因。因为它试图极大化这些$p^{(i)}$的范数，它们是训练样本到决策边界的距离。   最后一点，我们的推导自始至终使用了这个简化假设，就是参数 $\\theta_0=0$。它能够让决策界通过原点。如果你令 $\\theta_0$不是 0 的话，含义就是你希望决策界不通过原点。我将不会做全部的推导。实际上，支持向量机产生大间距分类器的结论，会被证明同样成立，证明方式是非常类似的，是我们刚刚做的证明的推广。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/11/11-3.html",
        "teaser":null},{
        "title": "11-4 核技巧 I",
        
        "excerpt":
            "核技巧 I      回顾分类问题的多项式模型   高斯核函数   分类问题的多项式模型：      为了获得上图所示的判定边界，我们的模型可能是$\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1x_2+\\theta_4x^2_1+\\theta_5x^2_2+…$的形式。   我们可以用一系列的新的特征 f 来替换模型中的每一项。例如令：   $f_1 =x_1, f_2=x_2,f_3=x_1x_2, f_4=x^21, f_5=x^2_2,…$  得到 $h\\theta(x)=f1+f2+…+fn$。   然而，除了对原有的特征进行组合以外，有没有更好的方法来构造 f1,f2,f3？我们可以利用核函数来计算出新的特征。   核函数   给定一个训练实例x，我们利用x的各个特征与我们预先选定的地标（landmarks） $l^{(1)},l^{(2)},l^{(3)}$的近似程度来选取新的特征 f1,f2,f3。      例如：    其中：$||x-l^{(1)}||^2=\\sum^n_{j=1}(x_j-l^{(1)}_j)^2$为实例 x 中所有特征与地标$l^{(1)}$之间的距离的和。   上例中的 $similarity（x,l^{(1)})$就是核函数，具体而言，这里是一个高斯核函数（Gaussian Kernel）。 **注：这个函数与正态分布没什么实际上的关系，只是看上去像而已。 **   这些地标的作用是什么？如果一个训练实例 x 与地标 L 之间的距离近似于 0，则新特征 f 近似于$ e^{-0}=1$，如果训练实例 x 与地标 L 之间距离较远，则 f 近似于 $e^{-(一个较大的数)}=0$。   假设我们的训练实例含有两个特征$[x_1 x_2]$，给定地标 $l^{(1)}$与不同的 σ 值，见下图：      图中水平面的坐标为$[x_1 x_2]$,而垂直坐标轴代表 f。可以看出，只有当 x 与 $l^{(1)}$重合时 f 才具有最大值。随着 x 的改变 f 值改变的速率受到 σ2的控制。   在下图中，当实例处于洋红色的点位置处，因为其离$l^{(1)}$更近，但是离$l^{(2)}$和$l^{(3)}$较远，因此 f1接近 1，而 f2,f3 接近 0。因此 hθ(x)=θ0+θ1f1+θ2f2+θ1f3&gt;0，因此预测 y=1。同理可以求出， 对于离 l(2)较近的绿色点，也预测 y=1，但是对于蓝绿色的点，因为其离三个地标都较远，预测 y=0。      这样，图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练实例和我们选 取的地标所得出的判定边界，在预测时，我们采用的特征不是训练实例本身的特征，而是通 过核函数计算出的新特征 f1,f2,f3。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/11/11-4.html",
        "teaser":null},{
        "title": "11-5 核技巧 II",
        
        "excerpt":
            "核技巧 II   视频主要内容     使用核函数的细节实现   如何选择地标   如何选择下标  通常是根据训练集的数量选择地标的数量，即如果训练集中有 m 个实例，则我们 选取 m 个地标，并且令：$ l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},…,l^{(m)}=x^{(m)}$。这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即：   SVM中的核函数  下面我们将核函数运用到支持向量机中，修改我们的支持向量机假设为：         给定 x，计算新特征 f，当 $\\theta^Tf&gt;=0$ 时，预测 y=1，否则反之。   在具体实施过程中，还需要对最后的正则化项进行些微调整，在计算$\\sum^{n=m}_{j=1}\\theta^2_j=\\theta^T\\theta$         时，我们用 $\\theta^TM\\theta$ 代替$\\theta^T\\theta$，其中 M 是根据我们选择的核函数而不同的一个矩阵。这样做的原因是为了简化计算。   理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用 M 来简化计算的方法 不适用与逻辑回归，因此计算将非常耗费时间。   在此，我们不介绍最小化支持向量机的代价函数的方法，你可以使用现有的软件包（如  liblinear,libsvm 等）。在使用这些软件包最小化我们的代价函数之前，我们通常需要编写核函数，并且如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的。   另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数（linear kernel）， 当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。   SVM参数C与$\\sigma$  下面是支持向量机的两个参数 C (1/λ)和 σ 的影响：     C 较大时，相当于 λ 较小，可能会导致过拟合，高方差；   C 较小时，相当于 λ 较大，可能会导致低拟合，高偏差；   σ 较大时，可能会导致低方差，高偏差；   σ 较小时，可能会导致低偏差，高方差。   牛刀小试   我们来实现以下Guassian Kernel           import numpy as np def kernelWithGuassian(x1, x2, sigma):     result = np.exp(- np.power(x1 - x2, 2) / (2 * (sigma ** 2)))     return result x1 = np.array([1, 2, 1]) x2 = np.array([0, 4, -1]) sigma = 2  kernelWithGuassian(x1, x2, sigma)                          array([0.8824969 , 0.60653066, 0.60653066])                   另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数（linear kernel）， 当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/11/11-5.html",
        "teaser":null},{
        "title": "11-6 使用 SVM",
        
        "excerpt":
            "使用 SVM      使用 SVM的一些注意事项   核函数的选择   使用 SVM的一些注意事项   强烈建议使用高优化软件库，比如liblinear 和 libsvm等，来实现SVM算法。   核函数的选择   在高斯核函数之外我们还有其他一些选择，如：     多项式核函数（Polynomial Kernel）   字符串核函数（String kernel）   卡方核函数（ chi-square kernel）   直方图交集核函数（histogram intersection kernel）  这些核函数的目标也都是根据训练集和地标之间的距离来构建新特征，这些核函数需要 满足 Mercer’s 定理，才能被支持向量机的优化软件正确处理。   多分类问题   假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有 k 个类，则 我们需要 k 个模型，以及 k 个参数向量 θ。我们同样也可以训练 k 个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。   尽管你不去写你自己的 SVM（支持向量机）的优化软件，但是你也需要做几件事：     是提出参数 C 的选择。我们在之前的视频中讨论过误差/方差在这方面的性质。   你也需要选择内核参数或你想要使用的相似函数，其中一个选择是：我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。   选择机器学习模型的一些准则   n 为特征数，m 为训练样本数。          如果相较于 m 而言，n 要大许多，即训练集数据量不够支持我们训练一个复杂的非 线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。            如果 n 较小，而且 m 大小中等，例如 n 在 1-1000 之间，而 m 在 10-10000 之间，使用高斯核函数的支持向量机。            如果 n 较小，而 m 较大，例如 n 在 1-1000 之间，而 m 大于 50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。       值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络 可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/11/11-6.html",
        "teaser":null},{
        "title": "第十一章 支持向量机(Support Vector Machines)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/11/features.html",
        "teaser":null},{
        "title": "12-1 非监督学习介绍",
        
        "excerpt":
            "非监督学习介绍   典型的监督学习中，我们有一个有标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界需要据此拟合一个假设函数。      无监督学习中，数据没有附带任何标签：      图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到我圈出的这些点集的算法，就被称为聚类算法。   聚类算法                           聚类算法试图将数据集中的样本划分为若干个通常是不交集的子集，每个子集 称为一个簇（cluser）。形式化地说，假定样本集$D= {x_1,x_2,…,x_m}$包含m个无标记样本，每个样本$x_i$是一个n维特征向量，则聚类算法将样本D划分为k个不相交的簇${C_l         l=1,2,…,k}$，其中$C_{l^}\\cap_{l^ \\neq l}C_l = \\emptyset$且$D=\\cup^k_{l=1}C_l$。相应地，我们用$\\lambda_j \\in {1,2,…,k}$表示样本$x_j$的“簇标记”（cluster label），即$x_ \\in C_{\\lambda_j}$。聚类结果也可以用包含m个元素的簇标记向量$\\lambda = (\\lambda_1,\\lambda_2,…,\\lambda_k)$表示。                  牛刀小试   Todo: 你认为上面的无监督学习示意图中的样本点,可以聚为几类?   答：2类。   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/12/12-1.html",
        "teaser":null},{
        "title": "12-2 K-Means 算法",
        
        "excerpt":
            "K-Means 算法   K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。   K-均值算法的输入：     K，聚类数目   无标签数据集   K-均值是一个迭代算法，假设我们想要将数据聚类成 n 个组，其方法为:     选择 k 个随机的点，称为聚类中心（cluster centroids）；   对于数据集中的每一个数据，按照距离 K 个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类；   计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置；   重复步骤 2-4 直至中心点不再变化。   用$\\mu^1,\\mu^2,…,\\mu^k$来表示聚类中心，用$c^{(1)},c^{(2)},…c^{(m)}$来存储与第 i 个实例数据最近的聚 类中心的索引，K-均值算法的伪代码如下：  Repeat {      for i = 1 to m      c(i) := index (form 1 to K) of cluster centroid closest to x(i)    for k = 1 to K  μk := average (mean) of points assigned to cluster k  }   算法分为两个步骤，第一个 for 循环是赋值步骤，即：对于每一个样例 i，计算其应该属于的类。第二个 for 循环是聚类中心的移动，即：对于每一个类 k，重新计算该类的质心。   K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用 K-均值算法将数据分为三类，用于帮助确定将要生产的 T-恤衫的三种尺寸。      牛刀小试   Todo: 上面的问题,可以将 K 设置为其他值吗?   答：可以。   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/12/12-2.html",
        "teaser":null},{
        "title": "12-3 优化目标",
        
        "excerpt":
            "优化目标      K-均值的代价函数   K-均值的迭代算法   K-均值的代价函数   K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此 K-均值的代价函数（又称畸变函数 Distortion function）为：   优化目标      其中$\\mu_{c^{(i)}}$代表与$x^{(i)}$ 最近的聚类中心点。优化目标便是找出使得代价函数最小的$c^{(1)},c^{(2)},…c^{(m)}$和 $\\mu^1,\\mu^2,…,\\mu^k$。   回顾刚才给出的：   优化目标      K-均值的迭代算法      K-均值迭代算法，第一个循环是用于减小$c^{(i)}$引起的代价，而第二个循环则是用于减小$\\mu_i$引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/12/12-3.html",
        "teaser":null},{
        "title": "12-4 随机初始化 II",
        
        "excerpt":
            "随机初始化 II   随机初始化聚类中心点   随机初始化所有的聚类中心点的做法：     我们应该选择 K&lt;m，即聚类中心点的个数要小于所有训练集实例的数量   随机选择 K 个训练实例，然后令 K 个聚类中心分别与这 K 个训练实例相等   K-均值的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。      为了解决这个问题，通常需要多次运行 K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行 K-均值的结果，选择代价函数最小的结果。这种方法在 k 较小的时候（2–10）还是可行的，但是如果 k 较大，这么做也可能不会有明显地改善。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/12/12-4.html",
        "teaser":null},{
        "title": "12-5 选择聚类数目",
        
        "excerpt":
            "选择聚类数目   选择聚类数  没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用 K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。   “肘部法则”： 改变 聚类数k 值，运行K-均值聚类方法，然后计算成本 函数或者计算畸变函数 J。      我们可能会得到一条这样像肘部的曲线，这就是“肘部法则”所做的。这种模式下，它的畸变值会迅速下降，从 1 到 2，从 2 到 3 之后，你会在 3 的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用 3 个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，那么我们就选 k 等于 3。   当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/12/12-5.html",
        "teaser":null},{
        "title": "第十二章 聚类(Clustering)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/12/features.html",
        "teaser":null},{
        "title": "13-1 用途 I 数据压缩",
        
        "excerpt":
            "用途 I 数据压缩      降维问题   动机一：数据压缩   假设我们未知两个的特征：$x_1$:长度：用厘米表示；$x_2$：是用英寸表示同一物体的长度。      所以，这给了我们高度冗余表示，也许不是两个分开的特征$x_1$和 $x_2$，这两个基本的长度度量，我们可以减少数据到一维。   将数据从二维降至一维： 假使我们要采用两种不同的仪器来测量一些东西的尺寸，其中一个仪器测量结果的单位是英寸，另一个仪器测量的结果是厘米，我们希望将测量的结果作为我们机器学习的特征。现在的问题是，两种仪器对同一个东西测量的结果不完全相等（由于误差、精等），而将两者都作为特征有些重复，因而，我们希望将这个二维的数据降至一维。   将数据从三维降至二维： 这个例子中我们要将一个三维的特征向量降至一个二维的特征向量。过程是与上面类似的，我们将三维向量投射到一个二维的平面上，强迫使得所有的数据都在同一个平面上，降至二维的特征向量。      这样的处理过程可以被用于把任何维度的数据降到任何想要的维度，例如将 1000 维的特征降至 100 维。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/13/13-1.html",
        "teaser":null},{
        "title": "13-2 用途 II 可视化",
        
        "excerpt":
            "用途 II 数据可视化   在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。      假使我们有有关于许多不同国家的数据，每一个特征向量都有 50 个特征（如，GDP，人均GDP，平均寿命等）。如果要将这个 50 维的数据可视化是不可能的。使用降维的方法将其降至 2 维，我们便可以将其可视化了。      这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/13/13-2.html",
        "teaser":null},{
        "title": "13-3 主成分分析问题",
        
        "excerpt":
            "主成分分析问题      主成分分析（PCA）   主成分分析与线性回归区别   PCA的优缺点   主成分分析   主成分分析（PCA）是最常见的降维算法。   在 PCA 中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。      下面给出主成分分析问题的描述：     问题是要将 n 维数据降至 k 维，目标是找到向量$u^{(1)},u^{(2)},…,u^{(k)}$使得总的投射误差最小。   主成分分析与线性回归区别   主成分分析与线性回顾的比较：  主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。      上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。   PCA的优缺点   PCA 将 n 个特征降维到 k 个，可以用来进行数据压缩，如果 100 维的向量最后可以用 10维来表示，那么压缩率为 90%。同样图像处理领域的 KL 变换使用 PCA 做图像压缩。但 PCA 要保证降维后，还要保证数据的特性损失最小。   PCA 技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。   PCA 技术的另一大好处是，它是完全无参数限制的。在 PCA 的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。   但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/13/13-3.html",
        "teaser":null},{
        "title": "13-4 主成分分析算法",
        
        "excerpt":
            "主成分分析算法      PCA的数学实现   主成分分析算法   PCA 减少 n 维到 k 维：   第一步是均值归一化。计算出所有特征的均值，然后令$x_j=x_j-\\mu_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $\\sigma^2$。   第二步是计算协方差矩阵（covariance matrix）Σ：    第三步是计算协方差矩阵 Σ 的特征向量（eigenvectors）:  可以利用奇异值分解（singular value decomposition）来求解，[U, S, V]= svd(sigma)。      对于一个 n×n 维度的矩阵，上式中的 U 是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从 n 维降至 k 维，我们只需要从 U 中选取前 K 个向量，获得一个 n×k 维度的矩阵，我们用 $U_{reduce}$表示，然后通过如下计算获得要求的新特征向量$z^{(i)}$:    其中 x 是 n×1 维的，因此结果为 k×1 维度。注，我们不对方差特征进行处理。   牛刀小试   我们来实现PCA算法           import numpy as np  def covariance_matrix(X):     \"\"\"     Args:         X (ndarray) (m, n)     Return:         cov_mat (ndarray) (n, n):             covariance matrix of X     \"\"\"     m = X.shape[0]      return (X.T @ X) / m   def normalize(X):     \"\"\"         for each column, X-mean / std     \"\"\"     X_copy = X.copy()     m, n = X_copy.shape      for col in range(n):         X_copy[:, col] = (X_copy[:, col] - X_copy[:, col].mean()) / X_copy[:, col].std()      return X_copy                     def pca(x, keep_dims=None):     if not keep_dims:         keep_dims = x.shape[1] - 1     # todo 进行归一化     normalize_x = normalize(x)     # todo 求出协方差矩阵     cov_x = covariance_matrix(x)     # todo 奇异值分解     U, S, V = np.linalg.svd(cov_x)  # U: principle components (n, n)     # todo 选取前 keep_dims 维特征     reduction = U[:, :keep_dims]     # todo 得到降维的结果     return np.matmul(x, reduction)                    x = np.random.uniform(size=(10, 10)) pca(x).shape                          (10, 9)                  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/13/13-4.html",
        "teaser":null},{
        "title": "13-5 选择主成分数目",
        
        "excerpt":
            "选择主成分数目   主要成分分析是减少投射的平均均方误差：                  训练集的方差为：$\\frac{1}{m}\\sum^m_{i=1}               x^{(i)}               ^2$           我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的 k 值。   如果我们希望这个比例小于 1%，就意味着原本数据的偏差有 99%都保留下来了，如果我们选择保留 95%的偏差，便能非常显著地降低模型中特征的维度了。   可以先令 k=1，然后进行主要成分分析，获得$U_{reduce}$和 z，然后计算比例是否小于 1%。如果不是的话再令 k=2，如此类推，直到找到可以使得比例小于 1%的最小 k 值（原因是各个特征之间通常情况存在某种相关性）。   还有一些更好的方式来选择 k，当我们在 Octave 中调用“svd”函数的时候，我们获得三个参数：[U, S, V] = svd(sigma)。   其中的 S 是一个 n×n 的矩阵，只有对角线上有值，而其它单元都是 0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例：      也就是：     在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/13/13-5.html",
        "teaser":null},{
        "title": "13-6 压缩表示重建",
        
        "excerpt":
            "压缩表示重建   给定的$z^{(i)}$，这可能 100 维，怎么回到你原来的表示$x^{(i)}$，这可能是 1000 维的数组？      PCA 算法，有一个这样的样本。如图中样本 $x^{(1)}$,$x^{(2)}$。我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如 $Z^{(1)}$，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点$z^{(1)}$，我们怎么能回去这个原始的二维空间呢？x 为 2 维，z 为 1 维，  $z = U^T_{reduce}x$，相反的方程为：$x_{appox}=U_{reduce}z, x_{appox}\\approx x$。如图：      这是一个漂亮的与原始数据相当相似。所以，这就是你从低维表示 z 回到未压缩的表示。我们得到的数据的一个之间你的原始数据 x，我们也把这个过程称为重建原始数据。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/13/13-6.html",
        "teaser":null},{
        "title": "13-7 使用 PCA 的建议",
        
        "excerpt":
            "使用 PCA 的建议   假使我们正在针对一张 100×100 像素的图片进行某个计算机视觉的机器学习，即总共有 10000 个特征。     第一步是运用主要成分分析将数据压缩至 1000 个特征   然后对训练集运行学习算法   在预测时，采用之前学习而来的$U_{reduce}$将输入的特征 x 转换成特征向量 z，然后再进行预测      注：如果我们有交叉验证集合测试集，也采用对训练集学习而来的$U_{reduce}$。   一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。   另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/13/13-7.html",
        "teaser":null},{
        "title": "第十三章 降维(Dimensionality Reduction)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/13/features.html",
        "teaser":null},{
        "title": "14-1 问题驱动",
        
        "excerpt":
            "问题驱动      异常检测(Anomaly Detection)   异常检测(Anomaly Detection) 异常检测(Anomaly detection)问题，这是机器学习算法的一个常见应用。这种算法的一个有趣之处在于：它虽然主要用于非监督学习问题，但从某些角度看，它又类似于一些监督学习问题。   假想你是一个飞机引擎制造商，当你生产的飞机引擎从生产线上流出时，你需要进行 QA (质量控制测试)，而作为这个测试的一部分，你测量了飞机引擎的一些特征变量，比如引擎 运转时产生的热量，或者引擎的振动等等。      如果生产了 m 个引擎的话，将这些数据绘制成图表，看起来就是这个样子：      异常检测问题可以定义如下：我们假设后来有一天，有一个新的飞机引擎从生产线上流出，新飞机引擎有特征变量$x_{test}$, 我们希望知道这个新的飞机引擎是否有某种异常。   给定数据集$x^{(1)}, x^{(2)},…,x^{(m)}$，假设数据集是正常的，我们希望知道新的数据$x_{test}$是不是异常的，即这个测试数据不属于该组数据的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性 $p(x)$。      上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该 组数据的可能性就越低。   这种方法称为密度估计，表达如下：  $$p(x)  \\begin{cases} \\leq\\varepsilon &amp; \\text{anomaly}\\     \\varepsilon&amp; \\text{normal} \\end{cases}$$ $x^{(i)}$用户的第 i 个特征     模型 $p(x)$为我们其属于一组数据的可能性，通过 $p(x)&lt;\\varepsilon$检测非正常用户。    牛刀小试   Todo: 小调查,列出你知道的异常检测的用途   答：  1.识别欺骗。特征：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。构建模型来识别那些不符合该模式的用户;  2.数据中心。特征：内存使用情况，被访问的磁盘数量，CPU 的负载，网络的通信量等。构建模型来判断某些计算机是不是有可能出错了。   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/14/14-1.html",
        "teaser":null},{
        "title": "14-2 高斯分布",
        
        "excerpt":
            "高斯分布   如果变量$x$符合高斯分布$x ~ N(\\mu, \\sigma^2) $, 则其概率密度函数为：     我们可以利用已有的数据来预测总体中的$\\mu$和$\\sigma^2$的计算方法如下：       高斯分布样例：      注：机器学习中对于方差我们通常只除以 m 而非统计学中的（m-1）。在实际使用中，到底是选择使用 1/m 还是 1/(m-1)其实区别很小，只要你有一个还算大的训练集，在机器学习领域大部分人更习惯使用 1/m 这个版本的公式。这两个版本的公式在理论特性和数学特性上稍有不同，但是在实际使用中，他们的区别甚小，几乎可以忽略不计。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/14/14-2.html",
        "teaser":null},{
        "title": "14-3 算法",
        
        "excerpt":
            "高斯分布的异常检测算法   高斯分布的异常检测算法   对于给定的数据集$x^{(1)}, x^{(2)},…,x^{(m)}$，我们要针对每一个特征计算$\\mu$和$\\sigma^2$的估计值。       一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算 $p(x)$：       当  $p(x)&lt;\\varepsilon$时，为异常。   下图是一个由两个特征的训练集，以及特征的分布情况：      下面的三维图表表示的是密度估计函数，z 轴为根据两个特征的值所估计$p(x)$值：      我们选择一个 $\\varepsilon$，将  $p(x)=\\varepsilon$ 作为我们的判定边界，当  $p(x)&gt;\\varepsilon$ 时预测数据为正常数据，否则为异常数据。   在这段视频中，我们介绍了如何拟合$p(x)$，也就是 x 的概率值，以开发出一种异常检测算法。同时，在这节课中，我们也给出了通过给出的数据集拟合参数，进行参数估计，得到参数$\\mu$和$\\sigma$，然后检测新的样本，确定新样本是否是异常。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/14/14-3.html",
        "teaser":null},{
        "title": "14-4 开发并评估一个异常检测系统",
        
        "excerpt":
            "开发和评价一个异常检测系统   异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量 y 的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。   当我们开发一个异常检测系统时，我们从带标记（异常或正常）的数据着手，我们从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。   例如：我们有 10000 台正常引擎的数据，有 20 台异常引擎的数据。 我们这样分配数据：     6000 台正常引擎的数据作为训练集   2000 台正常引擎和 10 台异常引擎的数据作为交叉检验集   2000 台正常引擎和 10 台异常引擎的数据作为测试集   具体的评价方法如下：     根据测试集数据，我们估计特征的平均值和方差并构建$p(x)$函数   对交叉检验集，我们尝试使用不同的$\\varepsilon$值作为阀值，并预测数据是否异常，根据F1值或者查准率与查全率的比例来选择$\\varepsilon$   选出$\\varepsilon$后，针对测试集进行预测，计算异常检验系统的 F1 值，或者查准率与查全率之比。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/14/14-4.html",
        "teaser":null},{
        "title": "14-5 异常检测 vs 监督学习",
        
        "excerpt":
            "异常检测与监督学习的对比   异常检测与监督学习的对比   之前我们构建的异常检测系统也使用了带标记的数据，与监督学习有些相似，下面的对 比有助于选择采用监督学习还是异常检测：   两者比较：                  异常检测       监督学习                       非常少量的正向类（异常数据 y=1）, 大量的负向类（y=0）       同时有大量的正向类和负向类                 许多不同种类的异常，非常难。根据非常少量的正向类数据来训练算法。       有足够多的正向类实例，足够用于训练0算法。                 未来遇到的异常可能与已掌握的异常、非常的不同。       未来遇到的正向类实例可能与训练集中的非常近似。                 例如： 1.  欺诈行为检测  2.  生产（例如飞机引擎）  3.  检测数据中心的计算机运行状况       例如： 1.  邮件过滤器 2.  天气预报 3.  肿瘤分类           希望这节课能让你明白一个学习问题的什么样的特征，能让你把这个问题当做是一个异常检测，或者是一个监督学习的问题。   另外，对于很多技术公司可能会遇到的一些问题，通常来说，正样本的数量很少，甚至有时候是0，也就是说，出现了太多没见过的不同的异常类型，那么对于这些问题，通常应该使用的算法就是异常检测算法。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/14/14-5.html",
        "teaser":null},{
        "title": "14-6 选择合适的特征",
        
        "excerpt":
            "特征选择      异常检测中的特征选择   异常检测中的误差分析   特征选择   对于异常检测算法，我们使用的特征是至关重要的。   异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数：$x=log(x+c)$，其中 c 为非负常数； 或者$x=x^c$，c为 0-1 之间的一个分数等方法。      误差分析 一个常见的问题是一些异常的数据可能也会有较高的 p(x)值，因而被算法认为是正常的。这种情况下误差分析能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。   异常检测误差分析：      我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小），例如，在检测数据中心的计算机状况的例子中，我们可以用 CPU 负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/14/14-6.html",
        "teaser":null},{
        "title": "14-7 多元高斯分布",
        
        "excerpt":
            "多元高斯分布   多元高斯分布   假使我们有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。   下图中是两个相关特征，洋红色的线（根据$\\varepsilon$的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的 x 所代表的数据点很可能是异常值，但是其$p(x)$值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。      在一般的高斯分布模型中，我们计算$p(x)$的方法是： 通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算$p(x)$。   构建特征的协方差矩阵，用所有的特征一起来计算$p(x)$。   我们首先计算所有特征的平均值，然后再计算协方差矩阵：         注:其中$\\mu$是一个向量，其每一个单元都是原特征矩阵中一行数据的均值。   最后我们计算多元高斯分布的$p(x)$:    其中：                                     $           \\Sigma           $是定矩阵，在Octave 中用 det(sigma)计算                           $\\Sigma^{-1}$是逆矩阵   下面我们来看看协方差矩阵是如何影响模型的：         上图是 5 个不同的模型，从左往右依次分析：     是一个一般的高斯分布模型   通过协方差矩阵，令特征 1 拥有较小的偏差，同时保持特征 2 的偏差   通过协方差矩阵，令特征 2 拥有较大的偏差，同时保持特征 1 的偏差   通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关 性   通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关 性   多元高斯分布模型与原高斯分布模型的关系   可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第  1、2、3，3 个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。   原高斯分布模型和多元高斯分布模型的比较：                  原高斯分布模型       多元高斯分布模型                       不能捕捉特征之间的相关性,但可以通过将特征进行组合的方法来解决       自动捕捉特征之间的相关性                 计算代价低，能适应大规模的特征       计算代价较高 训练集较小时也同样适用                         必须要有 m&gt;n，不然的话协方差矩阵不可逆的，通常需要 m&gt;10n 另外特征冗余也会导致协方差矩阵不可逆           原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们 可以通过构造新新特征的方法来捕捉这些相关性。   如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/14/14-7.html",
        "teaser":null},{
        "title": "14-8 使用高斯分布进行异常检测",
        
        "excerpt":
            "使用高斯分布进行异常检测   使用多元高斯分布进行异常检测   使用多元高斯分布，改变参数，$\\mu$和$\\Sigma$，制定一个不同的异常检测算法。   要回顾一下多元高斯分布和多元正态分布：      参数拟合/参数估计：   给定数据集$x^{(1)}, x^{(2)},…,x^{(m)}$，$x^{(i)}$是一个n维向量，样本来自多元高斯分布，则：       使用多元高斯分布进行异常检测      原始模型与多元高斯模型的关系如图：      原始模型和多元高斯分布比较如图：     ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/14/14-8.html",
        "teaser":null},{
        "title": "第十四章 异常检测(Anomaly Detection)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/14/features.html",
        "teaser":null},{
        "title": "15-1 问题公式化",
        
        "excerpt":
            "将问题公式化      讨论推荐系统的原因   定义推荐系统问题   讨论推荐系统的原因      它是机器学习中的一个重要的应用，工业界非常重视，但是在学术界它占了很小的份额，很少受到关注。   学习机器学习中的大思想，特征学习的思想。   定义推荐系统问题   假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分。      前三部电影是爱情片，后两部则是动作片，我们可以看出 Alice 和 Bob 似乎更倾向与爱情片， 而 Carol 和 Dave 似乎更倾向与动作片。并且没有一个用户给所有的电影都打过分。我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。   下面引入一些标记：     $n_u$ 代表用户的数量   $n_m$ 代表电影的数量   $r(i,j)$: 如果用户j给电影i评过分,则$r(i,j) =1$   $y^(i,j)$ 代表用户 j 给电影 i 的评分   $m_j$ 代表用户 j 评过分的电影的总数  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/15/15-1.html",
        "teaser":null},{
        "title": "15-2 基于内容的推荐问题",
        
        "excerpt":
            "基于内容的推荐系统   在一个基于内容的推荐系统算法中，假设对于我们希望推荐的东西有一些数据，这些数据是有关这些东西的特征。   在我们的例子中，我们可以假设每部电影都有两个特征，如$x_1$代表电影的浪漫程度，$x_2$代表电影的动作程度。      则每部电影都有一个特征向量，如 $x^{(1)}$是第一部电影的特征向量为[0.9  0]。   下面我们要基于这些特征来构建一个推荐系统算法。 假设我们采用线性回归模型，我们可以针对每一个用户都训练一个线性回归模型，如$\\theta^{(1)}$ 是第一个用户的模型的参数。 则有：     $\\theta^{(j)}$: 用户 j 的参数向量   $x^{(i)}$ 电影 i 的特征向量   对于用户 j 和电影 i，我们预测评分为：代价函数 $(\\theta^{(j)})^Tx^{(i)}$   针对用户j，该线性回归模型的代价为预测误差的平方和，加上正则化项：     其中 $i:r(i,j)$表示我们只计算那些用户 j 评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以 1/2m，在这里我们将 m 去掉。并且我们不对方差项$\\theta_0$进行正则化处理。   上面的代价函数只是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和：    如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为：     ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/15/15-2.html",
        "teaser":null},{
        "title": "15-3 协同过滤",
        
        "excerpt":
            "协同过滤   在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。相反地，如果我们拥有用户的参数，我们可以学习得出电影的特征。  如果我们既没有用户的参数，也没有电影的特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。   我们的优化目标便改为同时针对x和$\\theta$进行。     对代价函数求偏导数的结果如下：      ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/15/15-3.html",
        "teaser":null},{
        "title": "15-4 协同过滤算法",
        
        "excerpt":
            "协同过滤算法   协同过滤优化目标：      协同过滤的步骤      初始化 $x^{(1)},…,x^{(n_m)},\\theta^{(1)},…,\\theta^{(n_m)}$为一些随机小值   使用梯度下降算法最小化代价函数   在训练完算法后，我们预测$(\\theta^{(j)})^Tx^{(i)}$为用户 j 给电影 i 的评分   通过这个学习过程获得的特征矩阵包含了有关电影的重要数据，这些数据不总是人能读懂的，但是我们可以用这些数据作为给用户推荐电影的依据。                  例如，如果一位用户正在观看电影$x^{(i)}$，我们可以寻找另一部电影 $x^{(j)}$，依据两部电影的特征向量之间的距离 $               x^{(i)}-x^{(j)}               $的大小。          ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/15/15-4.html",
        "teaser":null},{
        "title": "15-5 向量化",
        
        "excerpt":
            "向量化      协同过滤算法的向量化实现   低秩矩阵分解   协同过滤算法的向量化实现   协同过滤考虑两件事：     当给出一件产品时，你能否找到与之相关的其它产品。   一位用户最近看上一件产品，有没有其它相关的产品，你可以推荐给他。    我们将要做的是：实现一种选择的方法，写出协同过滤算法的预测情况。   假设有关于五部电影以及四位用户的数据集，则将将这些用户的电影评分，进行分组并存到一个矩阵中，那么这个矩阵 Y 就是一个5行4列的矩阵，它将这些电影的用户评分数据都存在矩阵里：      推出评分：      找到相关影片：      现在既然你已经对特征参数向量进行了学习，那么我们就会有一个很方便的方法来度量两部电影之间的相似性。例如说：电影i有一个特征向量 $x^{(i)}$，你是否能找到一部不同的电影 j ，保证两部电影的特征向量之间的距离$x^{(i)}$和$x^{(j)}$很小，那就能很有力地表明电影i 和电影 j 在某种程度上有相似，至少在某种意义上，某些人喜欢电影i ，或许更有可能也对电影 j 感兴趣。总结一下，当用户在看某部电影i 的时候，如果你想找 5 部与电影非常相似的电影，为了能给用户推荐 5 部新电影，你需要做的是找出电影 j ，在这些不同的电影中与我们要找的电影i的距离最小，这样你就能给你的用户推荐几部不同的电影了。   通过这个方法，希望你能知道，如何进行一个向量化的计算来对所有的用户和所有的电影进行评分计算。同时希望你也能掌握，通过学习特征参数，来找到相关电影和产品的方法。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/15/15-5.html",
        "teaser":null},{
        "title": "15-6 平均标准化的实现细节",
        
        "excerpt":
            "平均标准化的实现细节   均值归一化   让我们来看下面的用户评分数据：      如果我们新增一个用户 Eve，并且 Eve 没有为任何电影评分，那么我们以什么为依据为 Eve 推荐电影呢？   我们首先需要对结果 Y 矩阵进行均值归一化处理，将每一个用户对某一部电影的评分减去所有 用户对该电影评分的平均值：      然后我们利用这个新的 Y 矩阵来训练算法。   如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测$(\\theta^{(j)})^Tx^{(i)}+ \\mu_i$ ，对于 Eve，我们的新模型会认为她给每部电影的评分都是该电影的平均分。      均值归一化 用于处理新用户或者新物品冷启动问题   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/15/15-6.html",
        "teaser":null},{
        "title": "第十五章 推荐系统(Recommender Systems)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/15/features.html",
        "teaser":null},{
        "title": "16-1 基于大数据的学习",
        
        "excerpt":
            "大型数据集的学习   我们应该怎样应对一个有 100 万条记录的训练集？   以线性回归模型为例，每一次梯度下降迭代，我们都需要计算训练集的误差的平方和，如果我们的学习算法需要有 20 次迭代，这便已经是非常大的计算代价。   首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许我们只用 1000 个训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。      拓展问题:   Todo: 如果在1000个样本的情况下,学习曲线是右图所示的样子,继续增加样本,结果会有很大的变化吗?   答：应该不会   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/16/16-1.html",
        "teaser":null},{
        "title": "16-2 随机梯度下降",
        
        "excerpt":
            "随机梯度下降   如果我们一定需要一个大规模的训练集，我们可以尝试使用随机梯度下降法来代替批量梯度下降法。   在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价：     随机梯度下降算法为：首先对训练集随机“洗牌”，然后：      随机梯度下降算法在每一次计算之后便更新参数 θ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。     ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/16/16-2.html",
        "teaser":null},{
        "title": "16-3 小批量梯度下降",
        
        "excerpt":
            "小批量梯度下降   批量梯度下降（BGD）   批量梯度下降法是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新。   它的数学理解如下：   （1）对目标函数求偏导：        其中 $i=1,2,…,m$ 表示样本数，$ j=0,1$ 表示特征数，这里我们使用了偏置项 $x^{(i)}_0=1$ 。      （2）每次迭代对参数进行更新：        注意这里更新时存在一个求和函数，即为对所有样本进行计算处理，可与下文SGD法进行比较。   伪代码形式为：      优点：     一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。   由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。   缺点：     当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。      从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：      随机梯度下降（SGD）     随机梯度下降法不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。   对于一个样本的目标函数为：      （1）对目标函数求偏导：       （2）参数更新：  注意，这里不再有求和符号        伪代码形式为：      优点：     由于不是在全部训练数据上的代价函数，而是在每轮迭代中，随机优化某一条训练数据上的代价函数，这样每一轮参数的更新速度大大加快。   缺点：     准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。   可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。   不易于并行实现。   解释一下为什么SGD收敛速度比BGD要快：      答：这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）；而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。   从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下：      小批量梯度下降（MBGD）   小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用 batch_size 个样本来对参数进行更新。    这里我们假设 batchsize=10 ，样本数 m=1000 。      伪代码形式为：      优点：     通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。   每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次)   可实现并行化。   缺点：     batch_size的不当选择可能会带来一些问题。   batcha_size的选择带来的影响：      在合理地范围内，增大batch_size的好处：  a. 内存利用率提高了，大矩阵乘法的并行化效率提高。  b. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。      c. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。   盲目增大batch_size的坏处：  a. 内存利用率提高了，但是内存容量可能撑不住了。  b. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。  c. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。     下图显示了三种梯度下降算法的收敛过程：      参考及引用：https://www.cnblogs.com/lliuye/p/9451903.html  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/16/16-3.html",
        "teaser":null},{
        "title": "16-4 随机梯度下降的收敛性",
        
        "excerpt":
            "随机梯度下降算法的收敛性      随机梯度下降算法的调试   学习率 α 的选取   随机梯度下降算法的调试   在批量梯度下降中，我们可以令代价函数 J 为迭代次数的函数，绘制图表，根据图表来判断梯度下降是否收敛。但是，在大规模的训练集的情况下，这是不现实的，因为计算代价太大了。   在随机梯度下降中，我们在每一次更新$\\theta$之前都计算一次代价，然后每 X 次迭代后，求出这 X 次对训练实例计算代价的平均值，然后绘制这些平均值与 X 次迭代的次数之间的函数图表。      如果得到一个颠簸不平但是不会明显减少的函数图像（如上面左下图中蓝线所示）。我们可以增加 X 来使得函数更加平缓，也许便能看出下降的趋势（如上面左下图中红线所示）；或者可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么我们的模型本身可能存在一些错误。   如果我们得到的曲线如上面右下方所示，不断地上升，那么我们可能会需要选择一个较小的学习率 α。也可以令学习率随着迭代次数的增加而减小，例如令：  $\\alpha = \\frac{const1}{iteratiobNumber + const2}$   随着我们不断地靠近全局最小值，通过减小学习率，我们迫使算法收敛而非在最小值附近徘徊。但是通常我们不需要这样做便能有非常好的效果了，对 α 进行调整所耗费的计算通常不值得。   ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/16/16-4.html",
        "teaser":null},{
        "title": "16-5 在线学习",
        
        "excerpt":
            "在线学习   在线学习机制   一个算法来从中学习的时候来模型化问题在线学习算法指的是对数据流而非离线的静态数据集的学习。许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。   假使我们正在经营一家物流公司，每当一个用户询问从地点 A 至地点 B 的快递费用时，我们给用户一个报价，该用户可能选择接受（y=1）或不接受（y=0）。   现在，我们希望构建一个模型，来预测用户接受报价使用我们的物流服务的可能性。因此报价是我们的一个特征，其他特征为距离，起始地点，目标地点以及特定的用户数据。模型的输出是 p(y=1)。   在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环。      一旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。   在线学习的一个优点就是，如果你有一个变化的用户群，又或者你在尝试预测的事情，在缓慢变化，就像你的用户的品味在缓慢变化，这个在线学习算法，可以慢慢地调试你所学习到的假设，将其调节更新到最新的用户行为。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/16/16-5.html",
        "teaser":null},{
        "title": "16-6 Map Reduce 和数据并行",
        
        "excerpt":
            "Map Reduce 和 数据并行   Map Reduce和数据并行   Map Reduce和数据并行对于大规模机器学习问题而言是非常重要的概念。   之前提到，如果我们用批量梯度下降算法来求解大规模数据集的最优解，我们需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。如果我们能够将我们的数据集分配给多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计算的结果汇总然后再求和。这样的方法叫做Map Reduce。      具体而言，如果任何学习算法能够表达为，对训练集的函数的求和，那么便能将这个任务分配给多台计算机（或者同一台计算机的不同 CPU 核心），以达到加速处理的目的。   例如，我们有 400 个训练实例，我们可以将批量梯度下降的求和任务分配给 4 台计算机进行处理：      很多高级的线性代数函数库已经能够利用多核 CPU 的多个核心来并行地处理矩阵运算，这也是算法的向量化实现如此重要的缘故（比调用循环快）。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/16/16-6.html",
        "teaser":null},{
        "title": "第十六章 大规模机器学习(Large Scale Machine Learning)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/16/features.html",
        "teaser":null},{
        "title": "17-1 问题描述与管道",
        
        "excerpt":
            "问题描述与管道      图片文字识别(Application Example: Photo OCR)的问题描述   图片文字识别流程图   图片文字识别问题描述   图像文字识别应用所作的事是，从一张给定的图片中识别文字。这比从一份扫描文档中识别文字要复杂的多。   ![](https://i.loli.net/2018/12/02/5c03216750f6e.png   图片文字识别流程图   为了完成这样的工作，需要采取如下步骤：     文字侦测（Text detection）——将图片上的文字与其他环境对象分离开来   字符切分（Character segmentation）——将文字分割成一个个单一的字符   字符分类（Character classification）——确定每一个字符是什么   可以用任务流程图来表达这个问题，每一项任务可以由一个单独的功能来负责解决：     ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/17/17-1.html",
        "teaser":null},{
        "title": "17-2 滑动窗口",
        
        "excerpt":
            "滑动窗口      滑动窗口   文字识别中的滑动窗口   滑动窗口   滑动窗口是一项用来从图像中抽取对象的技术。      假使我们需要在一张图片中识别行人， 首先要做的是用许多固定尺寸的图片来训练一个能够准确识别行人的模型。      然后我们用之前训练识别行人的模型时所采用的图片尺寸在目标图片上进行剪裁，然后将剪裁得到的切片交给模型，让模型判断是否为行人，然后在图片上滑动剪裁区域重新进行剪裁，将新剪裁的切片也交给模型进行判断，如此循环直至将图片全部检测完。 一旦完成后，我们按比例放大剪裁的区域，再以新的尺寸对图片进行剪裁，将新剪裁的切片按比例缩小至模型所采纳的尺寸，交给模型进行判断，如此循环。      文字识别中的滑动窗口   滑动窗口技术也被用于文字识别，首先训练模型能够区分字符与非字符，然后，运用滑动窗口技术识别字符，一旦完成了字符的识别，我们将识别得出的区域进行一些扩展，然后 将重叠的区域进行合并。接着我们以宽高比作为过滤条件，过滤掉高度比宽度更大的区域 （认为单词的长度通常比高度要大）。下图中绿色的区域是经过这些步骤后被认为是文字的 区域，而红色的区域是被忽略的。      以上便是文字侦测阶段。 下一步是训练一个模型来完成将文字分割成一个个字符的任 务，需要的训练集由单个字符的图片和两个相连字符之间的图片来训练模型。      模型训练完后，我们仍然是使用滑动窗口技术来进行字符识别。      以上便是字符切分阶段。 最后一个阶段是字符分类阶段，利用神经网络、支持向量机 或者逻辑回归算法训练一个分类器即可。  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/17/17-2.html",
        "teaser":null},{
        "title": "17-3 获取海量数据和人工数据",
        
        "excerpt":
            "获取海量数据和人工数据   获取大量数据和人工数据   如果我们的模型是低方差的，那么获得更多的数据用于训练模型，是能够有更好的效果的。问题在于，我们怎样获得数据，数据不总是可以直接获得的，我们有可能需要人工地创造一些数据。   以我们的文字识别应用为例，我们可以字体网站下载各种字体，然后利用这些不同的字体配上各种不同的随机背景图片创造出一些用于训练的实例，这让我们能够获得一个无限大的训练集。这是从零开始创造实例。   另一种方法是，利用已有的数据，然后对其进行修改，例如将已有的字符图片进行一些扭曲、旋转、模糊处理。只要我们认为实际数据有可能和经过这样处理后的数据类似，我们便可以用这样的方法来创造大量的数据。      有关获得更多数据的几种方法：     人工数据合成   手动收集、标记数据   众包  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/17/17-3.html",
        "teaser":null},{
        "title": "17-4 上限分析",
        
        "excerpt":
            "上限分析      上限分析：机器学习的一个应用中，哪一部分最值得花时间和精力去改善   上限分析   回到我们的文字识别应用中，我们的流程图如下：      流程图中每一部分的输出都是下一部分的输入。   上限分析中，我们选取一部分，手工提供100%正确的输出结果，然后看应用的整体效果提升了多少。   假使我们的例子中总体效果为 72%的正确率。   如果我们令文字侦测部分输出的结果 100%正确，发现系统的总体效果从 72%提高到了  89%。这意味着我们很可能会希望投入时间精力来提高我们的文字侦测部分。   接着令字符切分输出的结果 100%正确，发现系统的总体效果只提升了 1%，这意味着，我们的字符切分部分可能已经足够好了。   最后我们手工选择数据，让字符分类输出的结果 100%正确，系统的总体效果又提升了  10%，这意味着我们可能也会应该投入更多的时间和精力来提高应用的总体表现。     ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/17/17-4.html",
        "teaser":null},{
        "title": "第十七章 应用实例-图片文字识别(Application Example-Photo OCR)",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/17/features.html",
        "teaser":null},{
        "title": "Features and customization",
        
        "excerpt":
            "Features   This is a short demonstration textbook to show the general layout / style of textbooks built with Jupyter and Jekyll. The markdown files for this page (and others in the textbook) is generated from the notebooks with the scripts/generate_textbook.py script, which is called when you run make book.   The content for the book is contained in a folder in the site’s repository called content/. It has a combination of markdown and Jupyter notebooks. This content is rendered into the textbook that you see here!   To begin, click on one of the chapter sections in the sidebar to the left. The first section demonstrates some simple functionality of this repository, while the following chapters contain a subset of content from the Foundations in Data Science.   Quickstart   This chapter shows a couple ways to add content to your course textbook. Click on the section headers to the left, or on the “next” button below in order to read further.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/features/features.html",
        "teaser":null},{
        "title": "Markdown files",
        
        "excerpt":
            "Creating book content   The two kinds of files that contain course content are:      Jupyter Notebooks   Markdown files   Each are contained in the content/ folder and referenced from _data/toc.yml.   If the file is markdown, it will be copied over with front-matter YAML added so that Jekyll can parse it   Sidebars with Jekyll   You may notice that there’s a sidebar to the right (if your screen is wide enough). These are automatically generated from the headers that are present in your page. The sidebar will automatically capture all 2nd and 3rd level section headers. The best way to designate these headers is with # characters at the beginning of a line.   Here’s a third-level header   This section is here purely to demonstrate the third-level header of the rendered page!   Embedding media   Adding images   You can reference external media like images from your markdown file. If you use relative paths, then they will continue to work when the markdown files are copied over, so long as they point to a file that’s inside of the repository.   Here’s an image relative to the site root      Adding movies   You can even embed references to movies on the web! For example, here’s a little gif for you!      This will be included in your website when it is built.  ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/features/markdown.html",
        "teaser":null},{
        "title": "Jupyter notebooks",
        
        "excerpt":
            "Content with notebooks   You can also create content with Jupyter Notebooks. The content for the current page is contained in a Jupyter Notebook in the notebooks/ folder of the repository. This means that we can include code blocks and their outputs, and export them to Jekyll markdown.   You can find the original notebook for this page at this address   Markdown + notebooks   As it is markdown, you can embed images, HTML, etc into your posts!      You an also $add_{math}$ and     or     But make sure you \\$Escape \\$your \\$dollar signs \\$you want to keep!   Code blocks and image outputs   Textbooks with Jupyter will also embed your code blocks and output in your site. For example, here’s some sample Matplotlib code:           from matplotlib import rcParams, cycler import matplotlib.pyplot as plt import numpy as np plt.ion()                    # Fixing random state for reproducibility np.random.seed(19680801)  N = 10 data = [np.logspace(0, 1, 100) + np.random.randn(100) + ii for ii in range(N)] data = np.array(data).T cmap = plt.cm.coolwarm rcParams['axes.prop_cycle'] = cycler(color=cmap(np.linspace(0, 1, N)))   from matplotlib.lines import Line2D custom_lines = [Line2D([0], [0], color=cmap(0.), lw=4),                 Line2D([0], [0], color=cmap(.5), lw=4),                 Line2D([0], [0], color=cmap(1.), lw=4)]  fig, ax = plt.subplots(figsize=(10, 5)) lines = ax.plot(data) ax.legend(custom_lines, ['Cold', 'Medium', 'Hot']);                                      Note that the image above is captured and displayed by Jekyll.   Removing content before publishing   You can also remove some content before publishing your book to the web. For example, in the original notebook there used to be a cell below…           thisvariable = \"none of this should show up in the textbook\"  fig, ax = plt.subplots() x = np.random.randn(100) y = np.random.randn(100) ax.scatter(x, y, s=np.abs(x*100), c=x, cmap=plt.cm.coolwarm) ax.text(0, .5, thisvariable, fontsize=20, transform=ax.transAxes) ax.set_axis_off()                                      You can also remove only the code so that images and other output still show up.   Below we’ll only display an image. It was generated with Python code in a cell, which you can see in the original notebook           # NO CODE thisvariable = \"this plot *will* show up in the textbook.\"  fig, ax = plt.subplots() x = np.random.randn(100) y = np.random.randn(100) ax.scatter(x, y, s=np.abs(x*100), c=x, cmap=plt.cm.coolwarm) ax.text(0, .5, thisvariable, fontsize=20, transform=ax.transAxes) ax.set_axis_off()                                      And here we’ll only display a Pandas DataFrame. Again, this was generated with Python code from this original notebook.           # NO CODE import pandas as pd pd.DataFrame([['hi', 'there'], ['this', 'is'], ['a', 'DataFrame']], columns=['Word A', 'Word B'])                                                  Word A       Word B                       0       hi       there                 1       this       is                 2       a       DataFrame                       You can configure the text that Textbooks with Jupyter uses for this by modifying your site’s _config.yml file.   Interactive outputs   We can even do the same for interactive material. Below we’ll display a map using ipyleaflet. When the notebook is converted to Markdown, the code for creating the interactive map is retained.   Note that this will only work for some packages. They need to be able to output standalone HTML/Javascript, and not depend on an underlying Python kernel to work.           import folium                    m = folium.Map(     location=[45.372, -121.6972],     zoom_start=12,     tiles='Stamen Terrain' )  folium.Marker(     location=[45.3288, -121.6625],     popup='Mt. Hood Meadows',     icon=folium.Icon(icon='cloud') ).add_to(m)  folium.Marker(     location=[45.3311, -121.7113],     popup='Timberline Lodge',     icon=folium.Icon(color='green') ).add_to(m)  folium.Marker(     location=[45.3300, -121.6823],     popup='Some Other Location',     icon=folium.Icon(color='red', icon='info-sign') ).add_to(m)   m                                       ",
        "categories": [],
        "tags": [],
        "url": "http://0.0.0.0:4000/features/notebooks.html",
        "teaser":null},]
</script>
              <nav class="c-page__nav">
  

  
</nav>

            </div>
          </div>
        </div>
      </main>
    </div>

  </body>
</html>
