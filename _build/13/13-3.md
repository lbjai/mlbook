---
interact_link: content/C:\Users\lbj\Desktop\book\content\13/13-3.ipynb
kernel_name: python3
has_widgets: false
title: '13-3 主成分分析问题'
prev_page:
  url: /13/13-2
  title: '13-2 用途 II 可视化'
next_page:
  url: /13/13-4
  title: '13-4 主成分分析算法'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

### 主成分分析问题

+ 主成分分析（PCA）
+ 主成分分析与线性回归区别
+ PCA的优缺点

**主成分分析** 

主成分分析（PCA）是最常见的降维算法。 

在 PCA 中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。

![](https://i.loli.net/2018/12/02/5c02c94a8ba9c.png)
 
下面给出主成分分析问题的描述：     
问题是要将 n 维数据降至 k 维，目标是找到向量$u^{(1)},u^{(2)},...,u^{(k)}$使得总的投射误差最小。

**主成分分析与线性回归区别** 

主成分分析与线性回顾的比较： 
主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。 
 
![](https://i.loli.net/2018/12/02/5c02c9c1841e4.png)
 
上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。 

**PCA的优缺点** 

PCA 将 n 个特征降维到 k 个，可以用来进行数据压缩，如果 100 维的向量最后可以用 10维来表示，那么压缩率为 90%。同样图像处理领域的 KL 变换使用 PCA 做图像压缩。但 PCA 要保证降维后，还要保证数据的特性损失最小。 

PCA 技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。

PCA 技术的另一大好处是，它是完全无参数限制的。在 PCA 的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。

但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。 
