---
interact_link: content/C:\Users\lbj\Desktop\book\content\02-3.ipynb
kernel_name: python3
has_widgets: false
title: '第二章 单变量线性回归'
prev_page:
  url: /02-2
  title: '第二章 单变量线性回归'
next_page:
  url: /02-4
  title: '第二章 单变量线性回归'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

### 理解代价函数

**<img width='16px' src='http://imgbed.momodel.cn/5cc1a0b1e3067ce9b6abf757.jpg'><span class='md-video-link https://player.bilibili.com/player.html?aid=9912938&cid=19653997&page=8'>这一节我们主要学习以下内容 - 0m5s</span>**

+ 代价函数的直观理解 I 

**代价函数的直观理解 I **

**本节强烈建议观看视频理解，以下仅列出关键步骤。**

回顾上文，我们已经得到了单线性回归的模型：

hypothesis(假设):    
         $h_{0}(x) = \theta_{0} + \theta_{1}x$     

parameters（参数）:      
$\theta_{0}$ , $\theta_{1}$   
通过选择不同的参数,会得到不同的直线拟合.

Cost Function(代价函数）:     
$J(\theta_{0}, \theta_{1}) = \frac{1}{2m}\sum_{i=1}^m(h_{0}(x^{(i)})-y^{(i)})^{2}$

Goal:     
$minimize_{\theta_{0},\theta_{1}}J(\theta_{0}, \theta_{1})$   
我们的优化目标是,最小化代价函数

为理解方便，本节讲解中将h函数简化为$h_{0}(x) =\theta_{1}x$，即设$\theta_{0}=0$

首先明确一个概念：
+ 在假设h中，$\theta_{1}$是一个固定的参数，只有$x$才是自变量。因变量为预测值 $h_{0}(x)$
+ 在优化函数$J(\theta)$中，$\theta_{1}$是自变量，因变量为预测值$h_{0}(x)$与真实值$y$的误差$J(\theta)$


从这里开始,用实例来说明,不同的 $\theta_{1}$ , 代价函数的值.建议查看视频

**<img width='16px' src='http://imgbed.momodel.cn/5cc1a0b1e3067ce9b6abf757.jpg'><span class='md-video-link https://player.bilibili.com/player.html?aid=9912938&cid=16412309&page=8'>理解代价函数 - I - 1m40s</span>**

由此我们可以画出假设h和优化函数$J(\theta)$对应的函数图像。

<img src='https://i.loli.net/2018/12/04/5c0652fe1637f.png' width=617 height=345 >

**牛刀小试**

Todo: 当 $\theta_{1}$ 是 0 的时候, $J(\theta)$ 的值是多少?


<span class='md-hint-alone-link pop 0'>查看答案</span>
