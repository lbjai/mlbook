---
interact_link: content/C:\Users\lbj\Desktop\book\content\06/06-2.ipynb
kernel_name: python3
has_widgets: false
title: '06-2 代价函数 II'
prev_page:
  url: /06/06-1
  title: '06-1 过拟合问题'
next_page:
  url: /06/06-3
  title: '06-3 正则化与线性回归'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

### 代价函数 II

+ 代价函数中的正则化处理
+ 正则化参数选择

高次项导致了过拟合的产生。

正则化的基本方法：对高次项添加惩罚值，让高次项的系数接近于0。

假如我们有非常多的特征，我们并不知道其中哪些特征我们要惩罚，我们将对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设： 

$$
J(\theta) = \frac{1}{2m} [ \sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^{2} + \lambda\sum_{j=1}^n\theta^2_j  ]
$$
 
其中$\lambda$又称为正则化参数（Regularization Parameter）。 注：根据惯例，我们不对$\theta_0$进行惩罚。

**牛刀小试**

<div markdown="1" class="cell code_cell">
<div class="input_area" markdown="1">
```python
# todo 实现带正则的mse代价函数
import numpy as np

def mseWithRegular(predict, y, w, lmd=0.1):
    '''
        predict: 模型输出
        y: 真实标签
        w: 模型权重
        lmd: 正则化参数
    '''
    # todo 实现上方公式
    constrct_loss = np.sum((predict - y) ** 2)
    experience_loss = lmd * np.sum(w ** 2)
    loss = (constrct_loss + experience_loss) / (2 * len(predict))
    return loss

predict = np.array([1, 1.5, 2])
y = np.array([0.9, 1.4, 2.1])
w = np.array([[1], [1], [1]])
mseWithRegular(predict, y, w)

```
</div>

<div class="output_wrapper" markdown="1">
<div class="output_subarea" markdown="1">


{:.output_data_text}
```
0.055000000000000014
```


</div>
</div>
</div>

**正则化参数选择**

如果选择的正则化参数$\lambda$过大，则会把所有的参数都最小化了，导致模型变成 $h_\theta(x) = \theta_0$，造成欠拟合。 

原因是：增加$\lambda\sum_{j=1}^n\theta^2_j$后，如果令λ的值很大的话，为了使 Cost Function 尽可能的小，所有的 $\theta$的值（不包括$\theta_0$）都会在一定程度上减小。 但若λ的值太大了，那么$\theta$的值（不包括$\theta_0$）都会趋近于 0，这样我们所得到的只能是一条平行于 x 轴的直线。

所以对于正则化，我们要取一个合理的λ的值，这样才能更好的应用正则化。 

回顾一下代价函数，为了使用正则化，让我们把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。
