---
interact_link: content/C:\Users\lbj\Desktop\book\content\11/11-4.ipynb
kernel_name: python3
has_widgets: false
title: '11-4 核技巧 I'
prev_page:
  url: /11/11-3
  title: '11-3 大间距分类器背后的数学原理 (选学)'
next_page:
  url: /11/11-5
  title: '11-5 核技巧 II'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

### 核技巧 I

+ 回顾分类问题的多项式模型
+ 高斯核函数

分类问题的多项式模型：

![](https://i.loli.net/2018/12/02/5c0396232c71f.png)

为了获得上图所示的判定边界，我们的模型可能是$\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x^2_1+\theta_5x^2_2+...$的形式。 

我们可以用一系列的新的特征 f 来替换模型中的每一项。例如令：

$f_1 =x_1, f_2=x_2,f_3=x_1x_2, f_4=x^2_1, f_5=x^2_2,...$
 得到 $h_\theta(x)=f1+f2+...+fn$。
 
 然而，除了对原有的特征进行组合以外，有没有更好的方法来构造 f1,f2,f3？我们可以利用核函数来计算出新的特征。 
 

**核函数**  

给定一个训练实例x，我们利用x的各个特征与我们预先选定的地标（landmarks） $l^{(1)},l^{(2)},l^{(3)}$的近似程度来选取新的特征 f1,f2,f3。 
 
![](https://i.loli.net/2018/12/02/5c0397de38f63.png)
 
例如： 
 $$f_1=similarity(x,l^{(1)})= exp(-\frac{||x-l^{(1)}||^2}{2\sigma^2})$$
其中：$||x-l^{(1)}||^2=\sum^n_{j=1}(x_j-l^{(1)}_j)^2$为实例 x 中所有特征与地标$l^{(1)}$之间的距离的和。

上例中的 $similarity（x,l^{(1)})$就是核函数，具体而言，这里是一个高斯核函数（Gaussian Kernel）。 **注：这个函数与正态分布没什么实际上的关系，只是看上去像而已。 **

这些地标的作用是什么？如果一个训练实例 x 与地标 L 之间的距离近似于 0，则新特征 f 近似于$ e^{-0}=1$，如果训练实例 x 与地标 L 之间距离较远，则 f 近似于 $e^{-(一个较大的数)}=0$。 

假设我们的训练实例含有两个特征$[x_1 x_2]$，给定地标 $l^{(1)}$与不同的 σ 值，见下图：

![](https://i.loli.net/2018/12/02/5c03995344bc0.png)

 
图中水平面的坐标为$[x_1 x_2]$,而垂直坐标轴代表 f。可以看出，只有当 x 与 $l^{(1)}$重合时 f 才具有最大值。随着 x 的改变 f 值改变的速率受到 σ2的控制。 

在下图中，当实例处于洋红色的点位置处，因为其离$l^{(1)}$更近，但是离$l^{(2)}$和$l^{(3)}$较远，因此 f1接近 1，而 f2,f3 接近 0。因此 hθ(x)=θ0+θ1f1+θ2f2+θ1f3>0，因此预测 y=1。同理可以求出，
对于离 l(2)较近的绿色点，也预测 y=1，但是对于蓝绿色的点，因为其离三个地标都较远，预测 y=0。 

![](https://i.loli.net/2018/12/02/5c0399ec07e55.png)

 
这样，图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练实例和我们选
取的地标所得出的判定边界，在预测时，我们采用的特征不是训练实例本身的特征，而是通 过核函数计算出的新特征 f1,f2,f3。 
