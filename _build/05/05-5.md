---
interact_link: content/C:\Users\lbj\Desktop\book\content\05/05-5.ipynb
kernel_name: python3
has_widgets: false
title: '05-5 简化的代价函数和梯度下降'
prev_page:
  url: /05/05-4
  title: '05-4 代价函数 I'
next_page:
  url: /05/05-6
  title: '05-6 高级优化技巧'
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

### 简化的代价函数和梯度下降

+ 简化代价函数
+ 梯度下降


逻辑回归的代价函数： 

![](https://i.loli.net/2018/12/01/5c01d18b66e49.png)

这个式子可以合并成： 

![](https://i.loli.net/2018/12/01/5c01d26cab241.png)

要试图找出让 $J(\theta)$取得最小值的参数$\theta$。  


**梯度下降**

最小化代价函数的方法，是使用梯度下降法(gradient descent)。这是通常用的梯度下降法的模板： 

![](https://i.loli.net/2018/12/01/5c01d34b3ee5e.png)

反复更新每个参数，用这个式子减去学习率 α 乘以后面的微分项。求导后得到： 

![](https://i.loli.net/2018/12/01/5c01d3aa903c7.png)

计算得到等式： 
$$\theta_j := \theta_j - \alpha \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j$$

来它同时更新所有$\theta$的值。 

这个更新规则和之前用来做线性回归梯度下降的式子是一样的， 但是假设的定义发生了变化。即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西。

**牛刀小试**

Todo:  列出你知道的梯度下降的技巧

答:监控梯度下降法以确保其收敛、参数向量化、特征缩放等

